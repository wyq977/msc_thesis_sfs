\chapter{Fourier-analytic Barron spaces}
\label{ch:fourier}

In this chapter, we study the model classes of functions that are \textit{well
approximated} by 2NN. \cite{barronUniversalApproximationBounds1993} has
demonstrated that under some mild assumptions for the target functions, a
dimension-independent approximation rate can be achieved by 2NN. Specifically,
this assumption is formulated as a spectral condition on the Fourier transform
of the target function. When approximating functions with finite Fourier moment
by 2NN with n nodes, an approximation rate of $\bigO(n^{-1/2})$ is obtained in
$\lp{2}$. In section \ref{sec:fourier_variation_space}, we connect this result
with $n$-term dictionary approximation, where we identify the dictionary of
choice for the spectral condition.

The chapter is structured as follows. In section \ref{sec:spectral_condition},
we introduce the class of functions that satisfies certain smoothness
constraints. Section \ref{sec:construction_of_fouier} characterizes the function
spaces constructed based on the previous smoothness restriction. In section
\ref{sec:approximation_rate_fouier}, we prove an approximation rate of
$\bigO(n^{-\frac{1}{2}})$ with respect to the supremum norm, where $n$ is the
number of nodes. Section \ref{sec:fourier_variation_space} presents the
dictionary corresponding to the smoothness condition. Finally, we provide some
high order error rates for ReLU\textsuperscript{k} networks.

\section{Barron class and spectral condition}
\label{sec:spectral_condition}

In this section, we would identify the class of functions originally proposed by
\cite{barronUniversalApproximationBounds1993}. We define the \textit{spectral
condition} on which the smoothness constrain of a function is imposed. A
modified spectral condition proposed by
\cite{siegelCharacterizationVariationSpaces2022} is also introduced.

\begin{definition}[Barron class]
    \label{def:barron_class}
    Let $U$ be a nonempty bounded domain in $\mathbb{R}^d$. A function $f: U \to
    \mathbb{R}$ is said to be in \textit{Barron class} with a constant $C > 0$,
    if there is a $x_0$ in $U$, $c \in [-C, C]$, and a measurable function $f:
    \mathbb{R}^d \to \mathbb{C}$ satisfying:
    \begin{align}
        & \int_{\mathbb{R}^d} \abs{\omega}_{U, x_0} 
        \cdot \abs{\fourier{f}(\omega)} \,d\omega < C 
        \label{eq:another_spectral_seminorm_condition} \\
        & f(x) = c + \int_{\mathbb{R}^d} (
            e^{i\omega\tr x} - e^{i\omega\tr x_0}
        ) \cdot \fourier{f}(\omega)\,d\omega
    \end{align}

    where $\abs{\omega}_{U, x_0} := \sup_{x\in U}\norm{\spr{\omega}{x - x_0}}$
    and it is denoted by $\abs{\omega}_U$ when $x_0 = 0$ for simplicity. We
    refer the class of all functions satisfying the above condition as 
    $\Gamma_C(U, x_0)$\footnote{
        $\Gamma_C(U, x_0)$ is the Fourier-analytic Barron space
        $\bspace{\mcal{F}, 1}(U)$ discussed before. The integral condition in
        \eqref{eq:another_spectral_seminorm_condition} is the integral condition
        $v_{f,1} < \infty$ defined in \eqref{eq:spectral_condition}.
    }.
\end{definition}

\begin{remark}
    In the above definition, the domain $U$ bounded using $\abs{\omega}_{U}$ is
    interpreted as bounding the trigonometric component $e^{i\omega\tr x}$ where
    $\omega$ is the frequency in the Fourier representation of the functions
    restricted. It it easier to see that the constant $C$
    \begin{equation}
        C \leq r \int_{\R^d} \abs{\omega} \abs{\fourier{f}(\omega)}\,d\omega.
    \end{equation}
    if $U$ is in a ball of radius $r$ since $\abs{\omega}_{U} \leq r \cdot
    \abs{\omega}$ by the Cauchy-Schwarz inequality.

    More generally, if $U$ is a $l_p$ ball of radius $r$, we can rewrite the
    condition as
    \begin{equation}
        r \int_{\R^d} \norm{\omega}_p \abs{\fourier{f}(\omega)}\,d\omega 
        < \infty, \quad 1 \leq p \leq \infty.
    \end{equation}

\end{remark}


% Here it is the L1 norm of \omega

\begin{definition}[Spectral condition]
    \label{def:spectral_condition}
    Let $U$ be a nonempty bounded domain in $\R^d$.
    Suppose that a function $f: U \to \R$ admits a Fourier representation
    \begin{equation}
        f(x) = \int_{\R^d} e^{i\omega\tr x} \fourier{f}(\omega) \,d\omega
    \end{equation}
    where $\fourier{f}: \R^d \to \mathbb{C}$ is the Fourier transform of $f$.

    For any $s \in \Nat$, the spectral condition of $f$ is defined as
    \begin{equation}
        \label{eq:spectral_condition}
        v_{f,s} 
            = \int_{\R^d} \abs{\omega}^s \abs{\fourier{f}(\omega)}\,d\omega.
    \end{equation}
\end{definition}



\begin{definition}[Modified spectral condition]
    \label{def:spectral_condition_mod}
    Under the setup of Definition \ref{def:spectral_condition}, we define a
    modified spectral condition
    \begin{equation}
        \label{eq:spectral_condition_mod}
        v'_{f,s} 
            = \int_{\R^d} (1 + \abs{\omega})^s \abs{\fourier{f}(\omega)}
            \,d\omega.
    \end{equation}
\end{definition}



\section{Construction of Fourier-analytic Barron spaces}
\label{sec:construction_of_fouier}

Firstly, we define the spectral condition and the seminorm and norm\footnote{
    Often papers include ``Barron'' as readers can deduce from the context. 
    In the following chapters, we would like to call them \textit{spectral 
    semi/norm} as another definition of norm in infinite-width Barron spaces is 
    named \textit{Barron norm}.
} by which the smoothness of a function
is controlled. 

\begin{definition}[Spectral seminorm]
    \label{def:spectral_seminorm}
    Let $U$ be a nonempty bounded domain in $\R^d$. Suppose that a function $f:
    U \to \R$ admits a Fourier representation
    \begin{equation}
        f(x) = \int_{\R^d} e^{i\omega\tr x} \fourier{f}(\omega) \,d\omega
    \end{equation}
    where $\fourier{f}: \R^d \to \mathbb{C}$ is the Fourier transform of $f$.

    For any $s \in \Nat$, the spectral seminorm of $f$ is defined as
    \begin{equation}
        \specseminorm{f}{s} = \inf_{f_{e\mid U} = f} v_{f,s}
    \end{equation}
    where the infimum is taken over all extensions $f_e$ of $f$ in $\lp1(U)$.
\end{definition}


The notion of spectral norm was first introduced in
~\cite{siegelApproximationRatesNeural2021} since it is more convenient compared
to the seminorm.

\begin{definition}[Spectral norm]
    \label{def:spectral_norm}
    Under the setup of Definition \ref{def:spectral_seminorm}, we define a
    spectra norm
    \begin{equation}
        \specnorm{f}{s} = \inf_{f_{e\mid U} = f} v'_{f,s}.
    \end{equation}
\end{definition}

\begin{definition}[Fourier-analytic Barron spaces]
    \label{def:fourier_space}
    Let $U$ be a nonempty bounded domain in $\R^d$. The Fourier-analytic Barron
    spaces are
    \begin{equation}
        \bspace{\mcal{F},s}(U) := \Big\{
            f: U \to \R: v'_{f,s} < \infty  \textnormal{ and }
            \forall x\in U, 
                f(x) = \int_{\R^d} e^{i\omega\tr x} \fourier{f}(\omega)\,d\omega
        \Big\}
    \end{equation}
    equipped with a norm $\specnorm{f}{s}$ for all $s \in \Nat$.
\end{definition}

% \TONOTE{What is the extensions}

% \TONOTE{Why is 1/2 allowed? in \cite{siegelCharacterizationVariationSpaces2022}}

The smoothness index $s \in \Nat$ refers to the degree of smoothness of
functions in $\bspace{\mcal{F}, s}$. It is reasonable to expect that as $s$
increases, the functions in $\bspace{\mcal{F}, s}$ become smoother.
Consequently, the size of the function spaces of higher smoothness index is
expected to shrink, which may suggest better approximation error rates. In the
following sections, we will provide a precise statement of the improved
approximation error rates, along with a detailed explanation of the complexities
inherent within and between these spaces.

In general, the activation function associated with the infinite-width Barron
spaces is ReLU and we will explicitly state when other functions (e.g. squared
ReLU, ReLU\textsuperscript{k}, Heaviside) are used.




\section{Approximation rate in Fourier-analytic Barron spaces}
\label{sec:approximation_rate_fouier}

\cite{barronUniversalApproximationBounds1993} has demonstrated that functions of
d-variables with finite Fourier moments can be approximated using the
superpositions of sigmoidal functions at a rate that is independent of the
dimensionality $d$. This result has also been extended to the ReLU activation
function. In other words, any function in this class can be approximated using a
2NN with an error rate of $\bigO(n^{-1} \cdot C)$, where $n$ is the number of
nodes in the single hidden layer, and $C$ is a constant that depends only on the
smoothness of the target function. Although the convergence rate is independent
of the dimensionality $d$ of the input vector $x \in \R^d$, the constant $C$
could be dimension-dependent, as the Fourier transform is used in this approach.

This section is based on \cite{barronUniversalApproximationBounds1993}.

% \TONOTE{what is the probability here?}

\begin{theorem}
    % \cite[Theorem~1]{barronUniversalApproximationBounds1993}
    \label{thm:barron_1993_1}
    Let $U$ be a nonempty bounded domain in $\R^d$, $x_0 \in U$, and $C > 0$ a
    constant. For every function in the Barron class $\Gamma_C(U, x_0)$, every
    sigmoidal function $\sigma$, and every $n \in \Nat$, there exists a linear
    combination of sigmoidal function $f_n(x) = \sum_{j=1}^n a_j \sigma(b_j\tr
    x+c_j), a_j, c_j \in \R, b_j \in \R^d$ such that
    \begin{equation}
        \norm{f - f_n}_{\lp{2}(U)}\leq n^{-\frac{1}{2}} \cdot 2C
    \end{equation}
\end{theorem}

\begin{proof}
    The main idea behind the the proof is to show functions with finite Fourier
    moment are in the closure of the convex hull of the set of half planes.

    \textbf{Step 0} (\textit{Fix $x_0$ to $0$}): Let $x_0, x_1$ be two
    arbitrarily selected points in $U$, and $f \in \Gamma_C(U, x_0)$.  For any
    $\omega \in \mathbb{R}^d$, given $x_0, x_1$, we have
    \begin{equation}
        \abs{\omega}_{U, x_0} 
            = \sup_{x\in U}\norm{\spr{\omega}{x-x_0}} 
            \leq \sup_{x\in U}\norm{\spr{\omega}{x-x_1}} + \norm{\spr{\omega}{x_0-x_1}} 
            \leq 2\abs{\omega}_{U, x_1}
    \end{equation}

    Therefore, we have $\int_{\mathbb{R}^d} \abs{\omega}_{U, x_1}
    \abs{\fourier{f}(\omega)}\,d\omega \leq 2C$. If we have $\tilde{c} = c +
    \int_{\mathbb{R}^d} (e^{\omega\tr x_0} - e^{\omega\tr x_1}) d\omega$, then
    $f(x) = \tilde{c} + \int_{\mathbb{R}^d} (e^{i\omega\tr x} - e^{i\omega\tr
    x_1})$ with $\tilde{c} \leq 2C$.

    This shows that changing $x_0$ would only affect the constant in the RHS of
    Theorem \ref{thm:barron_1993_1} by a factor of at most two, i.e.
    $\Gamma_C(U, x_0) \subset \Gamma_{2C}(U, x_1)$. Therefore, we continue the
    proof assuming $x_0 = 0$.

    \textbf{Step 1} (\textit{Represent $f$ via Inverse Fourier Transform}): With
    the polar decomposition, we have $\fourier{f}(\omega) = e^{i\theta(\omega)}
    \cdot \abs{\fourier{f}(\omega)}$ where $\theta(\omega) \in \mathbb{R}$
    denote the magnitude decomposition. From the assumption, and the fact that
    $f$ is real-valued ($f: U \to \mathbb{R}$), the real-valued part of $f(x) -
    f(0)$ can be written as:
    \begin{align}
        \label{eq:barron_fouier_int}
        f(x) - f(0)
        & = \Re \int (e^{i\omega\tr x} - e^{i\omega\tr 0}) e^{i\theta(\omega)} \cdot 
        \abs{\fourier{f}(\omega)} \,d\omega \\
        & = \int_{\Omega}\Big(\cos(\omega\tr x + \theta(\omega)) - \cos(\theta(\omega))\Big)
        \abs{\fourier{f}(\omega)} \,d\omega \\
        & = \int_{\Omega} \frac{C_{f,U}}{\abs{\omega}_{U, 0}}\Big(\cos(\omega\tr x + \theta
        (\omega)) - \cos(\theta(\omega))\Big)\,d\mu_g \\
        & = \int_{\Omega} g(x, \omega)\,d\mu_g.
    \end{align}
    where we denote $\int_{\mathbb{R}^d} \abs{\omega}_{U} \cdot
    \abs{\fourier{f}(\omega)} d\omega \leq C$ by $C_{f, U}$.

    $\mu_g$ is a probability distribution $d\mu_g =
    \abs{\omega}_{U}/C_{f,U}\abs{\fourier{f}(\omega)} d\omega$, the integral is
    evaluated on $\Omega = \{\omega \in \R^d: \omega \not = 0\}$ and
    \begin{equation}
        g(x, \omega) = \frac{C_{f,U}}{\abs{\omega}_{U}}
        \Big(
            \cos(\omega\tr x + \theta(\omega)) - \cos(\theta(\omega))
        \Big).
    \end{equation}

    \textbf{Step 2} (\textit{$f(x) - f(0)$ is in the closure of the convex hull
    of $G_{\cos}$}): The integral form in (\ref{eq:barron_fouier_int}) shows that
    $f(x) - f(0)$ can be represented as an infinite convex combination of
    functions in the class

    \begin{equation}
        G_{\cos} = \Bigg\{
            \frac{\abs{\gamma}}{\abs{\omega}_{U}}
            \Big(
                \cos(\omega\tr x + b) - \cos(b)
            \Big):
                \omega \not= 0, \abs{\gamma} \leq C, b \in \mathbb{R} 
        \Bigg\}
    \end{equation}

    Suppose we have drawn $n$ samples ($\{\omega_i, i = 1,\dots, n\}$) from
    $\mu_g$, the expected norm in $\lp{2}(U, \mu_g)$ converges to zero as $n \to
    \infty$ by $\lp{2}$ law of large numbers. Therefore, there exist a convex
    combination of elements in $G_{\cos}$ that converges to $f(x) - f(0)$ in
    $\lp{2}$.


    \textbf{Step 3} (\textit{$G_{\cos}$ is in the closure of the convex hull of
    $G_{\textnormal{step}}$}): It is sufficient to check $g(z), z = \alpha x,
    \alpha = \omega/\abs{\omega}_{U}$ on $[-1, 1]$ for some $\omega \not= 0$. As
    $g(z)$ is a uniformly continuous sinusoidal function on $[-1, 1]$, it can be
    uniformly approximately by piecewise constant step function.

    Restricting $g(z)$ on $[0, 1]$, for a partition ${0 \leq p_1 \leq p_2 \leq
    \cdots \leq p_k = 1}$, define

    \begin{align}
        g_{k,+}(z) = \sum_{i=1}^{k-1} \Big(g(p_i) - g(p_{i-1})\Big) \cdot
        \indicator{\{z\geq p_i\}}(z)
    \end{align}

    Similarly, we can construct $g_{k,-}(z) = \sum_{i=1}^{k-1} (g(-p_i) -
    g(-p_{i-1})) \cdot \indicator{\{z\leq -p_i\}}(z)$, resulting in a sequence
    of piecewise step function on $[-1, 1]$ uniformly close to $g(z)$. We have
    $g(z) = g_{k,+}+g_{k,-}$, a linear combination of step function (or
    heaviside function) and the sum of the coefficients is bounded by 2C (The
    sum of coefficients of $g_{k,+}$ is bounded by $C$ as a result of the
    derivative of $g$ bounded by $C$, so does $g_{k,-}$ and hence $2C$).

    We can see that functions $g(z)$ are in the closure of the convex hull of
    the step functions (by Lemma 1 in
    \cite{barronUniversalApproximationBounds1993})

    By substituting $z = \frac{\omega}{\abs{\omega}_{U}} x$, we have $G_{\cos}
    \subset G_{\textnormal{step}}$,
    \begin{equation}
        G_{\textnormal{step}} = \Bigg\{
            \gamma\indicator{\{\alpha x-t\}}(x):
            \abs{\gamma} \leq 2C,
            \abs{t} \leq 1,
            \abs{\alpha}_{U} = 1
        \Bigg\}.
    \end{equation}

    \textbf{Step 4} (\textit{Closure of $G_{\phi}$}): There exists a sequence of
    sigmoidal functions $\phi(\abs{c}(\alpha x - t))$, as $\abs{c} \to \infty$,
    they converge to step functions pointwise (except at points where $\alpha x
    - t = 0$). If we introduce a measure $\mu$ that has zero measure at those
    points, previous statement on $G_{\cos} \subset G^{\mu}_{\textnormal{step}}$
    still holds on $\{\abs{t} \leq 1: \alpha x - t \not=0\}$ given a particular
    $\alpha$. We subsequently have convergence in $L_2(U, \mu)$ by the Dominated
    Convergence Theorem, which implies that $G^{\mu}_{\textnormal{step}} \subset
    G_{\phi}$.

    Finally, we arrive at the following relationship since the closure of a
    convex set is also convex \eqref{def:closed_convex_hull}
    \begin{equation*}
        \Gamma_{U, x_0} \subset \closure{G_{\cos}} 
        \subset \closure{G_{\textnormal{step}}} \subset \closure{G_{\phi}}.
    \end{equation*}

    % \cite[\textit{Lemma~1}]{barronUniversalApproximationBounds1993}: If $f$ is


    % Lemma 1 in \cite{barronUniversalApproximationBounds1993} showed that
    % function in a closure of the convex hull of a set in a Hilbert space can
    % be approximated with a sequence of functions from such closure and the
    % norm between the function and the sequences $\{f_i, i = 1, \dots, N\}$ are
    % bounded in a magnitude of $\bigO(N^{-1})$.

    It has been shown above that function $f(x) - f(0)$ is in the closure of
    the convex hull of $G_{\phi}$ where $\norm{g} \leq (2C)^2$ for every $g \in
    G_{\phi}$. Hence the $L_2$ norm of the approximation error is bounded for
    any choice of $C' > (2C)^2 - \norm{f(x) - f(0)}^2$ by Corollary
    \ref{cor:maurey}.
    % Suppose we restrict $t$ t

    % Suppose we now restrict $t$ to the continuity point induced by measure
    % $\mu$ in We can check that the functions in $G_{\textnormal{step}}$ are in the closure
    % of the convex hull of 

    % \textbf{Step 3} (\textit{Putting it together}): We can further show
    % $G_{\cos}$ are in the class of sigmoidal functions. 

    % Theorem 2 in \cite{barronUniversalApproximationBounds1993}, we have that

\end{proof}


\section{Connection with variation space}
\label{sec:fourier_variation_space}

As a expansion of nonlinear approximation of dictionary, the variation space is
defined in terms of a convex hull based on integral representations
\citep{parhiBanachSpaceRepresenter2021, parhiWhatKindsFunctions2022}. Suppose
that the dictionary is uniformly bounded in a separable Hilbert space
$(\mcal{H}, \norm{\cdot}_{\mcal{H}})$.
\begin{equation}
    \sup_{d\in\mathbb{D}} \norm{d}_{\mcal{H}} < \infty.
\end{equation}

If $G$ is a subset of $\mcal{H}$ and $c \in \R$, then we define the set
\begin{equation}
    cG = \{cg: g \in G\}.
\end{equation}

\begin{definition}[Varitaion norm]
    The variation norm of $\normVar{f}{\mathbb{D}}$ of a subset $\mathbb{D}$ of
    a linear space $X$ is defined for all $f \in X$ as
    \begin{equation}
        \normVar{f}{\mathbb{D}} := \inf \{
            c > 0: f/c \in \closure{\conv(\mathbb{D} \bigcup - \mathbb{D})}
        \}.
    \end{equation}

    This is the Minkowski functional of the closed symmetric convex hull of $\mathbb{D}$
    \begin{equation}
        \closure{\conv(\mathbb{D} \bigcup - \mathbb{D})} := \Bigg\{ 
            \sum_{j=1}^n a_j d_j: n \in \Nat, d_j \in \mathbb{D}, 
            \sum_{j=1}^n \abs{a_j} \leq 1
        \Bigg\}.
    \end{equation}
\end{definition}

\begin{definition}[Varitaion space]
    The variation space $\spaceVar{\mathbb{D}}$ is given by
    \begin{equation}
        \spaceVar{\mathbb{D}} := \{ 
            f \in \mathcal{H}: \normVar{f}{\mathbb{D}} < \infty
        \}.
    \end{equation}
\end{definition}

By the definitions, the following elementary properties of the variation space
hold.
\begin{proposition}
    \label{prop:spaceVar_properties}
    Let $\mathbb{D}$ be a uniformly bounded subset of a Hilbert space $\mcal{H}$
    \begin{enumerate}
        \item $\closure{\conv(\mathbb{D} \bigcup - \mathbb{D})} = \{
            f\in\mcal{H}: \normVar{f}{\mathbb{D}} \leq 1
        \}$
        \item $\norm{f}_{\mcal{H}} \leq \normVar{f}{\mathbb{D}} \cdot
        \sup_{d\in\mathbb{D}} \norm{d}_{\mcal{H}}$
        \item $\spaceVar{\mathbb{D}}$ is a Banach space equipped with norm
        $\normVar{f}{\mathbb{D}}$
    \end{enumerate}
\end{proposition}

\begin{proof}
    (1) and (2) is clear from the previous definitions. In order for
    $\spaceVar{\mathbb{D}}$ to be a Banach space, we need to show
    $\spaceVar{\mathbb{D}}$ is complete with norm $\normVar{\cdot}{\mathbb{D}}$.

    Let $\{f_n\}$ be a Cauchy sequence w.r.t. $\normVar{\cdot}{\mathbb{D}}$. By
    (2), this automatically implies that $f_n \to f$ in $\mcal{H}$ so the
    sequence is Cauchy w.r.t. $\norm{\cdot}_{\mcal{H}}$.
\end{proof}


\begin{proposition}
    Let $(\mcal{H}, \norm{\cdot}_{\mcal{H}})$ be a separable Hilbert space and
    $f$ be a function in $\mcal{H}$. Suppose that a sequence $\{f_n\}$ in
    $\mcal{H}$ where $f_n \in \Sigma_{n}(\mathbb{D}, M)$
    \begin{equation}
        \Sigma_n(\mathbb{D}, M) := \Bigg\{
            f = \sum_{j=1}^n \alpha_j d_j: 
            d_j \in \mathbb{D} \textnormal{ and } 
            \sum_{j=1}^n \abs{\alpha_j} \leq M, \quad 
            n \in \Nat, \alpha_j \in \R
        \Bigg\}.
    \end{equation}
    $f_n \to f$ in $\mcal{H}$ for some fixed $M < \infty$. Then $f$ is in the
    variation space $\spaceVar{\mathbb{D}}$ and its variation norm is bounded by
    $M$
    \begin{equation}
        f \in \spaceVar{\mathbb{D}}, \quad \normVar{f}{\mathbb{D}}\leq M.
    \end{equation}
\end{proposition}

\begin{proof}
    Without loss of generality, we only need to prove for $M = 1$.

    From the definition, $\Sigma_n(\mathbb{D}, 1) \subset
    \closedConvexHull{\mathbb{D}}$ for every $n \in \Nat$. 
    
    Therefore, $f_n \in \closedConvexHull{\mathbb{D}}$. Note that the set is
    closed, and we immediately have $f \in \closedConvexHull{\mathbb{D}}$.
    Hence the $\normVar{f}{\mathbb{D}} \leq 1$ and the proof is completed.
\end{proof}


Here we would like to connect $\mathbb{D}$ with the integral representation. An
integral representation of a function $f$ using dictionary $\mathbb{D}$ is
\begin{equation}
    f = \int_{\mathbb{D}} \inclusionMap{\mathbb{D}}{\mcal{H}} \,d\mu.
\end{equation}

Here, $\mu$ is a signed Borel measure on $\mathbb{D}$ with finite variation on $\mathbb{D}$:
\begin{equation}
    \label{eq:measure_set_variation}
    \norm{\mu}_{\mathbb{D}} := \sup_{\substack{
            g: \:\mathbb{D} \to [-1, 1] \\ g \textnormal{ is measurable}
        }
    } \int_{\mathbb{D}} g\,d\mu < \infty.
\end{equation}

\begin{remark}
    The existence of $\mu$ by is detailed in \cite[Proposition
    2]{siegelCharacterizationVariationSpaces2022} and \citep[Chapter
    4]{diestelSequencesSeriesBanach1984}
\end{remark}

\begin{definition}[Inclusion map]
    \label{def:inclusionMap}
    Let $A \subseteq B$. The injection $\inclusionMap{A}{B}: A\to B$ is an
    inclusion map if
    \begin{equation}
        \inclusionMap{A}{B}(a) = a, \quad \forall a \in A.
    \end{equation}
\end{definition}

\begin{lemma}
    \label{lemma:compact_set_integral_representation}
    Let $\mathbb{D}$ be a compact subset on $\mcal{H}$. Then $f \in
    \spaceVar{\mathbb{D}}$ if and only if there exists a Borel measure $\mu$ on
    $\mathbb{D}$ such that
    \begin{equation}
        f = \int_{\mathbb{D}} \inclusionMap{\mathbb{D}}{\mcal{H}} \,d\mu
    \end{equation}
    where $\inclusionMap{\mathbb{D}}{\mcal{H}}$ is an inclusion map from
    $\mathbb{D}$ to $\mcal{H}$. Furthermore, the variation norm of $f$ is given
    by the infimum of measure's variation over all the signed Borel measure on
    $\mathbb{D}$:
    \begin{equation}
        \normVar{f}{\mathbb{D}} = \inf_{\mu}\Big\{
            \norm{\mu}_{\mathbb{D}}: 
            f = \int_{\mathbb{D}} \inclusionMap{\mathbb{D}}{\mcal{H}} \,d\mu
        \Big\}.
    \end{equation}
\end{lemma}

\begin{definition}[Spectral dictionary]
    The spectral dictionary of order $s \in \Nat^0$ is given by
    \begin{equation}
        \mathbb{F}_s := \Bigg\{ 
            (1 + \abs{\omega})^{-s} \cdot e^{2\pi i\omega\tr x}: 
            \omega \in \R^d
        \Bigg\}
    \end{equation}
\end{definition}

\begin{theorem}[Equal norm]
The Fourier-analytic Barron space is equivalent to the variation space
constructed with $\mathbb{F}_s$
\begin{equation}
    \bspace{\mathcal{F}, s} = \spaceVar{\mathbb{F}_s}.
\end{equation}

Furthermore, the spectral norm is equivalent to the variation norm:
\begin{equation}
    \normVar{f}{\mathbb{F}_s} =
    \inf_{f_{e\mid U} = f} \int_{\R^d} 
    (1+\abs{\omega})^s \abs{\fourier{f}} \,d\omega.
\end{equation}
\end{theorem}

\begin{proof}
    One can find a proof in the Section 5 of
    \cite{siegelCharacterizationVariationSpaces2022}.
\end{proof}

\section{Improved rate via Heaviside function}
\label{sec:improved_heaviside}

In the following sections, we will discuss how the approximation error rates can
be improved by imposing stricter conditions on the smoothness of the dictionary
set or by ensuring its compactness. The key idea is to derive the $n$-th dyadic
entropy number corresponding to the dictionary $\mathbb{D}$. Maurey’s Theorem
\ref{thm:maurey} provides a bound for any compact dictionary in a Hilbert space.

The first instance of improvement in this area was reported in
\cite{makovozRandomApproximantsNeural1996}, where the compactness of the
dictionary of Heaviside functions was used and showed that the error can be
improved to $\bigO(n^{\frac{1}{2} - \frac{1}{p \cdot d}})$ in $\lp{p}(\Omega)$,
$p < \infty$. More recently, \cite{klusowskiApproximationCombinationsReLU2018a}
demonstrated that ReLU and ReLU\textsuperscript{2} networks can also be made
compact by controlling the $\lp{1}$ and $\lp{\infty}$ norms of their parameters.
In the latter case, the inner parameters $b\in\R^d$ in
\eqref{eq:improved_fourier_dict} are suitably constrained to ensure compactness.

Our discussion will primarily be based on the works of
\cite{maUniformApproximationRates2022, siegelSharpBoundsApproximation2022,
klusowskiApproximationCombinationsReLU2018a}, unless we explicitly cite other
sources.

\begin{theorem}
    % \cite[Theorem 3]{makovozRandomApproximantsNeural1996}
    \label{thm:improve_barron}
    Let $U$ be bounded set on $\R^d$. Let $f: U \to \R$ be a function and $V$ be
    the closure of all functions $f: U \to \R$ of the following form
    \begin{equation}
        V = \closure{
            \{f(x) = \sum_{j=1}^n a_j H(b_j\tr x + c_j) \quad 
            \sum a_j \leq 1, \abs{b_j} = 1, c_j \in \R \}
            }
    \end{equation}
    where $H$ is the Heaviside function.

    Then there exists a finite linear combination $f_n$ 
    \begin{equation}
        f_n = \sum_{j=1}^n a_j \sigma(b_j\tr x + c_j), b_j \in R^d, a_j, c_j \in \R
    \end{equation}
    where $\sigma(x)$ is a sigmoidal function from $\R$ to $\R$.

    For any $f \in V$ and any $n \in \Nat$ such that 
    \begin{equation}
        \norm{f- f_n}_{\lp{p}(U)} \leq C n^{-\frac{1}{2} - \frac{1}{p \cdot d}}
        \quad 1 \leq p < \infty
    \end{equation}
    where $C$ is a constant dependent only $U$, $p$.
\end{theorem}

% In some literature, V is represented as the dictionary

Note that this theorem does not cover the case $p = \infty$. However,
\cite{barronUniversalApproximationBounds1993} has already showed that $f\in V$
are dense w.r.t. supremum norm ($\norm{f - f_n}_{\lp{\infty}(U)} =
\bigO(n^{-1/2})$), which implies $\norm{f-f_n}_{\lp{p}(U)}$ for all $\lp{p},
1 \leq p < \infty$.

As the dimensionality $d$ increases, this rate approaches to the original rate
and therefore this theorem is insignificant for high dimensionality. It is
sufficient to prove the case $\sigma$ is the Heaviside function. It is easy to
check $\sigma(\lambda t) \to H(t)$ as $\lambda \to \infty$ and $\sigma(\lambda
t) \to H(t)$ as $\lambda \to -\infty$. On a closed interval $[-u, u]$ on $\R$,
note that the difference $H(t) - \sigma(\lambda t)$ is still bounded everywhere.
Therefore, we can see that distance between $H$ and $\sigma$ can be made
arbitrarily small on the space $\lp{d}(\R)$ with a sufficiently large $b$.

\begin{proof}
    We denote the set of sigmoidal functions
    \begin{equation}
        A = \{\sigma(b,c): \sigma(b,c) = \sigma(b\tr x + c), \quad 
        b\in\R^d, c\in\R\}
    \end{equation}

    As we already knew from \cite{barronUniversalApproximationBounds1993} that
    $V$ is the closure of the convex, symmetric hull of $A$ in $\lp{p}(U)$
    \begin{equation}
        V = \closure{\conv(A \bigcup - A)}.
    \end{equation}
    
    Now it remains to finding an estimate for the entropy of $A$. Without loss
    of generality, only the case $\sigma$ with $\abs{b} = 1$ is considered. We
    can assume $U$ is inside a ball with a suitable radius $r$ since $U$ is
    bounded subset in $\R^d$. Then this implies $\abs{c} \leq r$ as $\sigma(b\tr
    x + c)$ would be ones or zeros over all $U$. Suppose $b_0, b_1, c_0, c_1$
    and $\abs{b_0 - b_1} \leq \epsilon$ and $\abs{c_0 - c_1} \leq \epsilon$ for
    some $\epsilon < 0$. It is easy to check that 
    \begin{equation}
        \sup_{x\in U} \norm{\sigma({b_0, c_0}) - \sigma({b_1, c_1})}_2
            \lesssim \sqrt{\epsilon}
    \end{equation}
    in $\lp{2}$ with some constant independent of $d$.
    
    By the volume ratio argument
    \citep{vandervaartWeakConvergenceEmpirical1996}, one can obtain a
    $\bigO(\sqrt{\epsilon})$-net for $A$ in $\lp{2}(U)$ if we are able to find a
    $\epsilon$-net for the set $P:= \{(b,c)\in \R^{d+1}: \abs{b} = 1, \abs{c}
    \leq r\}$. To build a $\epsilon$-net for the sphere $\abs{b}=1$,
    $\bigO(\epsilon^{1-d})$ elements is needed for the sphere $\{v \in \R^d,
    \abs{v}=1\}$. An interval $[-r,r]$ requires $\bigO(\epsilon^{-d})$ elements.
    Therefore, a $\epsilon$-net of $\bigO(\epsilon^{-2d})$ elements can be
    constructed for $A$ and hence the covering number for $A$ is of the order
    $\bigO(\epsilon^{-\frac{1}{2d}})$.

    The statement then follows the Corollary on~\cite[p.
    104]{makovozRandomApproximantsNeural1996}.

    % We denote by $D$ to avoid clutters.
    % \begin{equation}
    %     \Sigma = \{\sum_{j=1}^m a_j \indicator{b\tr x + c}, 
    %     \quad \sum_j \abs{a_i} \leq 1, \abs{b_j} = 1,
    %     b\in\R^d,
    %     a, c\in\R\}
    % \end{equation}
    % An estimate of entropy of $\Sigma$ w.r.t. the $\lp{q}$ norm.

    % It is sufficient to prove (10) only for the case s=_. Indeed, if s is an
    % arbitrary sigmoidal function, then s(*t) Ä _(t), * Ä +􏱶, uniformly on
    % every set |t|􏱵a>0; on [&a,a] the difference _(t)&s(*t) remains bounded. It
    % follows that &_v, b&sv, b&Lq(D) can be made arbitrarily small by taking a
    % sufficiently large &v&. For s=_ we can use (8) since obviously V=(A_)c. We
    % need an estimate for =n(A_). We may consider only the _v, b with |v|=1. If D
    % is contained in some ball |x|􏱴r, then we may assume that |b|􏱴r for
    % otherwise _v, b is identically 1 or 0 on D. Suppose that |v&v1|<=, |b&b1|<=
    % for some =>0. If v=v1 and, say, b>b1 , then _v, b &_v1, b1 is equal to \1 on
    % the strip &b􏱴vx􏱴&b1 of width 􏱴=, and to zero elsewhere. Similarly, if
    % b=b1, then _v,b&_v1,b1{0 only on a strip of width O(=). It follows that
    % &_v,b&_v1,b1&􏱴C-= in L2. (Here and below C stands for various con- stants
    % independent of n). Therefore we obtain an O(-=)-net for A_ in L2(D) if we
    % find an =-net for the set P :=[(v, b) # Rd+1 : |v|=1, |b|􏱴r]. By a standard
    % volume ratio argument, one needs O((1􏱩=)d&1) elements to build an =-net for
    % the sphere |v|=1 and O(1􏱩=) elements for the interval [&r, r], which gives
    % O(=&d ) elements for P. Consequently, one can find an =-net for A_ in L2
    % consisting of O(=&2d) elements. Thus =n(A_)= O(n&1􏱩(2d)), and (10) now
    % follows from (8).


\end{proof}

\section{Improved rate with higher smoothness index}

In section \ref{sec:approximation_rate_fouier}, an error rate of
$\bigO(n^{-1/2})$ is obtained for functions in $\bspace{\mcal{F}, 1}$ using $n$
elements from the dictionary
\eqref{eq:dict_represent}
\begin{align}
    \label{eq:improved_fourier_dict}
    \mathbb{D} = \{
        \sigma(b\tr x + c), \quad b\in\R^d, c\in\R
    \}.
\end{align}
where $\sigma$ is a sigmoidal function.

The smoothness of a function $f$ is expressed through its \textit{first} Fourier
representation and controlled via the spectral condition $v_{f,s}$
\eqref{eq:spectral_condition}. In particular, how ``oscillating'' or
``fluctuating'' of a function $f$ is measured by the mean of the norm of the
frequency vector weighted by the Fourier magnitude distribution. Naturally, we
would like to extend the findings with tighter restriction on the smoothness and
ideally decrease the error rate $\bigO(n^{-1/2})$. Tighter rates of
approximation is made possible with stricter conditions on the Barron spectral
norm while bounding the inner parameter $b\in\R^d$ in
\eqref{eq:improved_fourier_dict}.

It it shown in section \ref{sec:fourier_variation_space}, the Fourier analytic
Barron space $\bspace{\mcal{F},s}$ is equivalent to the variation space
$\spaceVar{\mcal{F}_s}$ of the dictionary:
\begin{equation}
    \mathbb{F}_{s} := \{
        (1 + \abs{\omega})^{-s} e^{i\omega\tr x}: \omega \in \R^d
    \}.
\end{equation}

This implies that we can apply Maurey's argument regarding $n$-term
approximation using dictionaries. Improvements in this direction
\cite{siegelCharacterizationVariationSpaces2022,
siegelHighOrderApproximationRates2021,
siegelSharpBoundsApproximation2022,klusowskiRiskBoundsHighdimensional2018a} is
concerned about calculating the metric entropy of the convex hull. 

We define the collection of functions which can be expressed as linear
combinations of elements from $\mathbb{F}_s$
\begin{equation}
    \Sigma_n := \Bigg\{
        \sum_{j=1}^n \alpha_j d_j: 
        d_j \in \mathbb{F}_s \textnormal{ and } 
        \sum_{j=1}^n \abs{\alpha_j} \leq M, \quad 
        n \in \Nat, \alpha_j \in \R
    \Bigg\}.
\end{equation}

\begin{theorem}[Approximation in $\lp{\infty}$ with bounded coefficients]
    Let $\Omega = [0,1]^d$ and $s > 0$. If $f \in \mcal{K}(\mathbb{F}_s)$, then
    for $n \in \Nat$, there exists a $f_n \in \Sigma_n$ such that
    \begin{equation}
        \norm{f-f_n}_{\lp{\infty}(\Omega)} \lesssim 
        n^{-\frac{1}{2}-\frac{s}{d}} \sqrt{\log{n}} \norm{f}_{\bspace{\mcal{F},s}}.
    \end{equation}

\end{theorem}

\begin{proof}
    One can find the proof in
    \cite{klusowskiApproximationCombinationsReLU2018a,siegelSharpBoundsApproximation2022}.
\end{proof}

% A smoothness property of the function to be approximated is expressed in terms
% of its Fourier representation. In particular, an average of the norm of the
% frequency vector weighted by the Fourier magnitude distribution is used to
% measure the extent to which the function oscillates. In this Introduction, the
% result is presented in the case that the Fourier distribution has a density that
% is integrable as well as having a finite first moment. Somewhat greater
% generality is permitted in the theorem stated and proven in Sections III and IV.

% In addition to the smoothness property ensured via the finite first moment,


\section{Approximation with ReLU\textsuperscript{k} activation function}

This section focuses on the problem of approximating functions $f\in
\mcal{K}(\mathbb{F}_s)$ uniformly using 2NN with the ReLU\textsuperscript{k}
activation function. In the previous section, we examined the approximation
rates for ReLU and Heaviside networks in the space $\mcal{K}(\mathbb{F}_s)$. In
this section, we extend that analysis to include the approximation by
ReLU\textsuperscript{k} networks. 

This part builds on the research by
\cite{klusowskiApproximationCombinationsReLU2018a}.
\cite{siegelHighOrderApproximationRates2021} obtained the error rate in $\lp{2}$
and the result is extended to $\lp{\infty}$ in
\cite{maUniformApproximationRates2022}.


We define 
\begin{equation}
    \Sigma_n^k := \Bigg\{
        \sum_{j=1}^n a_j \sigma_k(b_j\tr x + c_j): 
        b_j \in \R^d, a_j, c_j \in \R
    \Bigg\}.
\end{equation}

\begin{theorem}
    Let $\Omega = [0,1]^d$, $k \geq 1$, and $f \in \mcal{K}(\mathbb{F}_s)$. The
    smoothness index $1 < s \leq (d+1)k + 1$. Then for a large $n \in \Nat$,
    there exists a finite linear combination of elements $f_n \in \Sigma_n^k$
    such that
    \begin{equation}
        \norm{f-f_n}_{\lp{\infty}(\Omega)} \lesssim 
        n^{-\frac{1}{2} - \frac{s-1}{d+1}}
        (\log{n})^{1 + \frac{(s-1)t}{d+1}}
        \norm{f}_{\bspace{\mcal{F},s}}
    \end{equation}
    where $t=0$ when $s < (d+1)k+1$ and $t=1$ when $s = (d+1)k+1$.
\end{theorem}

An error rate of $\bigO(n^{-\frac{1}{2}- \frac{2k+1}{2(d+1)}})$ is obtained for
$\lp{p}$ norm in Theorem 3 \citep{siegelHighOrderApproximationRates2021}.

In the case where the target functions are highly smooth (large $s$), we
obtained the error rate below.

\begin{theorem}
    Let $\Omega = [0,1]^d$, $k \geq 1$, and $f \in \mcal{K}(\mathbb{F}_s)$. The
    smoothness index $s \geq (d+1)(k+1)$. Then for a large $n \in \Nat$, there
    exists a finite linear combination of elements $f_n \in \Sigma_n^k$ such
    that
    \begin{equation}
        \norm{f-f_n}_{\lp{\infty}(\Omega)} \lesssim 
        n^{-(k+1)} (\log{n})^t \norm{f}_{\bspace{\mcal{F},s}}
    \end{equation}
    where $t=0$ when $s < (d+1)(k+1)$ and $t=1$ when $s = (d+1)(k+1)$.
\end{theorem}

\begin{remark}
    The implied constants denoted by $\lesssim$ can be seen to be only depend on
    $s, k, p, d$ but not on the target function $f$ or the number nodes $n$.
    Furthermore, the constant might depend on the dimension $d$. This dependence
    could be exponential, i.e. $C^d$ for some $C$.
\end{remark}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "MasterThesisSfS"
%%% End: 
