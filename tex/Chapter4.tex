\chapter{Implicit Regularization}

Much progress has been made in recent years in the estimation of the population
risk of the population risk, specifically under explicit regularization. Though
numerical experiments has well documented \TOCITE the behavior that deep neural
networks model and shallow neural neural model are ``biased'' or ``skewed''
towards a more regularized, smooth output without explicitly setting
regularization in the form of explicit penalty term or through various
optimization techniques such as drop-outs, weight decay, etc.

In the case of explicit regularization, 

\section{Prevalence of Implicit Regularization}




\section{GD based}

Gradient descent ”favors”: min norm or max margin solution w/o explicit regularization

\section{qualitative work}

mainly on low-Complexity ones, mainly in the context of regression
and mostly on infinitely wide network.

\section{quantitative work}

1d-1d function 

\section{Norm of the weights}

Function spaces associated with shallow NN, (bounded norm)

\section{Learning rate}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "MasterThesisSfS"
%%% End: 
