\chapter*{Notation}
\label{c:Notation}

Throughout this paper, the set of all natural numbers without zero is denoted by
$\Nat = \{1,2,3,\dots\}$ and the set of all natural numbers including zero is
denoted by $\Nat^0 = \Nat \setminus \{0\}$. 

For a arbitrary set $S$, we write $\abs{S} := \# S \in \Nat^0 \bigcup\,
\{\infty\}$ as the number of elements of $S$.

For a set $S$ in a space $P$, we write the closure of $S$ in $P$ as
$\closure{S}$. The convex hull of $S$ is denoted by $\conv(S)$.

The standard scalar product of $x,y\in\R^d$ will be written as $x\tr y =
\spr{x}{y} = \sum_{i=1}^d x_iy_i$. We use $\abs{x}$ and $\norm{x} := \sqrt{x\tr
x}$ to denote $\lp{1}$ and $\lp{2}$-norm for convenience. $\norm{\cdot}_{p}$ is
used for norm in $\lp{p}$ space, $1 \leq p \leq \infty$.
$\norm{\cdot}_{\mcal{H}}$ denotes the norm for a normed space $(\mcal{H},
\norm{\cdot}_{\mcal{H}})$. We would use the notation
\begin{equation}
    \norm{f - g}_{G} := \sup_{g\in G} \abs{f - g}
\end{equation}
where the supremum is taken over all functions $g \in G$.

We write $f(x) \lesssim g(x)$ if $f(x) \leq C g(x)$ for a constant $C>0$ and for
all $x \in X$. We will explicitly state the constant if $C$ depends on $f$ or
any other relevant conditions.

\section*{List of Abbreviations}

\printglossary

\newglossaryentry{ann}
{
    name=ANN,
    description={Artificial Neural Network}
}

\newglossaryentry{cod}{
    name=CoD,
    description={Curse of Dimensionality}
}

\newglossaryentry{pde}{
    name=PDE,
    description={Partial Differential Equation}
}

\newglossaryentry{tpt}{
    name=TPT,
    description={Terminal Phase of Training}
}
\newglossaryentry{2nn}{
    name=2NN,
    description={Two-Layer Neural Network}
}

\newglossaryentry{dag}{
    name=DAG,
    description={Directed Acyclic Graph}
}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "MasterThesisSfS"
%%% End: 
