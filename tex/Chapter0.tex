\chapter{Introduction}
\label{ch:introduction}

\numberwithin{equation}{chapter}

% talk about NN is good in many fields
% its history
% lack good theory

Neural networks (\gls{nn}s) have emerged as a standard method for numerical
approximation and learning algorithms due to their empirical success in various
fields such as computer vision and natural language processing
\citep{shalev-shwartzUnderstandingMachineLearning2014}. Although the history of
NN dates back to late 1940s, its popularity only emerged from the
state-of-the-art performance in a variety of learning domains. However, despite
their popularity, NN-based learning is often viewed as "black magic" due to the
lack of convincing and rigorous theoretical explanations, particularly in the
context of hyperparameter tuning and architecture design. The fundamental
challenge of these learning tasks revolves around approximating an unknown
complex function from limited observed data points. A comprehensive
understanding of the approximation ability of NNs can provide a partial
explanation for their success in practice.

% cod

The term \textit{curse of dimensionality} (\gls{cod}) was coined by
\cite{bellmanTheoryDynamicProgramming1952} almost 70 years ago to describe the
overwhelming complexity associated with solving a multi-stage processes through
dynamic programming. In the scope of approximation theory, it amounts to the
exponentially growing number of data points required to maintain the accuracy.
Such dimensionality cursed problems have appeared in computational and applied
mathematics. For approximations in high dimensions, the accuracy will drop
exponentially as dimensionality increases but the deep learning based numerical
application methods for partial differential equation (\gls{pde}) indicates
satisfactory performance in \cite{eDeepRitzMethod2017,
eDeepLearningbasedNumerical2017,beckMachineLearningApproximation2019} but there
is a absence of rigorous mathematical results to demonstrate this conjecture.

% https://arxiv.org/pdf/2205.14421.pdf

% understanding helps solves other questions

Naturally, the empirical success in numerical applications calls for a
theoretical explanation. The understanding of the advantages of using NN over
traditional methods as an approximation tool is crucial for explaining their
performance. Classical approximation methods include polynomials, wavelets,
splines, and sparse approximation from bases, and dictionaries
\citep{devore_1998}. Additionally, it can facilitate the integration of their
use in various domains, such as numerical methods for solving PDEs.

% we only concern about 2NN

The variety of architectures in NN presents a challenge in characterizing their
approximation properties due to the wide variety of architectural choices
available, such as the width, depth, activation functions, and connectivity. To
address this challenge, two main directions concerning the approximation power
of NN have emerged in the community. Let $\nnFunc{W}{L}{\sigma}$ be the
collections of the outputs of NN of width $W$, depth $L$, and a activation
function $\sigma$. Similar to a classical approximation scheme, one can consider
the output of 2NN, $\nnFunc{W}{1}{\sigma}$, where the depth is fixed at $L=1$
and $W$ can grow to infinity. Alternatively, one can investigate deep neural
networks by fixing the width and allowing the depth to increase to infinity. We
will not dig too deeply into the deep neural networks and this thesis is
confined on the performance of 2NN as an approximation tool.

% what is the question of density

One of the first question in the approximation of 2NN is the question of
\textit{density}: what conditions must 2NN satisfy in order that an arbitrary
target function can be approximated arbitrarily well by 2NN:

For any $f \in C(\R^d)$, any compact set $U$ of $\R^d$, and an arbitrary
accuracy $\epsilon > 0$, there is a $g$ produced by 2NN, $g \in
\nnFunc{W}{1}{\sigma}$ such that 
\begin{equation}
    \sup_{x \in U} \abs{f(x) - g(x)} < \epsilon.
\end{equation}

Since the late 1980s, it has been established that NNs are universal
approximators \citep{carrollConstructionNeuralNets1989,
cybenkoApproximationSuperpositionsSigmoidal1989,
hornikMultilayerFeedforwardNetworks1989a,
funahashiApproximateRealizationContinuous1989}. Using the Hahn-Banach Theorem
and the Riesz Representation Theorem,
\cite{cybenkoApproximationSuperpositionsSigmoidal1989} stated the density
property required is $\sigma$ is sigmoidal, (defined in \eqref{def:sigmoidal}).
Independent from Cybenko, Funahashiâ€™s proof uses the result of
\cite{irieCapabilitiesThreelayeredPerceptrons1988} on the integral
representation $\lp{1}$ functions, using a kernel which can be expressed as a
difference of two sigmoidal functions. In paper by
\cite{hornikMultilayerFeedforwardNetworks1989a}, the activation function needs
to monotone and bounded, which allows noncontinuous $\sigma$. Additionally, the
summation and product of activation functions are allowed in their statement. In
\cite{jonesSimpleLemmaGreedy1992}, a constructive method with a bounded
sigmoidal function is sufficient to ensure the density. A sequence of papers
adopted various techniques to attack this problem and yet the answer is
surprisingly simple, as shown by \cite{leshnoMultilayerFeedforwardNetworks1993}.
The necessary and sufficient condition on $\sigma$ for $\nnFunc{W}{1}{\sigma}$
to be \textit{dense} is that $\sigma$ must not be a polynomial. 

% one way to look at the NN approximation power
% F -> NN 

While the density question is an essential step in understanding the
approximation power of NNs, this fact alone does not provide a sufficient
explanation for why NNs are more effective than traditional approximation
methods, as methods using polynomials, splines, and wavelets are also universal
approximators. In particular, we need to consider the degree of approximation as
well as the possibility of developing stable algorithms to find the
approximates. The effectiveness of NN-based learning in high-dimensional
settings remains a mystery. To gain insights into the approximation abilities of
2NN, one can estimate the worst-case error rate of approximation by measuring
the performance of the network on a target function $f$ from a classical model
class. One then tries to address the following problem \ref{problem:1}.
\begin{problem}
    \label{problem:1}
    Given a target function $f$ in one of the classical model class such as unit
    balls of Lipschitz, Sobolev and Besov space, find the upper and lower bound
    for the approximation error with 2NN. 
\end{problem}

% other  way to look at the NN approximation power
% NN -> F

A different approach is to describe the class of functions that are \textit{well
approximated} by 2NN. This is generally more difficult and less straightforward.
By the success in practical numerical experiments, one could assume that the
model class should be quite large.
\begin{problem}
    \label{problem:2}
    Describe the model classes of functions that are guaranteed to be well
    approximated by NN.
\end{problem}

% Barron result

One of the celebrated results was introduced by Prof. Andrew
\cite{barronApproximationEstimationBounds1994}. Given a domain $U \subset \R^d$,
the model class $K$ consists of all functions $f$ in $\lp{2}(U)$ whose Fourier
transform $\fourier{f}$ satisfies
\begin{equation}
    \int_{\R^d} \abs{\omega} \abs{\fourier{f}(\omega)} \,d\omega < \infty.
\end{equation}

Initially, the approximation error was obtained for any sigmoidal activation
functions for $\lp{2}$ norm on $U$. For any $n \in \Nat$, there exists a a
linear combination of sigmoidal functions $f_n = \sum_{j=1}^n a_j \sigma(b_j\tr
x + c_j), a_j,c_j \in \R, b_j \in \R^d$ such that 
\begin{equation}
    \norm{f - f_n}_{\lp{2}(\Omega)} \leq C n^{-1/2}, \quad 
    f_n \in K.
\end{equation}

This result clearly extend to the \textit{rectified linear unit} (ReLU)
activation as wells since $\sigma_{\text{ReLU}}(x) - \sigma_{\text{ReLU}}(x-1)$
is a sigmoidal function.

% inspired by Barron

Inspired by Barron's result \citep{eRepresentationFormulasPointwise2020,
carageaNeuralNetworkApproximation2022}, many generalizations and
improvements have been made. A important generalization is given by
\cite{makovozRandomApproximantsNeural1996} where improved error rate is
$\bigO(n^{-1/2-1 / p\cdot d})$. This improvement relies on the fact that one can
reduce the number of terms needed in the approximation. 

As noted earlier, there is much interest in identifing new classes of functions
associated with 2NN. To honor Prof. Andrew Barron's contribution in the
understanding of neural nets, the term \textit{Barron space} was coined by many
but the notation of Barron spaces is not in consensus within the community, and
different terms have been given to describe the same model classes or spaces.
For the function spaces in which functions have finite Fourier moments,
~\cite{xuFiniteNeuronMethod2020} calls this model classes \textit{Barron
spectral spaces} while \cite{carageaNeuralNetworkApproximation2022} refers to
\textit{Fourier-analytic Barron space}.  For function spaces in which functions
admit an integral representation with a ReLU activation function,
\cite{eBarronSpaceFlowinduced2021} refer them simply as \textit{Barron spaces}
of different orders $p \in \{1, 2, 3, \cdots, \infty\}$. In some literature
including~\cite{carageaNeuralNetworkApproximation2022}, these spaces are named
\textit{infinite-width Barron spaces} associated with different activation
functions and the term \textit{classical Barron space} is reserved for those
associated with Heaviside function\footnote{Also called step function or unit
step function}.

To avoid confusion, we will use two definitions throughout
\begin{itemize}
    \item Fourier-analytic Barron spaces.
    \item infinite-width Barron spaces.\footnote{
        We limit the model classes to those associated with ReLU only. In the 
        case of Heaviside function, we denote the space still by the term 
        \hyperref[def:heaviside_space]{\textit{classical Barron space}}.
    }
\end{itemize}

Recently, \cite{siegelCharacterizationVariationSpaces2022} connect the variation
space introduced by \cite{parhiBanachSpaceRepresenter2021,
parhiWhatKindsFunctions2022}. It has been shown that the spectral Barron norm in
is equivalent to the variation norm of a dictionary. This allows us to properly
frame these results in the context of nonlinear approximation. The compactness
or smoothness condition can then be used for calculating the metric entropy of
the closed convex hull of said dictionaries.

% Let $\mcal{H}$ be a Hilbert space. We define a set (or dictionary) $\mathbb{D}
% \subset \mcal{H}$ such that each element is bounded $\sup_{d\in\mathbb{D}}
% \norm{d}_{\mcal{H}} \leq \infty$. Given such a dictionary, if function $f$ is in
% the closed convex, symmetric hull of $\mathbb{D}$, then there is a $g =
% \sum_{j=1}^n a_j d_j$ with $d_j \in \mathbb{D}$ such that
% \begin{equation}
%     \norm{f - g}_{\mcal{H}} \lesssim n^{-1/2}, \quad n \in \Nat.
% \end{equation}

% In applications, the phenomenon of implicit regularization is frequently observed [15, 27, 30, 29, 26, 22, 33]. Nonetheless, the theory behind it is still largely unexplored [26, 22, 33, 27]. 


The rest of this thesis is organized as follows. Chapter \ref{ch:preliminary}
introduces the question setup and the 2NN model. The question of
\textit{density} with 2NN is answered for any continuous functions from $\R^d$
to $\R$. We show that the 2NN are universal approximators with a activation
function that is not a polynomial when the number of nodes are allowed to grow
unlimited. In Chapter \ref{ch:fourier}, we consider the model classes of
functions (Fourier-analytic Barron spaces) that are guaranteed to be
\textit{well approximated} by 2NN. The smoothness restriction is enforced by
bounding the Fourier transform term of the functions. Chapter \ref{ch:infinite}
describes the Infinite-width Barron spaces in which each function is also well
approximated with the help of an integral representation. Furthermore, the
connection between these spaces and their variation spaces is included. Each
corresponding variation space is constructed using a compact dictionary of
functions. Moreover, we compare and articulate the relationship between the
model classes.

\numberwithin{equation}{section}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "MasterThesisSfS"
%%% End: 
