\chapter{Introduction}
\label{ch:introduction}

The term \textit{curse of dimensionality} (\gls{cod}) was coined by
\cite{bellmanTheoryDynamicProgramming1952} almost 70 years ago to describe the
overwhelming complexity associated with solving a multi-stage processes through
dynamic programming. In approximation, it amounts to the exponentially growing
number of data points required to maintain the accuracy. Such dimensionality
cursed problems has appeared in computational and applied mathematics. For
approximations in high dimensions, the accuracy will drop exponentially as
dimensionality increases but the deep learning based numerical application
methods for partial differential equation (\gls{pde}) indicates satisfactory
performance in \cite{eDeepRitzMethod2017,
eDeepLearningbasedNumerical2017,beckMachineLearningApproximation2019} but there
is a absence of rigorous mathematical results to demonstrate this conjecture.
Naturally, the success in applications like computer vision and pattern
detection even in the case of low data points calls for a theoretical
explanation.
% https://arxiv.org/pdf/2205.14421.pdf
 
A few analyses attempt to explain the success of learning nonlinear operators by
neural networks including DeepOnets with a focus on reducing the high/infinite
dimensional space to a low dimensional space.
\cite{lanthalerErrorEstimatesDeepOnets2022} rigorously proves that the CoD is
overcome when considering smooth functions with exponentially decaying
coefficients.

I also need to talk about something in PDE, monte carol sampling shit https://arxiv.org/pdf/1807.01212.pdf

Indeed, any supervised learning model suffers in principle from the curse of
dimensionality: under minimal assumptions on the function to be learnt,
achieving a fixed target generalisation error e requires a number of training
samples P which grows exponentially with the dimensionality d of input data ,
 https://openreview.net/pdf?id=sBBnfOFtPc

What is the teacher-student model

\section{Approximation and estimation scheme}

Here they author prove the CoD is overcome by a factor of $1/m$
https://arxiv.org/pdf/2205.14421.pdf

Approximation of functions in Ko space https://arxiv.org/pdf/2012.05451.pdf

\section{Classical approximation spaces}

Here I will need to talk about something like affine or spilne regression as the
bach paper

Polynomial spaces?

Ko spaces?

Repres thm https://pages.stat.wisc.edu/~wahba/ftp1/wahba.wang.2019submit.pdf

\section{RKHS and kernel regression shit}

I need to briefly go over some kernel regression stuff and talk a bit about its
connection with NN?

Also on the represnetation results

\subsection{Difference between representation theorem and approximation}

Also on the debate between linking kernel with NN.


\section{The setup of supervised learning}

Here I need to answer what is the functions?

The error

The norm

and the spaces

What is student teacher model ???

Here we briefly review the statistical learning framework and we refer readers to \cite{shalev-shwartzUnderstandingMachineLearning2014a} for details.

Under the batch learning settings, a \textit{training set} dataset $S$ is
accessible to the learner. Generally, it is assumed that $S = \{(x_1,y_1),
\dots, (x_n, y_n)\}$ of $n$ points independently and identically distributed
(i.i.d.) $\mathcal{X} \times \mathcal{Y}$ with some unknown distribution
$\mathcal{D}$. For simplicity, we only write the regression where the output
function $f: \mathcal{X} \to \mathcal{Y}$ with minimum expected error on $S$
with some loss function $L: \mathcal{Y} \to \R$:

\begin{equation}
    \label{eq:training_loss}
    L_{\mathcal{D}}(f) = \PRi{(x, y) \sim \mathcal{D}}{y - f(x)}
\end{equation}

The learner cannot minimize the expected loss $L_{\mathcal{D}}$ due to limited
sample points $n$ but an estimate of the expected loss of using training set $S$
can be evaluated:

\begin{equation}
    L_{S}(f) = \frac{1}{n} \cdot
\end{equation}

$L(f)$ and $\hat{L}(f)$ is used. A expected margin loss for any margin $\gamma$:

\begin{equation}
    L_{\gamma} = \PRi{sad}{sdsd}
\end{equation}


A minimized loss \eqref{eq:training_loss} does not guarantee a low expected
error and the simplest case would be the case where a trained predictor $f$ that
memorize $S$ and has zero $L_{s}(f)$. However, it is shown in \TOCITE{about over
fitting} training in deep networks (over-parameterized) beyond $0$ training
error, i.e. during the \textit{terminal phase of training} (\Gls{tpt}) \TOCITE.
This is in sharp contrast to the conventional belief in learning or regression
models where \textit{over-fitting} is discouraged. Interesting empirical
phenomena such as ``Nerual Collapse'' by \TOCITE sheds some lights into why deep
networks can escape or evade the complexity based techniques.

Add a section of over parameterization in NN.

In learning, we are interested in the size of NN or the numbers of parameters
required for an accuracy $\epsilon$. An \textit{generalisation error} is
difference $L_\mathcal{D}(f) - L_{S}(f)$ and this quantity shows the differnece between learning and memorizing.

As the predictor $f$ is chosen by the training algorithm using $S$, we would
like to obtain a bound that holds for the set of all possible functions.
Therefore, it is 

Add something from neural network theory????

Historically, linear regression \TOCITE was the method of choice in supervised
learning where the search for \textit{true} function is performed only within a
small subspace of all functions: $\mathcal{H}_{\text{linear}}$, the space of
linear functions. Despite its popularity and extensive usage in real-world
applications, the space of linear functions is often insufficient since the
\textit{true} relations between input $\mathcal{X}$ and $\mathcal{Y}$ often
involve non-linearity. Therefore, it is preferable to choose a \textit{larger}
or more expressive hypothesis space $\mathcal{H}$ so that these mappings
$\mathcal{X} \to \mathcal{Y}$ can be found.

This intuition that the target functions can be formalized as the Bayesian prior
knowledge \TOCITE. In theory, the prior distribution of $(X, Y)$ on $\mathcal{X}
\times \mathcal{Y}$ can be formulated as a probability measure over all
probability measures on $\mathcal{X} \times \mathcal{Y}$.  For example, one
could hypothesize that $f_{\text{True}}$ is mostly likely to be linear and
consider all the other functions equally likely. This is the case where the we
assign zero measure to all non-linear functions and assign all linear functions
the same weights. Note that the measure here is improper since it assigns
$\infty$ to the space of all linear functions $\mathcal{H}_{\text{linear}}$.
However, it is often difficult to articulate a realist intuition such as ``we
favors a smoother, simpler function rather than a more highly fluctuating
function''. These realist \textit{insights} or \textit{impressions} are
harder to formalize mathematically and the calculation of such problems within
the Bayesian framework is often intractable.

\TONOTE{Add something on LASSO, high dim regulzation stuffs}
Is Lasso or spline regression 


There have been great efforts in high-dimensional statistics where the idea
of smoothing or stabilizing can be incorporated through regularization. These
include LASSO, ridge regression with a sparsity-inducing norm. Yet, the shallow
as well as deep neural networks \cite{neyshaburSearchRealInductive2015,
maennelGradientDescentQuantizes2018, liLearningOverparameterizedNeural2019,
kuboImplicitRegularizationOverparameterized2019,
neyshaburImplicitRegularizationDeep2017} trained with standard algorithms
(gradient descent based applied to a loss $L$) are able to find solution close
to $f_{\text{True}}$ without explicit regularization including drop-outs,
weight-decay or early-stopping. This phenomenon is referred as \textit{Implicit
Regularization}.\footnote{ It is also defined as \textit{Implicit Bias} in
\cite{soudryImplicitBiasGradient2022} }
\cite{kuboImplicitRegularizationOverparameterized2019} has shown that the
(stochastic) gradient descent algorithm along with random initialization can
lead to surprisingly \textit{low} complexity of the over-parameterized networks.
\cite{maennelGradientDescentQuantizes2018} has shown in 2NN the with small
randomly initialized weights leads to ``simpler'' functions and
\cite{maennelGradientDescentQuantizes2018, neyshaburSearchRealInductive2015} has
empirically shown that the network size in the case of 1 hidden layer does not
behave as a regularization term (i.e. encourage ``simpler'' functions by the
predictors). In both setups, increasing the number of nodes results in a lower
test error and overall more general results even with overfitting with random
labels in \cite{neyshaburSearchRealInductive2015}. Therefore, a \textit{hidden}
notion of complexity must be present to account for lower complexity/better
generalization. Furthermore, this is proven by the phenomenon of training beyond
zero training loss \TOCITE $L_{\text{train}}$, i.e. the test error and
generalization improves despite $L_{\text{train}} = 0$.

The paradox of why 

% In applications, the phenomenon of implicit regularization is frequently observed [15, 27, 30, 29, 26, 22, 33]. Nonetheless, the theory behind it is still largely unexplored [26, 22, 33, 27]. 

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "MasterThesisSfS"
%%% End: 
