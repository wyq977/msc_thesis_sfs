% \chapter{Pre-requisite}

% This chapter provides a brief introduction on the materials required. 

% \section{Integral Transform}

% \section{Measure Theory}

% \section{Covering number}

% \section{Empirical Risk Minimization}

\chapter{Curse of dimensionality}

The term \textit{curse of dimensionality} (\gls{cod}) was coined by
\cite{bellmanTheoryDynamicProgramming1952} almost 70 years ago to describe the
overwhelming complexity associated with solving a multi-stage processes through
dynamic programming. In approximation, it amounts to the exponentially growing
number of data points required to maintain the accuracy. Such dimensionality
cursed problems has appeared in computational and applied mathematics. For
approximations in high dimensions, the accuracy will drop exponentially as
dimensionality increases but the deep learning based numerical application
methods for partial differential equation (\gls{pde}) indicates satisfactory
performance in \cite{eDeepRitzMethod2017,
eDeepLearningbasedNumerical2017,beckMachineLearningApproximation2019} but there
is a absence of rigorous mathematical results to demonstrate this conjecture.
Naturally, the success in applications like computer vision and pattern
detection even in the case of low data points calls for a theoretical
explanation.
% https://arxiv.org/pdf/2205.14421.pdf
 
A few analyses attempt to explain the success of learning nonlinear operators by
neural networks including DeepOnets with a focus on reducing the high/infinite
dimensional space to a low dimensional space.
\cite{lanthalerErrorEstimatesDeepOnets2022} rigorously proves that the CoD is
overcome when considering smooth functions with exponentially decaying
coefficients.

I also need to talk about something in PDE, monte carol sampling shit https://arxiv.org/pdf/1807.01212.pdf

Indeed, any supervised learning model suffers in principle from the curse of
dimensionality: under minimal assumptions on the function to be learnt,
achieving a fixed target generalisation error e requires a number of training
samples P which grows exponentially with the dimensionality d of input data ,
 https://openreview.net/pdf?id=sBBnfOFtPc

What is the teacher-student model

\section{Approximation and estimation scheme}


Here they author prove the CoD is overcome by a factor of $1/m$
https://arxiv.org/pdf/2205.14421.pdf

Approxmation of functions in Ko space https://arxiv.org/pdf/2012.05451.pdf

\section{A section on supervised learning and what not}

Here I need to answer what is the functions?

The error

The norm

and the spaces

What is student teacher model ???

Here we briefly review the statistical learning framework and we refer readers to \cite{shalev-shwartzUnderstandingMachineLearning2014a} for details.

Under the batch learning settings, a \textit{training set} dataset $S$ is
accessible to the learner. Generally, it is assumed that $S = \{(x_1,y_1),
\dots, (x_n, y_n)\}$ of $n$ points independently and identically distributed
(i.i.d.) $\mathcal{X} \times \mathcal{Y}$ with some unknown distribution
$\mathcal{D}$. For simplicity, we only write the regression where the output
function $f: \mathcal{X} \to \mathcal{Y}$ with minimum expected error on $S$
with some loss function $L: \mathcal{Y} \to \R$:

\begin{equation}
    \label{eq:training_loss}
    L_{\mathcal{D}}(f) = \PRi{(x, y) \sim \mathcal{D}}{y - f(x)}
\end{equation}

The learner cannot minimize the expected loss $L_{\mathcal{D}}$ due to limited
sample points $n$ but an estimate of the expected loss of using training set $S$
can be evaluated:

\begin{equation}
    L_{S}(f) = \frac{1}{n} \cdot
\end{equation}

$L(f)$ and $\hat{L}(f)$ is used. A expected margin loss for any margin $\gamma$:

\begin{equation}
    L_{\gamma} = \PRi{sad}{sdsd}
\end{equation}


A minimized loss \eqref{eq:training_loss} does not guarantee a low expected
error and the simplest case would be the case where a trained predictor $f$ that
memorize $S$ and has zero $L_{s}(f)$. However, it is shown in \TOCITE{about over
fitting} training in deep networks (over-parameterized) beyond $0$ training
error, i.e. during the \textit{terminal phase of training} (\Gls{tpt}) \TOCITE.
This is in sharp contrast to the conventional belief in learning or regression
models where \textit{over-fitting} is discouraged. Interesting empirical
phenomena such as ``Nerual Collapse'' by \TOCITE sheds some lights into why deep
networks can escape or evade the complexity based techniques.

Add a section of over parameterization in NN.

In learning, we are interested in the size of NN or the numbers of parameters
required for an accuracy $\epsilon$. An \textit{generalisation error} is
difference $L_\mathcal{D}(f) - L_{S}(f)$ and this quantity shows the differnece between learning and memorizing.

As the predictor $f$ is chosen by the training algorithm using $S$, we would
like to obtain a bound that holds for the set of all possible functions.
Therefore, it is 

Add something from neural network theory????

Historically, linear regression \TOCITE was the method of choice in supervised
learning where the search for \textit{true} function is performed only within a
small subspace of all functions: $\mathcal{H}_{\text{linear}}$, the space of
linear functions. Despite its popularity and extensive usage in real-world
applications, the space of linear functions is often insufficient since the
\textit{true} relations between input $\mathcal{X}$ and $\mathcal{Y}$ often
involve non-linearity. Therefore, it is preferable to choose a \textit{larger}
or more expressive hypothesis space $\mathcal{H}$ so that these mappings
$\mathcal{X} \to \mathcal{Y}$ can be found.

This intuition that the target functions can be formalized as the Bayesian prior
knowledge \TOCITE. In theory, the prior distribution of $(X, Y)$ on $\mathcal{X}
\times \mathcal{Y}$ can be formulated as probability measure over all a
probability measure on $\mathcal{X} \times \mathcal{Y}$.  For example, one could
hypothesize that $f_{\text{True}}$ is mostly likely to be linear and consider
all the other functions equally likely. This is the case where the we assign
zero measure to non-linear functions and treats all linear functions the same.
Here the measure is improper since it assigns $\infty$ to the space of all
linear functions $\mathcal{H}_{\text{linear}}$. However, it is difficult to
articulate a realist intuition such as ``we favors a smoother, simpler function
rather than a more highly fluctuating function''.

\tonote{Add something on LASSO, high dim regulzation stuffs}
There have been great efforts in high-dimensional statistics where these ideas
of smoothing or stabilizing can be incorporated through regularization. These
include LASSO, ridge regression with a sparsity-inducing norm. Yet, the shallow
as well as deep neural networks \TOCITE trained with standard algorithms
(gradient descent based applied to a loss $L$) are able to find functions close
to $f_{\text{True}}$ without explicit regularization. 

\section{What Is A Two-Layer Neural Network}

This section begins with a brief introduction on \textit{feed-forward neural
networks}. Although most targeted applications are utilizing other architectures,
e.g. recurrent neural network \TOCITE, fully connected feed-forward neural
networks allows us to connect the trade-off between model complexity and
approximation efficiency conveniently.

A feed-forward neural network $\mathcal{N}$ is a class of artificial neural
network (\gls{ann}) equipped with a directed acyclic graph (\gls{dag}),
$\mathcal{G} = (\mathcal{V}, \mathcal{E})$. The associated graph $\mathcal{G}$
is the \textit{architecture} of $\mathcal{N}$ represented by a finite collection
of vertices \footnote{
    A computation unit associated with each $v \in \mathcal{V} \setminus \mathcal{I}$
    is called a \textit{node} or \textit{neuron}. To unify terminology, we will 
    refer as node going forward.
}
$\mathcal{V}$ and a finite set of edges $\mathcal{E}$. If we
represent the input and output layer by $\mathcal{I, O}$, we have the following
basic properties: 
i) a activation function $\sigma$ is associated with each 
$v \in \mathcal{V} \setminus \mathcal{I}$ 
ii) a scalar $w$ is associated with each $e \in \mathcal{E}$.

Nodes in the input layer observe a scalar input, seen by the downstream nodes.
Aside from $\mathcal{I}$, a single node takes a superpositions of the upstream
nodes via the respective weights $w_e$ mediated by the edges $e = (v, v')$
between the associated vertex $v$ and upstream vertices $v'$. 

We can then view the output function $f_{\mathcal{N}}: \R^d \to \R^{d'}$ as a
mapping from $\R^d$ to $\R^{d'}$, $d = \abs{\mathcal{I}}, d' =
\abs{\mathcal{O}}$. With a fixed architecture, the family of functions produced
by $\mathcal{N}$ is determined by the number of trainable parameters: $\{w_e,
b_v\}, e \in \mathcal{E}, v \in \mathcal{V} \setminus \mathcal{I}$

\subsection{Fully connected networks}

The general definition stated covers virtually all networks architecture
encountered in practice. In our discussion, we will narrow our focus to a
specialized class, \textit{fully connected networks}, where the vertices are
structured into layers.

The \textit{input layer} consists of simply $d$ input vertices where each vertex
receive a external scalar signal. Similarly, the \textit{output layer} consists
of $d'$ output vertices. $L$ hidden layers follows the input layer and each
layer constituted of $n_j$ hidden vertices and the integer $n_j$ is called the
\textit{width} of the $j$th layer. Within a fully connected network, each vertex
is connected only to the proceeding layer and the forthcoming layer.

There is a activation function $\sigma$ associated with each hidden vertex and
identity function is assigned at the output vertices to ensure the output
function from $\mathcal{N}$ is a linear combination at the layer $L$ with a bias
term. Therefore, the \textit{weight matrices} and the \textit{bias vectors} are
sufficient to describe the output function $f_{\mathcal{N}}$ with a fixed
architecture.

The main interest of this section is to summarize the approximation power of
ReLU networks, specifically ReLU networks. The problem of finding a stable
algorithm to produce such approximation is intentionally omitted.

The two main direction of discussing the approximation power are concerned with
the width and the depth. The first case is to focus on the approximation as the
width approaches infinity in a shallow network and we will see that even this
case is fairly complicated even with one or two hidden layers. The other extreme
would be deep neural networks where the width of the networks are usually fixed
for simplicity and the depth $L$ is taken to infinity. 

\section{Classical approximation scheme}

Here I will need to talk about something like affine or spilne regression as the
bach paper

Repres thm https://pages.stat.wisc.edu/~wahba/ftp1/wahba.wang.2019submit.pdf

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "MasterThesisSfS"
%%% End: 
