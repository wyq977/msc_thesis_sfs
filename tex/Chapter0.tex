% \chapter{Pre-requisite}

% This chapter provides a brief introduction on the materials required. 

% \section{Integral Transform}

% \section{Measure Theory}

% \section{Covering number}

% \section{Empirical Risk Minimization}

\chapter{Curse of dimensionality}

The term \textit{curse of dimensionality} was coined by Bellamn 1952 to describe
the dynamic programming that 

I also need to talk about something in PDE, monte carol sampling shit https://arxiv.org/pdf/1807.01212.pdf

Indeed, any supervised learning model suffers in principle from the curse of
dimensionality: under minimal assumptions on the function to be learnt,
achieving a fixed target generalisation error e requires a number of training
samples P which grows exponentially with the dimensionality d of input data ,
 https://openreview.net/pdf?id=sBBnfOFtPc

What is the teacher-student model

\section{}

Here they author prove the CoD is overcome by a factor of $1/m$
https://arxiv.org/pdf/2205.14421.pdf

Approxmation of functions in Ko space https://arxiv.org/pdf/2012.05451.pdf

\section{A section on approximation, estimation and etc}

\section{What Is A Two-Layer Neural Network}

\section{Classical approximation scheme}

Repres thm https://pages.stat.wisc.edu/~wahba/ftp1/wahba.wang.2019submit.pdf

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "MasterThesisSfS"
%%% End: 
