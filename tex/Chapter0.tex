\chapter{Curse of dimensionality}

The term \textit{curse of dimensionality} (\gls{cod}) was coined by
\cite{bellmanTheoryDynamicProgramming1952} almost 70 years ago to describe the
overwhelming complexity associated with solving a multi-stage processes through
dynamic programming. In approximation, it amounts to the exponentially growing
number of data points required to maintain the accuracy. Such dimensionality
cursed problems has appeared in computational and applied mathematics. For
approximations in high dimensions, the accuracy will drop exponentially as
dimensionality increases but the deep learning based numerical application
methods for partial differential equation (\gls{pde}) indicates satisfactory
performance in \cite{eDeepRitzMethod2017,
eDeepLearningbasedNumerical2017,beckMachineLearningApproximation2019} but there
is a absence of rigorous mathematical results to demonstrate this conjecture.
Naturally, the success in applications like computer vision and pattern
detection even in the case of low data points calls for a theoretical
explanation.
% https://arxiv.org/pdf/2205.14421.pdf
 
A few analyses attempt to explain the success of learning nonlinear operators by
neural networks including DeepOnets with a focus on reducing the high/infinite
dimensional space to a low dimensional space.
\cite{lanthalerErrorEstimatesDeepOnets2022} rigorously proves that the CoD is
overcome when considering smooth functions with exponentially decaying
coefficients.

I also need to talk about something in PDE, monte carol sampling shit https://arxiv.org/pdf/1807.01212.pdf

Indeed, any supervised learning model suffers in principle from the curse of
dimensionality: under minimal assumptions on the function to be learnt,
achieving a fixed target generalisation error e requires a number of training
samples P which grows exponentially with the dimensionality d of input data ,
 https://openreview.net/pdf?id=sBBnfOFtPc

What is the teacher-student model

\section{Approximation and estimation scheme}

Here they author prove the CoD is overcome by a factor of $1/m$
https://arxiv.org/pdf/2205.14421.pdf

Approximation of functions in Ko space https://arxiv.org/pdf/2012.05451.pdf

\section{Classical approximation spaces}

Here I will need to talk about something like affine or spilne regression as the
bach paper

Polynomial spaces?

Ko spaces?

Repres thm https://pages.stat.wisc.edu/~wahba/ftp1/wahba.wang.2019submit.pdf

\section{RKHS and kernel regression shit}

I need to briefly go over some kernel regression stuff and talk a bit about its
connection with NN?

Also on the represnetation results

\subsection{Difference between representation theorem and approximation}

Also on the debate between linking kernel with NN.


\section{The setup of supervised learning}

Here I need to answer what is the functions?

The error

The norm

and the spaces

What is student teacher model ???

Here we briefly review the statistical learning framework and we refer readers to \cite{shalev-shwartzUnderstandingMachineLearning2014a} for details.

Under the batch learning settings, a \textit{training set} dataset $S$ is
accessible to the learner. Generally, it is assumed that $S = \{(x_1,y_1),
\dots, (x_n, y_n)\}$ of $n$ points independently and identically distributed
(i.i.d.) $\mathcal{X} \times \mathcal{Y}$ with some unknown distribution
$\mathcal{D}$. For simplicity, we only write the regression where the output
function $f: \mathcal{X} \to \mathcal{Y}$ with minimum expected error on $S$
with some loss function $L: \mathcal{Y} \to \R$:

\begin{equation}
    \label{eq:training_loss}
    L_{\mathcal{D}}(f) = \PRi{(x, y) \sim \mathcal{D}}{y - f(x)}
\end{equation}

The learner cannot minimize the expected loss $L_{\mathcal{D}}$ due to limited
sample points $n$ but an estimate of the expected loss of using training set $S$
can be evaluated:

\begin{equation}
    L_{S}(f) = \frac{1}{n} \cdot
\end{equation}

$L(f)$ and $\hat{L}(f)$ is used. A expected margin loss for any margin $\gamma$:

\begin{equation}
    L_{\gamma} = \PRi{sad}{sdsd}
\end{equation}


A minimized loss \eqref{eq:training_loss} does not guarantee a low expected
error and the simplest case would be the case where a trained predictor $f$ that
memorize $S$ and has zero $L_{s}(f)$. However, it is shown in \TOCITE{about over
fitting} training in deep networks (over-parameterized) beyond $0$ training
error, i.e. during the \textit{terminal phase of training} (\Gls{tpt}) \TOCITE.
This is in sharp contrast to the conventional belief in learning or regression
models where \textit{over-fitting} is discouraged. Interesting empirical
phenomena such as ``Nerual Collapse'' by \TOCITE sheds some lights into why deep
networks can escape or evade the complexity based techniques.

Add a section of over parameterization in NN.

In learning, we are interested in the size of NN or the numbers of parameters
required for an accuracy $\epsilon$. An \textit{generalisation error} is
difference $L_\mathcal{D}(f) - L_{S}(f)$ and this quantity shows the differnece between learning and memorizing.

As the predictor $f$ is chosen by the training algorithm using $S$, we would
like to obtain a bound that holds for the set of all possible functions.
Therefore, it is 

Add something from neural network theory????

Historically, linear regression \TOCITE was the method of choice in supervised
learning where the search for \textit{true} function is performed only within a
small subspace of all functions: $\mathcal{H}_{\text{linear}}$, the space of
linear functions. Despite its popularity and extensive usage in real-world
applications, the space of linear functions is often insufficient since the
\textit{true} relations between input $\mathcal{X}$ and $\mathcal{Y}$ often
involve non-linearity. Therefore, it is preferable to choose a \textit{larger}
or more expressive hypothesis space $\mathcal{H}$ so that these mappings
$\mathcal{X} \to \mathcal{Y}$ can be found.

This intuition that the target functions can be formalized as the Bayesian prior
knowledge \TOCITE. In theory, the prior distribution of $(X, Y)$ on $\mathcal{X}
\times \mathcal{Y}$ can be formulated as a probability measure over all
probability measures on $\mathcal{X} \times \mathcal{Y}$.  For example, one
could hypothesize that $f_{\text{True}}$ is mostly likely to be linear and
consider all the other functions equally likely. This is the case where the we
assign zero measure to all non-linear functions and assign all linear functions
the same weights. Note that the measure here is improper since it assigns
$\infty$ to the space of all linear functions $\mathcal{H}_{\text{linear}}$.
However, it is often difficult to articulate a realist intuition such as ``we
favors a smoother, simpler function rather than a more highly fluctuating
function''. These realist \textit{insights} or \textit{impressions} are
harder to formalize mathematically and the calculation of such problems within
the Bayesian framework is often intractable.

\TONOTE{Add something on LASSO, high dim regulzation stuffs}
Is Lasso or spline regression 


There have been great efforts in high-dimensional statistics where the idea
of smoothing or stabilizing can be incorporated through regularization. These
include LASSO, ridge regression with a sparsity-inducing norm. Yet, the shallow
as well as deep neural networks \cite{neyshaburSearchRealInductive2015,
maennelGradientDescentQuantizes2018, liLearningOverparameterizedNeural2019,
kuboImplicitRegularizationOverparameterized2019,
neyshaburImplicitRegularizationDeep2017} trained with standard algorithms
(gradient descent based applied to a loss $L$) are able to find solution close
to $f_{\text{True}}$ without explicit regularization including drop-outs,
weight-decay or early-stopping. This phenomenon is referred as \textit{Implicit
Regularization}.\footnote{ It is also defined as \textit{Implicit Bias} in
\cite{soudryImplicitBiasGradient2022} }
\cite{kuboImplicitRegularizationOverparameterized2019} has shown that the
(stochastic) gradient descent algorithm along with random initialization can
lead to surprisingly \textit{low} complexity of the over-parameterized networks.
\cite{maennelGradientDescentQuantizes2018} has shown in 2NN the with small
randomly initialized weights leads to ``simpler'' functions and
\cite{maennelGradientDescentQuantizes2018, neyshaburSearchRealInductive2015} has
empirically shown that the network size in the case of 1 hidden layer does not
behave as a regularization term (i.e. encourage ``simpler'' functions by the
predictors). In both setups, increasing the number of nodes results in a lower
test error and overall more general results even with overfitting with random
labels in \cite{neyshaburSearchRealInductive2015}. Therefore, a \textit{hidden}
notion of complexity must be present to account for lower complexity/better
generalization. Furthermore, this is proven by the phenomenon of training beyond
zero training loss \TOCITE $L_{\text{train}}$, i.e. the test error and
generalization improves despite $L_{\text{train}} = 0$.

The paradox of why 

% In applications, the phenomenon of implicit regularization is frequently observed [15, 27, 30, 29, 26, 22, 33]. Nonetheless, the theory behind it is still largely unexplored [26, 22, 33, 27]. 

\section{What ia a two-layer neural network}

This section begins with a brief introduction on \textit{feed-forward neural
networks}. Although most targeted applications are utilizing other architectures,
e.g. recurrent neural network, fully connected feed-forward neural
networks allows us to connect the trade-off between model complexity and
approximation efficiency conveniently.

\subsection{Feed-forward neural network}

The feed-forward neural network $\mathcal{N}$ is a class of artificial neural
networks (\gls{ann}) equipped with a directed acyclic graph (\gls{dag}),
\begin{equation*}
    \mathcal{G} = (\mathcal{V}, \mathcal{E}).
\end{equation*}

The associated graph $\mathcal{G}$ is the \textit{architecture} of $\mathcal{N}$
represented by a finite collection of vertices $\mathcal{V}$ 
\footnote{
    A computation unit associated with each $v \in \mathcal{V} \setminus \mathcal{I}$
    is called a \textit{node} or \textit{neuron}. To unify terminology, we will 
    refer the vertices as nodes.
}
and a finite set of edges $\mathcal{E}$. If we represent the input and output
layer by $\mathcal{I, O}$, we have the following basic properties: 
\begin{enumerate}
    \item a activation function $\sigma$ is associated with each $v \in
    \mathcal{V} \setminus \mathcal{I}$
    \item a scalar $w$ is associated with each $e \in
    \mathcal{E}$.    
\end{enumerate}

Nodes in the input layer observe a scalar input, seen by the downstream nodes.
Aside from $\mathcal{I}$, a single node takes a superpositions of the upstream
nodes via the respective weights $w_e$ mediated by the edges $e = (v, v')$
between the associated vertex $v$ and upstream vertices $v'$. 

We can then view the output function 
\begin{equation*}
    f_{\mathcal{N}} := 
\end{equation*}
as a mapping from $\R^d$ to $\R^{d'}$, $d = \abs{\mathcal{I}}, d' =
\abs{\mathcal{O}}$. With a fixed architecture, the family of functions produced
by $\mathcal{N}$ is determined by the number of trainable parameters: $\Theta :=
\{w_e,b_v\}, e \in \mathcal{E}, v \in \mathcal{V} \setminus \mathcal{I}$.

\subsection{Fully connected neural networks}

The general definition stated covers virtually all networks architecture
encountered in practice. In the next chapters, we will restrict ourself to a
specialized class, \textit{fully connected networks}. and the vertices of such
networks are structured into layers.

\TONOTE{Add a small figure of Neural network}

Within a fully connected network, each vertex is connected to all vertices in
the proceeding layer via the edges and to no other layers. The \textit{input
layer} $\mathcal{I}$ consists of simply $d$ input vertices where each vertex
receive a external scalar signal $x_i \in \R, i \in \{1, \dots, d\}$ and the
combined input $x := (x_1, \dots, x_d) \in \R^d$ forms the independent variable
for the function mapping $f_{\mathcal{N}}$. Similarly, the \textit{output layer}
$\mathcal{O}$ consists of $d'$ output vertices and the output vector is the
value $f_{\mathcal{N}}(X) \in \R^{d'}$. $L$ hidden layers follows the input
layer and the integer $L$ is the \textit{depth} of the networks. Each layer is
constituted of $n_j$ hidden vertices, and the integer $n_j$ is called the
\textit{width} of the $j$th layer. 

There is a activation function $\sigma$ associated with each hidden node.
Identity function is taken at nodes at $\mathcal{O}$ to ensure the
$f_{\mathcal{N}}(   X)$ is a linear combination of the nodes at $L$th layers plus a bias
term. Therefore for a fixed architecture, the \textit{weight matrices} and the
\textit{bias vectors} are sufficient to describe the output function
$f_{\mathcal{N}}$.



future stuff

The main interest of this section is to summarize the approximation power of
ReLU networks, specifically ReLU networks. The problem of finding a stable
algorithm to produce such approximation is intentionally omitted.

The two main direction of discussing the approximation power are concerned with
the width and the depth. The first case is to focus on the approximation as the
width approaches infinity in a shallow network and we will see that even this
case is fairly complicated even with one or two hidden layers. The other extreme
would be deep neural networks where the width of the networks are usually fixed
for simplicity and the depth $L$ is taken to infinity. 



It also holds for the nowadays more popular ReLU activation function since
$\sigma(z+1) - \sigma(z)$ is sigmoidal.
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "MasterThesisSfS"
%%% End: 
