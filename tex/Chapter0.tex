\chapter{Introduction}
\label{ch:introduction}

% explain the importance of approximation by NN

Approximation with neural networks has been gaining momentum and become the
``go-to'' method of choice when it comes to numerical approximations tools. The
history of NN dates back to late 1940s when its applications only become the
defector methods.

Despite its wide gained popularity and state-of-the-art performance across many
fields, a rigorous theory of why NN as an approximation tools are more
beneficial to the classical models using polynomials, wavelets, etc. 

% the 
It has already been established in 1980s that NN are in universal approximators
in the sense that any continuous function and a target accuracy $\epsilon$, one
can produce an approximation to $f$ within set accuracy $\epsilon$. However,
this does not explain well why NN is much more effective in practical numerical
experiments. In fact, this universal approximation property is present in
approximation with wavelets, splines or even polynomials. A understanding of why
NN, even shallow NN perform so well 

In Chapter \TODO, the question of
\textit{density} has been answered for 2NN in which functions on any compact
domain in $\R^d$ are dense in 2NN w.r.t. the supremum norm . Barron presented an
different approach where given a dictionary $\mathbb{D}$, one tries to find the
class of functions that are \textit{well approximated} by such dictionary.



For a fixed architecture such as 2NN, we consider the set of functions
$\mcal{M}$ produced by all possible choice of parameters in NN and we would like
to ask the following question of \textit{density}. What condition must the
activation function $\sigma: \R \to \R$ satisfy in order for the following
statement to hold:

For any $f \in C(\R^d)$, any compact set $U$ of $\R^d$, and an arbitrary accuracy
$\epsilon > 0$, there is a $f' \in \mcal{M}$ such that 
\begin{equation*}
    \sup_{x \in U} \abs{f(x) - f'(x)} < \epsilon.
\end{equation*}

In the case of 2NN, the set $\mcal{M}$ reads as the linear span dependent on
$\sigma$. Therefore, the questions is whether the linear space ($\mcal{M}$) is
\textit{dense} in the space of $C(\R^d)$ in the uniform convergence on compact
sets.

It has been well understood that this density question is of high importance, in
either shallow NN or NN of any number of hidden layers. However, density is only
concerned with the ability of the set $\mcal{M}$ to approximate well, which does
not translate into a efficient approximation scheme, not to mention finding a
stable algorithm to find such $f' \in \mcal{M}$.

There have been a wave of publications dealing with density in 2NN scheme in
different approaches by \cite{carrollConstructionNeuralNets1989,
cybenkoApproximationSuperpositionsSigmoidal1989,
hornikMultilayerFeedforwardNetworks1989a,
funahashiApproximateRealizationContinuous1989}. Using the Hahn-Banach Theorem
and the Riesz Representation Theorem,
\cite{cybenkoApproximationSuperpositionsSigmoidal1989} stated the density
property required is $\sigma$ is sigmoidal, (defined in \eqref{def:sigmoidal}).
In paper by \cite{hornikMultilayerFeedforwardNetworks1989a}, the activation
function needs to monotone and bounded, which allows noncontinous $\sigma$.

A sequence of paper adopted various techniques to attack this problem and yet
the answer is surprisingly simple. The necessary and sufficient condition on
$\sigma$  for the linear space $\mcal{M}$ to be dense is that $\sigma$ must not
be a polynomial.

The term \textit{curse of dimensionality} (\gls{cod}) was coined by
\cite{bellmanTheoryDynamicProgramming1952} almost 70 years ago to describe the
overwhelming complexity associated with solving a multi-stage processes through
dynamic programming. In approximation, it amounts to the exponentially growing
number of data points required to maintain the accuracy. Such dimensionality
cursed problems has appeared in computational and applied mathematics. For
approximations in high dimensions, the accuracy will drop exponentially as
dimensionality increases but the deep learning based numerical application
methods for partial differential equation (\gls{pde}) indicates satisfactory
performance in \cite{eDeepRitzMethod2017,
eDeepLearningbasedNumerical2017,beckMachineLearningApproximation2019} but there
is a absence of rigorous mathematical results to demonstrate this conjecture.
Naturally, the success in applications like computer vision and pattern
detection even in the case of low data points calls for a theoretical
explanation.
% https://arxiv.org/pdf/2205.14421.pdf
 
A few analyses attempt to explain the success of learning nonlinear operators by
neural networks including DeepOnets with a focus on reducing the high/infinite
dimensional space to a low dimensional space.
\cite{lanthalerErrorEstimatesDeepOnets2022} rigorously proves that the CoD is
overcome when considering smooth functions with exponentially decaying
coefficients.

I also need to talk about something in PDE, monte carol sampling shit https://arxiv.org/pdf/1807.01212.pdf

Indeed, any supervised learning model suffers in principle from the curse of
dimensionality: under minimal assumptions on the function to be learnt,
achieving a fixed target generalisation error e requires a number of training
samples P which grows exponentially with the dimensionality d of input data ,
 https://openreview.net/pdf?id=sBBnfOFtPc

What is the teacher-student model

\section{Approximation and estimation scheme}

Here they author prove the CoD is overcome by a factor of $1/m$
https://arxiv.org/pdf/2205.14421.pdf

Approximation of functions in Ko space https://arxiv.org/pdf/2012.05451.pdf

\section{Classical approximation spaces}

Here I will need to talk about something like affine or spilne regression as the
bach paper

Polynomial spaces?

Ko spaces?

Repres thm https://pages.stat.wisc.edu/~wahba/ftp1/wahba.wang.2019submit.pdf

\section{RKHS and kernel regression shit}

I need to briefly go over some kernel regression stuff and talk a bit about its
connection with NN?

Also on the represnetation results

\subsection{Difference between representation theorem and approximation}

Also on the debate between linking kernel with NN.


\section{The setup of supervised learning}

Here I need to answer what is the functions?

The error

The norm

and the spaces

What is student teacher model ???

Here we briefly review the statistical learning framework and we refer readers to \cite{shalev-shwartzUnderstandingMachineLearning2014a} for details.

Under the batch learning settings, a \textit{training set} dataset $S$ is
accessible to the learner. Generally, it is assumed that $S = \{(x_1,y_1),
\dots, (x_n, y_n)\}$ of $n$ points independently and identically distributed
(i.i.d.) $\mathcal{X} \times \mathcal{Y}$ with some unknown distribution
$\mathcal{D}$. For simplicity, we only write the regression where the output
function $f: \mathcal{X} \to \mathcal{Y}$ with minimum expected error on $S$
with some loss function $L: \mathcal{Y} \to \R$:

\begin{equation}
    \label{eq:training_loss}
    L_{\mathcal{D}}(f) = \PRi{(x, y) \sim \mathcal{D}}{y - f(x)}
\end{equation}

The learner cannot minimize the expected loss $L_{\mathcal{D}}$ due to limited
sample points $n$ but an estimate of the expected loss of using training set $S$
can be evaluated:

\begin{equation}
    L_{S}(f) = \frac{1}{n} \cdot
\end{equation}

$L(f)$ and $\hat{L}(f)$ is used. A expected margin loss for any margin $\gamma$:

\begin{equation}
    L_{\gamma} = \PRi{sad}{sdsd}
\end{equation}


A minimized loss \eqref{eq:training_loss} does not guarantee a low expected
error and the simplest case would be the case where a trained predictor $f$ that
memorize $S$ and has zero $L_{s}(f)$. However, it is shown in \TOCITE{about over
fitting} training in deep networks (over-parameterized) beyond $0$ training
error, i.e. during the \textit{terminal phase of training} (\Gls{tpt}) \TOCITE.
This is in sharp contrast to the conventional belief in learning or regression
models where \textit{over-fitting} is discouraged. Interesting empirical
phenomena such as ``Nerual Collapse'' by \TOCITE sheds some lights into why deep
networks can escape or evade the complexity based techniques.

Add a section of over parameterization in NN.

In learning, we are interested in the size of NN or the numbers of parameters
required for an accuracy $\epsilon$. An \textit{generalisation error} is
difference $L_\mathcal{D}(f) - L_{S}(f)$ and this quantity shows the differnece between learning and memorizing.

As the predictor $f$ is chosen by the training algorithm using $S$, we would
like to obtain a bound that holds for the set of all possible functions.
Therefore, it is 

Add something from neural network theory????

Historically, linear regression \TOCITE was the method of choice in supervised
learning where the search for \textit{true} function is performed only within a
small subspace of all functions: $\mathcal{H}_{\text{linear}}$, the space of
linear functions. Despite its popularity and extensive usage in real-world
applications, the space of linear functions is often insufficient since the
\textit{true} relations between input $\mathcal{X}$ and $\mathcal{Y}$ often
involve non-linearity. Therefore, it is preferable to choose a \textit{larger}
or more expressive hypothesis space $\mathcal{H}$ so that these mappings
$\mathcal{X} \to \mathcal{Y}$ can be found.

This intuition that the target functions can be formalized as the Bayesian prior
knowledge \TOCITE. In theory, the prior distribution of $(X, Y)$ on $\mathcal{X}
\times \mathcal{Y}$ can be formulated as a probability measure over all
probability measures on $\mathcal{X} \times \mathcal{Y}$.  For example, one
could hypothesize that $f_{\text{True}}$ is mostly likely to be linear and
consider all the other functions equally likely. This is the case where the we
assign zero measure to all non-linear functions and assign all linear functions
the same weights. Note that the measure here is improper since it assigns
$\infty$ to the space of all linear functions $\mathcal{H}_{\text{linear}}$.
However, it is often difficult to articulate a realist intuition such as ``we
favors a smoother, simpler function rather than a more highly fluctuating
function''. These realist \textit{insights} or \textit{impressions} are
harder to formalize mathematically and the calculation of such problems within
the Bayesian framework is often intractable.

\TONOTE{Add something on LASSO, high dim regulzation stuffs}
Is Lasso or spline regression 


There have been great efforts in high-dimensional statistics where the idea
of smoothing or stabilizing can be incorporated through regularization. These
include LASSO, ridge regression with a sparsity-inducing norm. Yet, the shallow
as well as deep neural networks \cite{neyshaburSearchRealInductive2015,
maennelGradientDescentQuantizes2018, liLearningOverparameterizedNeural2019,
kuboImplicitRegularizationOverparameterized2019,
neyshaburImplicitRegularizationDeep2017} trained with standard algorithms
(gradient descent based applied to a loss $L$) are able to find solution close
to $f_{\text{True}}$ without explicit regularization including drop-outs,
weight-decay or early-stopping. This phenomenon is referred as \textit{Implicit
Regularization}.\footnote{ It is also defined as \textit{Implicit Bias} in
\cite{soudryImplicitBiasGradient2022} }
\cite{kuboImplicitRegularizationOverparameterized2019} has shown that the
(stochastic) gradient descent algorithm along with random initialization can
lead to surprisingly \textit{low} complexity of the over-parameterized networks.
\cite{maennelGradientDescentQuantizes2018} has shown in 2NN the with small
randomly initialized weights leads to ``simpler'' functions and
\cite{maennelGradientDescentQuantizes2018, neyshaburSearchRealInductive2015} has
empirically shown that the network size in the case of 1 hidden layer does not
behave as a regularization term (i.e. encourage ``simpler'' functions by the
predictors). In both setups, increasing the number of nodes results in a lower
test error and overall more general results even with overfitting with random
labels in \cite{neyshaburSearchRealInductive2015}. Therefore, a \textit{hidden}
notion of complexity must be present to account for lower complexity/better
generalization. Furthermore, this is proven by the phenomenon of training beyond
zero training loss \TOCITE $L_{\text{train}}$, i.e. the test error and
generalization improves despite $L_{\text{train}} = 0$.

The paradox of why 

future stuff

The main interest of this section is to summarize the approximation power of
ReLU networks, specifically ReLU networks. The problem of finding a stable
algorithm to produce such approximation is intentionally omitted.

The two main direction of discussing the approximation power are concerned with
the width and the depth. The first case is to focus on the approximation as the
width approaches infinity in a shallow network and we will see that even this
case is fairly complicated even with one or two hidden layers. The other extreme
would be deep neural networks where the width of the networks are usually fixed
for simplicity and the depth $L$ is taken to infinity. 



It also holds for the nowadays more popular ReLU activation function since
$\sigma(z+1) - \sigma(z)$ is sigmoidal.

% In applications, the phenomenon of implicit regularization is frequently observed [15, 27, 30, 29, 26, 22, 33]. Nonetheless, the theory behind it is still largely unexplored [26, 22, 33, 27]. 


The rest of this thesis is organized as follows. Chapter \ref{ch:preliminary}
introduces the question setup and the 2NN model. The question of
\textit{density} with 2NN is answered for any continuous functions from $\R^d$
to $\R$. We showed that the 2NN are universal approximators with a activation
function that is not a polynomial when the number of nodes are allowed to grow
unlimited. In chapter \ref{ch:fourier}, we consider the model classes of
functions (Fourier-analytic Barron spaces) that are guaranteed to be
\textit{well approximated} by 2NN. The smoothness restriction is enforced by
bounding the Fourier transform term of the functions. Chapter \ref{ch:infinite}
describes the Infinite-width Barron spaces in which each function is also well
approximated with the help of an integral representation. Furthermore, the
connection between these spaces and their variation spaces is included. Each
corresponding variation space is constructed using a compact dictionary of
functions. Moreover, we compared and articulated the relationship between the
model classes.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "MasterThesisSfS"
%%% End: 
