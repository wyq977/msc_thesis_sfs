\chapter{The Approximation Properties of Two-Layer Neural Networks}

In this chapter, we will introduce some basic concepts about artificial
artificial neural networks with an emphasis on the two-layer neural networks
(\gls{2nn}). Firstly, we will define the 2NN model and the problem setup. In
section \ref{sec:uat}, we will state the result of the universal approximation
theorem. Later in section \ref{sec:spectral_norm} and \ref{sec:barron_norm}, we
formally introduce the function spaces associated with 2NN, along with the
definition of the \textit{spectral norm} by
\cite{barronUniversalApproximationBounds1993} and the \textit{Barron norm} by
\cite{eBarronSpaceFlowinduced2021}. Despite the term ``Barron space'' and
``Barron norm'' has been coined and mentioned earlier, the notion of different
Barron spaces and their relationship have not been made obvious. To avoid
confusion, we here borrow the terminology adopted by
\cite{carageaNeuralNetworkApproximation2022}: we refer the spaces associated
with the spectral norm as the \textit{Fourier-analytic Barron spaces} and the
spaces defined using the Barron norm as the \textit{infinite-width Barron
spaces} as it essentially consist of ``infinitely wide'' neural networks (with
some restrictions on the parameters). Finally, the relationship (mainly
inclusion) between the Fourier-analytic Barron spaces and the infinite-width
Barron spaces is examined in section \ref{sec:diff_barron_spaces}. The aim of
this chapter is to provide a concise summary of various well-known results
concerning the approximation properties of \ and we recommend work by
\cite{eMathematicalUnderstandingNeural2020,bernerModernMathematicsDeep2021} for
a comprehensive introduction.

% classical Barron space, or the Fourier-analytic Barron space

% infinitely wide

% infinite-width Barron spaces.

% sparsity-inducing norm ? in bach 2017 paper.


\section{Universal Approximation Theorem}
\label{sec:uat}

In general, neural networks, even 2NN with one hidden layer, are universal
approximators, which means any continuous functions on a compact set up to a
arbitrary precision under some mild restrictions on the activation functions
$\sigma$ and the number of nodes are allow to grow arbitrarily. The condition on
$\sigma$ is formally stated in
~\cite{cybenkoApproximationSuperpositionsSigmoidal1989}. Despite its obvious
importance, one can not benefit much as it can not provide enough quantitative
information in realist applications.

% We will state the the universal approximation theorem that the finite linear
% combinations of sigmoidal functions (sum of the functions of the form
% (\eqref{def:sigmoidal})) are dense in the space of continuous functions. This
% approximation result holds for any continuous sigmoidal functions.

% function on a compact set up to arbitrary precision [Cyb89, Fun89, HSW89,
% LLPS93]. modern mathematics of deep learning

\begin{definition}[sigmoidal function]\label{def:sigmoidal}
    A function $\sigma$ is \textit{sigmoidal} if
    \begin{equation}
        \sigma(t) =
        \begin{cases}
            1 & \text{as} \quad t \to +\infty \\
            0 & \text{as} \quad t \to -\infty
        \end{cases}
    \end{equation}
\end{definition}

\subsection*{Notations and setup}

Let $I_d = [0,1]^d$ denote the $d$-dimensional unit cube, and the space of
continuous functions over $I_d$ is denoted by $C(I_d)$. We denote the supremum
norm of a function $f \in C(I_d)$ by $\norm{f}_{\infty}$ 
\begin{equation}
    \norm{f}_{\infty} = \sup_{x\in I_d} \abs{f(x)}.
\end{equation}
We use $M(I_d)$ to denote the space of finite, signed regular Borel measures,
i.e. $\mu(A) \in \R$ for all Borel sets $A \in I_d$ and $\mu(\emptyset)= 0$. We
refer readers to \cite{rudinFunctionalAnalysis1991,
rudinRealComplexAnalysis1987} for a detailed presentation of functional
construction used and we include some basic materials in Appendix
\ref{app:function_measure}.

Let $h(x)$ be the sums of the form:

\begin{equation}
    \label{eq:sum_sigma}
    h(x) = \sum_{j=1}^m a_j \sigma(b_j\tr x + c_j)),
\end{equation}

where $a_j, c_j \in \R$ and $b_j \in \R^d$ and $\sigma$ is a univariate function
from $\R$ to $\R$.
 
The main contribution of approximation theorem is the statement on the
conditions of $\sigma$ when the above finite linear combinations $h(x)$ are
dense with respect to the supremum norm $\norm{\cdot}_{\infty}$.

\begin{theorem}[Universal Approximation Theorem]
    \label{thm:uat}
    If $\sigma$ is sigmoidal as defined in Def. $\ref{def:sigmoidal}$, then any
    function $f \in C(I_d)$ be approximated uniformly well by a finite linear
    combination of the form \eqref{eq:sum_sigma}.
\end{theorem}


The main structure of the proof is followed:
\begin{enumerate}
    \item any finite sums of the form \eqref{eq:sum_sigma} with a
    \hyperref[def:dis_func]{discriminatory function} $\sigma$ are dense in
    $C(I_d)$ with respective to the supremum norm.
    \item show that any bounded sigmoidal functions are discriminatory.
\end{enumerate}

\begin{definition}[Discriminatory function]
    \label{def:dis_func}
    A function $\sigma$ is \textbf{discriminatory} if for a measure $\mu \in
    M(I_d)$
    \begin{equation}
        \int_{I_d} \sigma(b\tr x + c) \,d\mu(x) = 0
    \end{equation}
    for all $b \in \R^d$ and $c \in \R$ implies $\mu = 0$
\end{definition}

We can now state that the linear span or the finite linear combinations of
continuous discriminatory functions are dense in the space of $C(I_d)$ equipped
with $\norm{\cdot}_{\infty}$.

\begin{theorem}
    Let $\sigma: \R \to \R$ be a
    continuous \hyperref[def:dis_func]{discriminatory function}, the finite sums
    of the form \eqref{eq:sum_sigma} are dense in the space $(C(I_d),
    \norm{\cdot}_{\infty})$. In other words, for any $\epsilon > 0$ and any
    $f \in C(I_d)$, there exists a sum $h(x)$ of the above form, where
    \begin{equation}
        \norm{f(x) - h(x)}_{\infty} < \epsilon \quad \forall x \in I_d.
    \end{equation}
\end{theorem}

\begin{proof}
    Let $G(\sigma) := \spn(\{\sigma(b\tr x + c): b \in \R^d, c \in \R \})$ be
    the linear span for every $b\in\R^d, c\in\R$. $G(\sigma)$ clearly is a
    linear subspace of $C(I_d)$. We claim that the closure of $G(\sigma)$,
    $\closure{G(\sigma)}$, is all of $C(I_d)$.

    We continue the proof by contradiction. Assuming $\closure{G(\sigma)}$ is
    not $C(I_d)$, then there is a bounded linear functional $L$ on $C(I_d)$ such
    that $L\equiv 0$ on $C(I_d)$ and $L(G(\sigma)) = L(\closure{G(\sigma)}) = 0$
    by the Hahn-Banach Theorem \ref{thm:hahn_banach_2}.

    By the \hyperref[thm:riesz_rep]{Riesz Representation Theorem}, there is a
    unique $\mu \in M(I_d)$ for this $L$ such that
    \begin{equation}
        L(f) = \int_{I_d} f(x) \,d\mu(x) \quad \forall f \in C(I_d)
    \end{equation}

    Since $L$ is zero on $G(\sigma)$, we must have for all $b$ and $c$ that
    \begin{equation}
        \int_{I_d} \sigma(b\tr x + c) \,d\mu(x) = 0
    \end{equation}

    However, the condition that $\sigma$ is discriminatory implies $\mu = 0$ and
    hence subspace $G(\sigma)$ must be dense in $C(I_d)$.
\end{proof}

Now it remains to show that sigmoidal functions are discriminatory.

\begin{lemma}
    Any bounded, measurable sigmoidal function is discriminatory.
\end{lemma}

\begin{proof}

    \textbf{Step 0} \textit{(Assume discriminatory and construct pointwise
    convergence function)}: Let $\sigma: \R \to \R$ be a sigmoidal function
    w.r.t. $\mu \in M(I_d)$. Assume that the $\sigma$ is discriminatory with
    measure $\mu$ as in Def. \ref{def:dis_func}, we need to show that the measure
    $\mu = 0$.

    Fix a arbitrary $b_0 \in \R^d$ and define $\sigma_{\lambda}(x) :=
    \sigma(\lambda (b\tr x + c) + \varphi)$. Then, for any $c, \lambda, \varphi
    \in \R$ we have
    \begin{equation}
        \sigma_{\lambda}(x)
        = \begin{cases}
            \to 1, \quad &\for b\tr x + c > 0 \quad \text{as}\, \lambda \to +\infty \\
            \to 0, \quad &\for b\tr x + c < 0 \quad \text{as}\, \lambda \to -\infty \\
            \sigma(\varphi), \quad &\for b\tr x + c = 0, \quad\forall \lambda
        \end{cases}
    \end{equation}

    Therefore, the functions $\sigma_{\lambda}$ converges to a function
    $\gamma(x): I_d \to \R$
    \begin{equation}
        \gamma(x) = 
        \begin{cases}
            1,               \quad &\for b\tr x + c > 0 \\
            0,               \quad &\for b\tr x + c < 0 \\
            \sigma(\varphi), \quad &\for b\tr x + c = 0
        \end{cases}
    \end{equation}
    pointwise as $\lambda \to + \infty$.
    
    Let $\Pi_{b_0,c}$ denote the hyperplane, $H_{b_0, c}$ denote the
    half-space as below 
    \begin{align}
        \Pi_{b_0,c} &= \{x\in\R^d \mid b_0\tr x + c = 0\} \\
        H_{b_0, c}  &= \{x\in\R^d \mid b_0\tr x + c > 0\}.
    \end{align}
    for a fixed $b_0 \in \R^d$ and all $c \in \R$.
    
    By the Lebesgue Convergence Theorem, we have
    \begin{align*}
        \sigma(\varphi) \mu(\Pi_{b_0, c}) + \mu (H_{b_0, c})
        &= \int_{I_d} \gamma(x) \,d\mu(x) \\
        &= \int_{I_d} \lim_{\lambda\to\infty} \sigma_{\lambda}(x)\,d\mu(x) \\
        &= \lim_{\lambda\to\infty} \int_{I_d} \sigma_{\lambda}(x)\,d\mu(x) = 0
    \end{align*}
    for all $\lambda, \varphi \in \R$.

    Thanks to the function $\sigma$ being sigmoidal, $\lim_{\varphi\to +\infty}
    \sigma(\varphi)= 1$ and $\lim_{\varphi\to -\infty} \sigma(\varphi)=0$
    \begin{equation}
        \mu(\Pi_{b_0, c}) = 0 \quad \text{and} \quad \mu (H_{b_0, c}) = 0 
        \quad \forall c\in\R
    \end{equation}
    
    We would like to show that the measure of all half-planes being zero implies
    that the measure $\mu$ must be zero.
    
    The function $\sigma_{\lambda}$ converges pointwise to $\gamma$ as $\lambda
    \to \infty$. Since $\sigma$ is bounded by assumption:
    $\abs{\sigma_{\lambda}(x) \leq \norm{\sigma}_{\infty} \leq \infty}$ for all
    $\lambda \in \R$ and all $x \in I_d$. By the Lebesgue bounded convergence
    theorem,
    \begin{equation}
        \int_{I_d} \gamma \,d\mu 
        = \int_{I_d} \lim_{\lambda\to\infty} \sigma_{\lambda}(x) \,d\mu(x)
        = \lim_{\lambda\to\infty} \int_{I_d} \sigma_{\lambda}{x} \,d\mu(x).
    \end{equation}


    \textbf{Step 1} \textit{(Construct a signed measure)}:

    \textbf{Step 2} \textit{(Define a linear functional $L$ and show $L = 0$ on
    with $\mu$)} Combining the 

    \textbf{Step 3} \textit{(Tidy up)}

\end{proof}

% Let $f_m(\mathbf{x}, \Theta)$ be the parameterized family of 2NN defined above
% of $m$ nodes that which map input vector $\mathbf{x}$ of dimension $d$.

% \begin{equation}
%     f_m(\mathbf{x}, \Theta) = \sum_{i=1}^m a_i \sigma(\mathbf{b_i}^T\mathbf{x} + c_i)
% \end{equation}

% where $\Theta = (a_1, \mathbf{b}_1, c_1, \dots, a_m, \mathbf{b}_m, c_m)$ denotes
% all the parameters (the total number of parameters is $(d+2)m + 1$). We will
% consider the hypothesis spaces where the activation functions $\sigma$ is ReLU
% \footnote{\tonote{The choice of activation function is important for
%         infinite-width Barron spaces}}.

%  homogeneity

\section{Add a section for F transform}

\section{Spectral Norm and Fourier-analytic Barron spaces}
\label{sec:spectral_norm}

It is shown by \cite{barronUniversalApproximationBounds1993} that functions with
finite Fourier moment can be approximated with the superpositions of sigmoidal
functions at a rate independent of the dimensionality $d$. In other words, for
any functions in said class can be approximated with a (shallow) neural network
at error rate $\bigO(m^{-1} \cdot C)$ where $m$ is the number of nodes in the
the single hidden layer and $C$ is some constant dependent on the smoothness of
the target function.

We formulated the class of functions introduced in
\cite{barronNeuralNetApproximation1992, barronUniversalApproximationBounds1993}
below.

\begin{definition}
    Let $X$ be a bounded set in $\mathbb{R}^d$, a function $f: X \to \mathbb{R}$
    is said to be in \textit{Barron class} with constant $C > 0$, if there are
    $x_0$ in $X$ and $c \in [-C, C]$ and a measurable function $F: \mathbb{R}^d
        \to \mathbb{C}$ satisfying:

    \begin{align}
         & \int_{\mathbb{R}^d} \abs{\omega}_{X, x_0} \cdot \abs{F(\omega)} d\omega < C                            \\
         & f(x) = c + \int_{\mathbb{R}^d} (e^{i\spr{x}{\omega}} - e^{i\spr{x_0}{\omega}}) \cdot F(\omega) d\omega
    \end{align}

    where $\abs{\omega}_{X, x_0} := \sup_{x\in X}\abs{\spr{\omega}{x - x_0}}$.
    We refer the class of all functions as $\Gamma_C(X, x_0)$.
\end{definition}

\begin{theorem}\cite[Theorem~1]{barronUniversalApproximationBounds1993}\label{thm:barron_1993_1}
    For every function in $\Gamma_C(X, x_0)$, every sigmoidal function $\phi$,
    every probability measure $\mu$, and every $n \geq 1$, there exists a linear
    combination of sigmoidal function $f_n(x)$ of the form, such that
    \begin{equation}
        \int_X(f(x) - f_n(x))^2 \mu(dx) \leq \frac{(2C)^2}{n}
    \end{equation}
\end{theorem}


\begin{proof}

    The main ideas behind the the proof of Theorem \ref{thm:barron_1993_1} is to
    show functions with finite Fourier moment are in the closure of the convex
    hull of the set of half planes and law of large numbers.

    \textbf{Step 0} (\textit{Fixing $x_0$}): Changing $x_0$ to $x_1$ affects the
    norm, at most, by a factor of $2$. Let $x_0, x_1 \in X$, and $f$ fulfilling
    the assumption above, that is $f \in \Gamma_C(X, x_0)$. For any $\omega \in
        \mathbb{R}^d$, given $x_0, x_1$, we have

    \begin{equation}
        \abs{\omega}_{X, x_0} = \sup_{x\in X}\abs{\spr{\omega}{x-x_1}} \leq \sup_{x\in X}\abs{\spr{\omega}{x-x_0}} + \abs{\spr{\omega}{x_0-x_1}} \leq 2\abs{\omega}_{X, x_1}
    \end{equation}

    Thus we have $\int_{\mathbb{R}^d} \abs{\omega}_{X, x_0} \cdot
        \abs{F(\omega)} d\omega < 2C$. If we have $\tilde{c} = c +
        \int_{\mathbb{R}^d} (e^{\spr{\omega}{x_0}} - e^{\spr{\omega}{x_1}})
        d\omega$, then $f(x) = \tilde{c} + \int_{\mathbb{R}^d} (e^{i\spr{x}{\omega}}
        - e^{i\spr{x_1}{\omega}})$ with $\tilde{c} \leq 2C$.

    This shows that changing $x_0$ would only affect the norm in the RHS of
    Theorem \ref{thm:barron_1993_1} at most by a factor of two, i.e.
    $\Gamma_C(X, x_0) \subset \Gamma_{2C}(X, x_1)$. Therefore, we continue the
    proof assuming $x_0 = 0$ for simplicity.

    \textbf{Step 1} (\textit{Represent $f$ via Inverse Fourier Transform}): From
    the assumption, and the fact that $f$ is real-valued ($X \in \mathbb{R}^d
        \to \mathbb{R}$), the real number part can be written as:

    Note that with polar decomposition $F(\omega) = e^{i\theta(\omega)} \cdot
        \abs{F(\omega)}$ where $\theta(\omega) \in \mathbb{R}$ denote the magnitude
    decomposition.

    \begin{align}
        f(x) - f(0)
         & = \Re \int (e^{i\spr{\omega}{x}} - e^{i\spr{\omega}{0}}) e^{i\theta(\omega)} \cdot F(\omega) d\omega                              \\
         & = \int_{\Omega}\Big(\cos(\spr{\omega}{x} + \theta(\omega)) - \cos(\theta(\omega))\Big)
        F(\omega) d\omega                                                                                                                    \\
         & = \int_{\Omega} \frac{C_{f,X}}{\abs{\omega}_{X, 0}}\Big(\cos(\spr{\omega}{x} + \theta(\omega)) - \cos(\theta(\omega))\Big) d\mu_g \\
         & = \int_{\Omega} g(x, \omega) d\mu_g \label{eq:barron_fouier_int}
    \end{align}

    $C_{f,X}$ is a constant defined with $f$ and $X$: $C_{f,X} =
        \int_{\mathbb{R}^d} \abs{\omega}_{X, 0} \cdot \abs{F(\omega)} d\omega \leq
        C$ and $\mu_g$ is a probability distribution $d\mu_g =
        \abs{\omega}_{X,0}/C_{f,X}\abs{F(\omega)} d\omega$, the integral is
    evaluated on $\Omega = \{\omega \in \mathcal{R}^d: \omega \not = 0\}$ and

    \begin{equation}
        g(x, \omega) = \frac{C_{f,X}}{\abs{\omega}_{X, 0}}\Big(\cos(\spr{\omega}{x} + \theta(\omega)) - \cos(\theta(\omega))\Big)
    \end{equation}

    \textbf{Step 2} (\textit{$f(x) - f(0)$ is in the closure of the convex hull
        of $G_{cos}$}) The integral form in (\ref{eq:barron_fouier_int}) shows that
    $f(x) - f(0)$ can be represented as an infinite convex combination of
    functions in the class

    \begin{equation}
        G_{cos} = \Bigg\{\frac{\abs{\gamma}}{\abs{\omega}_{X, 0}}\Big(\cos(\spr{\omega}{x} + b) - \cos(b)\Big): \omega \not= 0, \abs{\gamma} \leq C, b \in \mathbb{R} \Bigg\}
    \end{equation}

    Suppose we have drawn $n$ samples ($\{\omega_i, i = 1,\dots, n\}$) from
    $\mu_g$, the expected norm in $L_2(\mu_g, X)$ converges to zero as $n \to
        \infty$ by $L_2$ law of large numbers. Therefore, there exist a sequence of
    convex combination in $G_{cos}$ that converges to $f(x) - f(0)$ in $L_2$.


    \textbf{Step 3} (\textit{$G_{cos}$ is in the closure of the convex hull of
        $G_{step}$}): It is sufficient to check $g(z), z = \alpha x, \alpha =
        \omega/\abs{\omega}_{X,0}$ on $[-1, 1]$ for some $\omega$. As $g(z)$ is a
    uniformly continuous sinusoidal function on $[-1, 1]$, it can be uniformly
    approximately by piecewise constant step function.

    Restricting $g(z)$ on $[0, 1]$, for a partition ${0 \leq p_1 \leq p_2 \leq
                \cdots \leq p_k = 1}$, define

    \begin{align}
        g_{k,+}(z) = \sum_{i=1}^{k-1} \Big(g(p_i) - g(p_{i-1})\Big) \cdot
        \stepfunction{z\geq p_i}
    \end{align}

    Similarly, we can construct $g_{k,-}(z) = \sum_{i=1}^{k-1} (g(-p_i) -
        g(-p_{i-1})) \cdot \stepfunction{z\leq -p_i}$, results in a sequence of
    piecewise step function on $[-1, 1]$ uniformly close to $g(z)$. We have
    $g(z) = g_{k,+}+g_{k,-}$, a linear combination of step function (or
    heaviside function) and the sum of the coefficients is bounded by 2C (The
    sum of coefficients of $g_{k,+}$ is bounded by $C$ as a result of the
    derivative of $g$ bounded by $C$, so does $g_{k,-}$ and hence $2C$).

    We can see that functions $g(z)$ are in the closure of the convex hull of
    the step functions (by Lemma 1 in
    \cite{barronUniversalApproximationBounds1993})

    By substituting $z = \omega/\abs{\omega}_{X, 0} x$, we have $G_{cos} \subset
        G_{step}$,
    \begin{equation}
        G_{step} = \Bigg\{ \gamma\stepfunction{\alpha x-t}: \abs{\gamma} \leq 2C, \abs{t} \leq 1, \abs{\alpha}_{X} = 1\Bigg\}
    \end{equation}

    \textbf{Step 4} (\textit{Closure of $G_{\phi}$}) There exists a sequence of
    sigmoidal function $\phi(\abs{c}(\alpha x - t))$, as $\abs{c} \to \infty$,
    it converges to step functions pointwise (except at points where $\alpha x -
        t = 0$). If we introduce a measure $\mu$ that has zero measure at those
    points, previous statement on $G_{cos} \subset G^{\mu}_{step}$ still holds
    on $\{\abs{t} \leq 1: \alpha x - t \not=0\}$ give a particular $\alpha$. We
    subsequently have convergence in $L_2(\mu, X)$ by dominated convergence
    theorem, which implies that functions in $G^{\mu}_{step} \subset G_{\phi}$.

    Finally we have the following relationship since the closure of a convex set
    is also convex.

    \begin{equation*}
        \Gamma_{X, x_0} \subset \closure{G_{cos}} \subset \closure{G_{step}} \subset \closure{G_{\phi}}
    \end{equation*}

    \cite[\textit{Lemma~1}]{barronUniversalApproximationBounds1993}: If $f$ is
    in the closure of the convex hull of a set $G$ in a Hibert space with norm
    $\norm{\cdot}$ and every function $g \in G$ is bounded by some constant
    $C_G$. Then for every $N \geq 1$, and every constant $C' > C_g^2 -
        \norm{f}^2$, there is a sequence $\{f_i, i = 1, \dots, N\}$ in the convex
    hull in $G$ such that:
    \begin{equation}
        \norm{f - f_N}^2 \leq \frac{C'}{n}
    \end{equation}

    $f_N = \sum_{i=1}^N \lambda_i \cdot f_i, \quad \sum_{i=1}^N \lambda_i = 1$

    % Lemma 1 in \cite{barronUniversalApproximationBounds1993} showed that
    % function in a closure of the convex hull of a set in a Hilbert space can
    % be approximated with a sequence of functions from such closure and the
    % norm between the function and the sequences $\{f_i, i = 1, \dots, N\}$ are
    % bounded in a magnitude of $\bigO(N^{-1})$.

    As shown above that function $f(x) - f(0)$ are in the closure of the convex
    hull of $G_{\phi}$ where $\norm{g} \leq (2C)^2$ for every $g \in G_{\phi}$.
    For any choice of $C' > (2C)^2 - \norm{f(x) - f(0)}^2$, we have the $L_2$
    norm of the approximation error is bounded.
    % Suppose we restrict $t$ t

    % Suppose we now restrict $t$ to the continuity point induced by measure
    % $\mu$ in We can check that the functions in $G_{step}$ are in the closure
    % of the convex hull of 

    % \textbf{Step 3} (\textit{Putting it together}): We can further show
    % $G_{cos}$ are in the class of sigmoidal functions. 

    % Theorem 2 in \cite{barronUniversalApproximationBounds1993}, we have that

\end{proof}

% A smoothness property of the function to be approximated is expressed in terms
% of its Fourier representation. In particular, an average of the norm of the
% frequency vector weighted by the Fourier magnitude distribution is used to
% measure the extent to which the function oscillates. In this Introduction, the
% result is presented in the case that the Fourier distribution has a density that
% is integrable as well as having a finite first moment. Somewhat greater
% generality is permitted in the theorem stated and proven in Sections III and IV.

% In addition to the smoothness property ensured via the finite first moment,

In the above result, the approximation error is presented in the case where the
smoothness of the functions is ensured by the fact that functions in such class
has a integrable Fourier distribution, as well as finite first moment.
Naturally, we would like to extend the findings with tighter restriction on the
smoothness at a cost of smaller hypothesis space.

We start with the defining the \textit{Fourier-anlytic norm}, $v_{f, s}\, s \geq
    1$, for function $f$ that has a Fourier representation:
\begin{align}
    f(x)     & = \int_{\mathbb{R}^d} e^{i\spr{x}{\omega}} F(\omega) d\omega \label{eq:fourier_rep}           \\
    v_{f, s} & = \int_{\mathbb{R}^d} \norm{\omega}_1^s \cdot \abs{F(\omega)} d\omega \label{eq:fourier_norm}
\end{align}


\begin{theorem}\label{thm:appr_f2}
    Consider the class of function on $\mathbb{R}^d$ for which there is a
    Fourier representation of the form above. If $v_{f, 2}$ is finite, there
    exists a two-layer ReLU neural network $f_m$ of $m$ nodes $f_m(\mathbf{x})
        = \sum_{i=1}^m a_i \sigma(\mathbf{b_i}^T\mathbf{x} + c_i)$ with $\norm{b_i}
        = 1$ and $\abs{c_i}\leq 1$ such that
    \begin{equation}
        \norm{f
            - \mathbf{x} \cdot \nabla f(0)
            - f(0)
            - f_m}^2 \leq \frac{16 v_{f, 2}^2}{m}
    \end{equation}
\end{theorem}

\textbf{What is the diff between the above and below?}

Other than the norm $\ell^2$, what is the difference and where does the $d$ come
about? This is taken from \cite{klusowskiApproximationCombinationsReLU2018}.

\begin{theorem}
    Consider only the unit ball in $\mathbb{R}^d$, function $f$ that admits a
    Fourier representation as above with finite $v_{f_2}$. There exists a $f_m$:

    \begin{equation}
        f_m(x) = a_0 + \mathbf{b}_0^T \mathbf{x} + \frac{v}{m} \sum_{i=1}^m a_i \sigma(\mathbf{b}_i^T\mathbf{x} + c_i)
    \end{equation}

    with $a_i \in [-1,1], \norm{\mathbf{b}_i}_1 = 1, a_0 = f(0), \mathbf{b}_0^T
        = \nabla f(0), v < 2v_{f,2}$ such that

    % Let $D = \[-1, 1\]^d$, target function $f$ has a Fourier representation as 
    % above with finite $v_{f, 2}$. There exists a $f_m$ 

    \begin{equation}
        \sup_{\mathbf{x} \in D} \abs{f(x) - f_m(x)} \leq c v_{f,2} \sqrt{d+\log{m}} \, m^{-1/2-1/d}
    \end{equation}

    for some universal $c > 0$.
\end{theorem}

Similar bound are also attained with stronger assumption on functions'
smoothness via $v_{f,s}$.

\begin{theorem}\label{thm:appr_f3} Consider the class of function on
    $\mathbb{R}^d$ for which there is a Fourier representation of the form
    above. If $v_{f, 3}$ is finite, there exists a two-layer neural network
    $f_m$ with squared ReLU ridge function as activation function of $m$ nodes
    $f_m(\mathbf{x}) = \sum_{i=1}^m a_i \sigma(\mathbf{b_i}^T\mathbf{x} + c_i)$
    with $\norm{b_i} = 1$ and $\abs{c_i}\leq 1$ such that
    \begin{equation}
        \norm{f
            - \mathbf{x}^T H_f(0) \mathbf{x}/2
            - \mathbf{x} \cdot \nabla f(0)
            - f(0)
            - f_m}^2 \leq \frac{16 v_{f, 3}^2}{m}
    \end{equation}

    where $H_f(0)$ is the Hessian of $f$ at the point zero.
\end{theorem}

\begin{theorem}
    Similar to the settings above. $f$ with finite $v_{f,3}$. There exists a
    $f_m$:
    \begin{equation}
        f_m(x) = a_0 + \mathbf{b}_0^T \mathbf{x} + \mathbf{x}^T A_0 \mathbf{x}+ \frac{v}{m} \sum_{i=1}^m a_i \sigma(\mathbf{b}_i^T\mathbf{x} + c_i)
    \end{equation}

    with $a_i \in [-1,1], \norm{\mathbf{b}_i}_1 = 1, a_0 = f(0), \mathbf{b}_0^T
        = \nabla f(0), A_0 = \nabla\nabla^T f(0), v < 2v_{f,3}$ such that

    \begin{equation}
        \sup_{\mathbf{x} \in D} \abs{f(x) - f_m(x)} \leq c v_{f,3} \sqrt{d+\log{m}} \, m^{-1/2-1/d}
    \end{equation}

    for some universal $c > 0$. Furthermore, if the $a_i$ are restricted to
    $\{-1, 1\}$, the upper bound is of order $v_{f,3} \sqrt{d} \,
        m^{-1/2-1/(d+2)}$
\end{theorem}

% Considering the set of all Borel probability measures of $\Omega = (\mathbb{R}^1
%     \times \mathbb{R}^d \times \mathbb{R}^1)$ (denoted by $\mathcal{P}_d$), given a
% nonempty bounded subset $U \subset \mathbb{R}^d$

% \begin{align*}
%     \mathcal{B}_{p}(U)              & := \left\{f: U \to \mathbb{R} \mid \exists\, \rho \in \mathcal{P}_d, \barronnorm{f}{p} < \infty \, \text{and $f$ is in the form \eqref{eq:barron_represent}}\right\} \\
%     \mathcal{B}_{\mathcal{F}, s}(U) & := \left\{f: U \to \mathbb{R} \mid \exists\, \rho \in \mathcal{P}_d, v_{f, s} < \infty \, \text{and $f$ is in the form \eqref{eq:fourier_rep}}\right\}
% \end{align*}


\newpage

\section{Barron Norm and infinite-width Barron spaces}
\label{sec:barron_norm}

Here in the section introduce the definition of Barron space and its properties.
It should be noted that the term ``Barron space'' was coined by
\cite{ePrioriEstimatesPopulation2019} to honor Prof. Andrew Barron's
contribution in the understanding of neural nets.

For a function $f: \mathcal{X} \subset \mathbb{R}^d \to \mathbb{R}$, we consider
those that admit the following integral representation:

\begin{equation}\label{eq:barron_represent}
    f(\mathbf{x}) = \int_{\Omega} a \sigma(\mathbf{b}^T\mathbf{x} + c) \rho(da, d\mathbf{b}, dc), \quad \mathbf{x} \in \mathcal{X}
\end{equation}

where $\rho$ is a probability distribution on $(\Omega, \Sigma_\Omega)$, $\Omega
    = \mathbb{R}^1 \times \mathbb{R}^d \times \mathbb{R}^1$ and $\sigma(\cdot) =
    \max\{\cdot, 0\}$ is the ReLU activation function.

The representation can be seen as a continuum analogy of the 2NN with $m$ hidden
nodes:

\begin{equation}
    f_m(\mathbf{x}, \Theta) := \frac{1}{m} \sum_{j=1}^m a_j \sigma(\mathbf{b}^T\mathbf{x} + c_j), \quad \Theta = \{(a_j, \mathbf{b}_j, c_j), \ j = 1, \dots, m\}
\end{equation}

\begin{definition}[Barron norm] For functions that admit representation
    \eqref{eq:barron_represent}, its Barron norm is defined as
    \cite{eBarronSpaceFlowinduced2021}:
    \begin{equation}\label{eq:barron_norm}
        \barronnorm{f}{p} := \inf_{\rho} \Big(\ERWi{\rho}{\abs{a}^p (\norm{\mathbf{b}}_1 + \abs{c})^p}\Big)^{1/p}, \quad 1 \leq p \leq +\infty
    \end{equation}
    % where \begin{equation*} \Theta_f = \left\{ (a, \pi) \mid f(x) =
    % \int_{space} a(w) \sigma(\langle w, x \rangle)\dd{\pi(w)} \right\}.
    % \end{equation*}
\end{definition}

The infimum is taken over all probability distribution where
\eqref{eq:barron_represent} holds (\tonote{why} $\rho$ is not unique) for all $\mathbf{x} \in
    \mathcal{X}$. When $p = + \infty$, the Barron norm becomes:

\begin{equation}
    \inf_{\rho} \max_{a, \mathbf{b}, c \in \supp(\rho)} \abs{a} (\norm{\mathbf{b}}_1 + \abs{c})
\end{equation}

Barron spaces are denoted by $\mathcal{B}_p$ and defined as the set of
\textit{}{continuous} functions that admit representation in
\eqref{eq:barron_represent} with finite Barron norm.

By the definition of Barron norm, it is easy to see that:

\begin{equation}
    \mathcal{B}_{\infty} \subset \cdots \subset \mathcal{B}_{2} \subset \mathcal{B}_1
\end{equation}

% https://ocw.mit.edu/courses/18-125-measure-and-integration-fall-2003/6f21af6c40de1eccd70349bd3a3b0095_18125_lec17.pdf


The idea is similar to the inclusion of $L_p$, $L_q$ space.

Applying HÃ¶lder's inequality, for any $1 \leq p \leq q < \infty$

\begin{align*}
    \int \abs{a}^p (\norm{\mathbf{b}}_1 + \abs{c})^p d\rho
     & = \int \abs{a}^p (\norm{\mathbf{b}}_1 + \abs{c})^p \cdot 1 d\rho                                                    \\
     & \leq \Big(\int \abs{a}^{pq/p} (\norm{\mathbf{b}}_1 + \abs{c})^{pq/p} d\rho \Big)^{p/q} \Big(\int d\rho\Big)^{1-p/q} \\
     & = \Big(\int \abs{a}^q (\norm{\mathbf{b}}_1 + \abs{c})^q d\rho \Big)^{p/q} \Big(\int d\rho\Big)^{1-p/q}
\end{align*}

Therefore we have the inclusion $\mathcal{B}_{q} \subset \mathcal{B}_p$ for $1
    \leq p \leq q < \infty$. \tonote{Not sure how to justify for the case $+\infty$}

As the reverse also holds in the class of ReLU functions,  we have
$\mathcal{B}_{\infty} = \mathcal{B}_p$, $\barronnorm{\cdot}{\infty} =
    \barronnorm{\cdot}{p}$  for all $1 \leq p \leq +\infty$.

% ~\cite[Proposition 1]{eBarronSpaceFlowinduced2021}
\begin{lemma}\label{lamma:equivalence_barron_space}

    For any $f \in \mathcal{B}_1$, $f
        \,\text{also}\, \in \mathcal{B}_{\infty}$ and $\barronnorm{f}{\infty} =
        \barronnorm{f}{p}$ and hence $ \mathcal{B}_{\infty} = \cdots =
        \mathcal{B}_{2} = \mathcal{B}_1$ when $\sigma(\cdot)$ is ReLU function.
\end{lemma}

\begin{proof}
    As $f \in \mathcal{B}_1$,  there exist a probability distribution $\rho$ on
    $(\Omega, \Sigma_\Omega)$ satisfying the integral form in
    \eqref{eq:barron_represent}.

    For any $\epsilon > 0$, from the definition of Barron norm
    \eqref{eq:barron_norm}:

    \begin{equation}
        \ERWi{\rho}{\abs{a}(\norm{\mathbf{b}}_1 + \abs{c})} \leq \barronnorm{f}{1} + \epsilon
    \end{equation}

    \tonote{Why would this inequality hold? Why do you have to restrict A}

    The key point here is to construct a probability measure and then

    Here we construct two measure $\rho_+$ and $\rho_-$ on satisfying
    \begin{equation*}
        \ERWi{\rho}{\abs{a}(\norm{\mathbf{b}}_1 + \abs{c})}
    \end{equation*}


    where $\Lambda = \{(\mathbf{b}, c): \norm{\mathbf{b}}_1 + \abs{c} = 1\}$
\end{proof}

\section{Direct and Inverse Approximation Theorems}

\begin{theorem} [Direct Approximation]
    For any function $f \in \mathcal{B}$ and $m > 0$, there exists a two-layer
    neural network $f_m = f(\mathbf{x}, \Theta) = \frac{1}{m}\sum_{k=1}^m a_k
        \sigma(\mathbf{b}_k^T \mathbf(x) + c_k)$. $\Theta$ is the set of parameters
    $\Theta = \{(a_k, \mathbf{b}, c_k), k=1,\dots,m\}$ such that
    \begin{equation*}
        \norm{f - f_m}^2 \leq \frac{3\norm{f}_{\mathcal{B}}^2}{m}
    \end{equation*}

    Furthermore, we have
    \begin{equation}
        \norm{\Theta}_{path} := \frac{1}{m} \sum_{k=1}^m \abs{a_k} (\norm{\mathbf{b}_k}_1 + \abs{c_k})
        \leq 3\norm{f}_{\mathcal{B}}
    \end{equation}
\end{theorem}

The $\norm{\Theta}_{path}$ is the per-unit norm control for 2NN defined in
\cite{neyshaburNormBasedCapacityControl2015}. \tonote{add sth about the
    convexity}

\section{Difference and Connection Between Different Barron Spaces}
\label{sec:diff_barron_spaces}


As mentioned in the beginning of the chapter, we will start with the definition
of the different Barron spaces, namely the \textit{Fourier-analytic Barron
    spaces} and the \textit{infinite-width Barron spaces}. Although some
relationship between these spaces has been examined and understood partially in
\cite{eBarronSpaceFlowinduced2021,eMathematicalUnderstandingNeural2020}  and we
hope to clarify this problem in this section.

% todo: change of word

Firstly, we denote the set of all Borel probability measures on $\Omega = \R
    \times \R^d \times \R$ by $\mathcal{P}_d$.

Given a non-empty set $U \subset \R^d$, let us write

\begin{align*}
    \mathcal{B}_{H}(U) :=
     & \{f: U \to \R: \exists\: \mu \in \mathcal{P}_d, \int_{\Omega} \abs{a} d\mu(a, b, c) < \infty                           \\
     & \:\text{and}\: \forall x \in U, f(x) = a (b \cdot x + c)_+ d\mu(a, b, c) \}                                            \\
    \mathcal{B}_{ReLU}(U) :=
     & \{f: U \to \R: \exists\: \mu \in \mathcal{P}_d, \int_{\Omega} \abs{a} \cdot (\abs{b} + \abs{c}) d\mu(a, b, c) < \infty \\
     & \:\text{and}\: \forall x \in U, f(x) = a \sigma(b \cdot x + c) d\mu(a, b, c) \}                                          \\
    \mathcal{B}_{\mathcal{F},s}(U) :=
     & \{f: U \to \R: \exists\: F: \R^d \to \mathbb{C} \text{Fourier norm} < \infty                                           \\
     & \:\text{and}\: \forall x \in U, f(x) \text{has a Fourier representation} \}                                            \\
\end{align*}

As shown in section \ref{sec:barron_norm}, $\mathcal{B}_{ReLU}(U)$ is the Barron
space ($\mathcal{B}_1$) defined above and we can see that the differences
between the $\mathcal{B}_{ReLU}(U)$ and $\mathcal{B}_{H}(U)$ are the activation
function and the respective norm.

We state the following known properties from the literature.

% hookrightarrow or subset
\begin{lemma}
    Give a non-empty subset $U$ from $\R^d$, the following relationship holds:

    1) $\mathcal{B}_{ReLU}(U) \subset \mathcal{B}_{H}(U)$

    2) $\mathcal{B}_{\mathcal{F}, 1}(U) \subset \mathcal{B}_H(U)$

    3) $\mathcal{B}_{\mathcal{F}, 2}(U) \subset \mathcal{B}_{ReLU}(U)$
\end{lemma}


\begin{proof}

\end{proof}

\begin{remark}
    It has been discussed in earlier section that we have the equivalence of
    Barron norm and hence Barron spaces for $1 \leq p\leq +\infty$ (see
    \eqref{eq:barron_fouier_int}) with ReLU as activation function.

    In other words, combined with the fact that $\mathcal{B}_{\infty} =
        \mathcal{B}_p$, we can say that the class of functions where $f$ admits
        a Fourier representation with finite second moment ($v_{f, 2} < \infty$)
        are well ``contained'' inside the \textit{infinite-width Barron spaces}.
        However, $\mathcal{B}_{\mathcal{F}, 1}(U)$ still encapsulates a boarder
        class of functions.
\end{remark}


\begin{lemma}~\cite[Proposition 7.4]{carageaNeuralNetworkApproximation2022}

    Let $U \subset \mathbb{R}^d$ be bounded and have nonempty interior. For
    $s>0$, if $\mathcal{B}_{\mathcal{F},s}(U) \subset \mathcal{B}_{1}(U)$, then
    $s \geq 2$. In particular $\mathcal{B}_{\mathcal{F},1}(U) \not\subset
        \mathcal{B}_{1}(U)$
\end{lemma}

\begin{proof}

\end{proof}



% \section{Minimax Lower Bounds for Two Neural Network Model}

% For simplicity, data are of the form $\{(X_i, Y_i)\}_{i=1}^n$ from a

% For functions $f$ in the class $\mathcal{F}: [-1, 1]^d \to \mathcal{R}$, the
% minimax risk is:

% \begin{equation} R_{n,d} := \inf_{\hat{f}} \sup_{f\in \mathcal{F}}
%     \ERW{\norm{f - \hat{f}}^2} \end{equation}

% It has been shown in \TOCITE(Barron Minimax paper) that the minimax lower
% bound for neural nets is of order $\bigO(\log{d}/n)$ to some fractional power
% when $d$ is of larger order than $n$.

% \section{Risk Bounds for }

% {\itshape Firstly, I need to understand the difference between approximation
% rates and estimation of population risk.

% There exists a vast dictionary of definitions with similar wordings but the
% context is hughly different.

% e.g. What's the estimate of the population risk? In my understanding,

% $f^* - f_m$ where $f_m$ is the best solution in such hypothesis class is the
% approximation rates

% It should be that Population risk is w.r.t. a particular loss function?}

% \begin{equation} f(x) = \sum_{i=1}^m a_i \phi (\spr{\mathbf{w_i}}{x})
%     \end{equation}





%%% Local Variables: %% mode: latex %% TeX-master: "MasterThesisSfS" %% End: 
