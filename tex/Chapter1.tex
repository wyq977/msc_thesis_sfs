\chapter{The Approximation Properties of Two-Layer Neural Networks}

In this chapter, we will introduce some basic concepts about artificial
artificial neural networks with an emphasis on the two-layer neural networks
(\gls{2nn}). Firstly, we will define the 2NN model and the problem setup. In
section \ref{sec:uat}, we will state the result of the universal approximation
theorem. Later in section \ref{sec:spectral_norm} and \ref{sec:barron_norm}, we
formally introduce the function spaces associated with 2NN, along with the
definition of the \textit{spectral norm} by
\cite{barronUniversalApproximationBounds1993} and the \textit{Barron norm} by
\cite{eBarronSpaceFlowinduced2021}. Despite the term ``Barron space'' and
``Barron norm'' has been coined and mentioned earlier, the notion of different
Barron spaces and their relationship have not been made obvious. To avoid
confusion, we here borrow the terminology adopted by
\cite{carageaNeuralNetworkApproximation2022}: we refer the spaces associated
with the spectral norm as the \textit{Fourier-analytic Barron spaces} and the
spaces defined using the Barron norm as the \textit{infinite-width Barron
spaces} as it essentially consist of ``infinitely wide'' neural networks (with
some restrictions on the parameters). Finally, the relationship (mainly
inclusion) between the Fourier-analytic Barron spaces and the infinite-width
Barron spaces is examined in section \ref{sec:diff_barron_spaces}. The aim of
this chapter is to provide a concise summary of various well-known results
concerning the approximation properties of \ and we recommend work by
\cite{eMathematicalUnderstandingNeural2020,bernerModernMathematicsDeep2021} for
a comprehensive introduction.

% classical Barron space, or the Fourier-analytic Barron space

% infinitely wide

% infinite-width Barron spaces.


\section{Feed-forward Neural Networks and Two-Layer Neural Networks}

This section begins with a brief introduction on \textit{feed-forward neural
networks}. Although most targeted applications are utilizing other architectures,
e.g. recurrent neural network \TOCITE, fully connected feed-forward neural
networks allows us to connect the trade-off between model complexity and
approximation efficiency conveniently.

A feed-forward neural network $\mathcal{N}$ is a class of artificial neural
network (\gls{ann}) equipped with a directed acyclic graph (\gls{dag}),
$\mathcal{G} = (\mathcal{V}, \mathcal{E})$. The associated graph $\mathcal{G}$
is the \textit{architecture} of $\mathcal{N}$ represented by a finite collection
of vertices \footnote{
    A computation unit associated with each $v \in \mathcal{V} \setminus \mathcal{I}$
    is called a \textit{node} or \textit{neuron}. To unify terminology, we will 
    refer as node going forward.
}
$\mathcal{V}$ and a finite set of edges $\mathcal{E}$. If we
represent the input and output layer by $\mathcal{I, O}$, we have the following
basic properties: 
i) a activation function $\sigma$ is associated with each 
$v \in \mathcal{V} \setminus \mathcal{I}$ 
ii) a scalar $w$ is associated with each $e \in \mathcal{E}$.

Nodes in the input layer observe a scalar input, seen by the downstream nodes.
Aside from $\mathcal{I}$, a single node takes a superpositions of the upstream
nodes via the respective weights $w_e$ mediated by the edges $e = (v, v')$
between the associated vertex $v$ and upstream vertices $v'$. 

We can then view the output function $f_{\mathcal{N}}: \R^d \to \R^{d'}$ as a
mapping from $\R^d$ to $\R^{d'}$, $d = \abs{\mathcal{I}}, d' =
\abs{\mathcal{O}}$. With a fixed architecture, the family of functions produced
by $\mathcal{N}$ is determined by the number of trainable parameters: $\{w_e,
b_v\}, e \in \mathcal{E}, v \in \mathcal{V} \setminus \mathcal{I}$

\subsection{Fully connected networks}

The general definition stated covers virtually all networks architecture
encountered in practice. In our discussion, we will narrow our focus to a
specialized class, \textit{fully connected networks}, where the vertices are
structured into layers.

The \textit{input layer} consists of simply $d$ input vertices where each vertex
receive a external scalar signal. Similarly, the \textit{output layer} consists
of $d'$ output vertices. $L$ hidden layers follows the input layer and each
layer constituted of $n_j$ hidden vertices and the integer $n_j$ is called the
\textit{width} of the $j$th layer. Within a fully connected network, each vertex
is connected only to the proceeding layer and the forthcoming layer.

There is a activation function $\sigma$ associated with each hidden vertex and
identity function is assigned at the output vertices to ensure the output
function from $\mathcal{N}$ is a linear combination at the layer $L$ with a bias
term. Therefore, the \textit{weight matrices} and the \textit{bias vectors} are
sufficient to describe the output function $f_{\mathcal{N}}$ with a fixed
architecture.

The main interest of this section is to summarize the approximation power of
ReLU networks, specifically ReLU networks. The problem of finding a stable
algorithm to produce such approximation is intentionally omitted.

The two main direction of discussing the approximation power are concerned with
the width and the depth. The first case is to focus on the approximation as the
width approaches infinity in a shallow network and we will see that even this
case is fairly complicated even with one or two hidden layers. The other extreme
would be deep neural networks where the width of the networks are usually fixed
for simplicity and the depth $L$ is taken to infinity. 

\section{Universal Approximation Theorem}
\label{sec:uat}

In general, neural networks are good 


Let $f_m(\mathbf{x}, \Theta)$ be the parameterized family of two-layer neural
networks of $m$ nodes that which map input vector $\mathbf{x}$ of dimension $d$.

\begin{equation}
    f_m(\mathbf{x}, \Theta) = \sum_{i=1}^m a_i \sigma(\mathbf{b_i}^T\mathbf{x} + c_i)
\end{equation}

where $\Theta = (a_1, \mathbf{b}_1, c_1, \dots, a_m, \mathbf{b}_m, c_m)$ denotes
all the parameters (the total number of parameters is $(d+2)m + 1$). We will
consider the hypothesis spaces where the activation functions $\sigma$ is ReLU
\footnote{\tonote{The choice of activation function is important for
        infinite-width Barron spaces}}.

%  homogeneity

It has been shown that shallow neural networks (even with one hidden layer) have
good approximation properties with mild conditions on the activation function.
2NNs are good approximators in a sense that they can approximate any continuous
function arbitrarily well by
\cite{cybenkoApproximationSuperpositionsSigmoidal1989}. Despite its importance,
the theorem \ref{thm:uat} do not provide quantitative information regarding the
approximation error.

\begin{definition}[sigmoidal function]\label{def:sigmoidal}
    We stated a function $\sigma$ is \textit{sigmoidal} if
    \begin{equation}
        \sigma(t) =
        \begin{cases}
            1 & \text{as} \quad t \to +\infty \\
            0 & \text{as} \quad t \to -\infty
        \end{cases}
    \end{equation}
\end{definition}

\begin{theorem}\label{thm:uat}
    \cite[Theorem 5]{cybenkoApproximationSuperpositionsSigmoidal1989}
    If the activation $\sigma$ is sigmoidal as defined in Def. $\ref{def:sigmoidal}$, then any continuous functions on
    $[0, 1]^d$ in the can be approximated uniformly by two layer neural network
    networks.
\end{theorem}


\section{Add a section for F transform}

\section{Spectral Norm and Fourier-analytic Barron spaces}
\label{sec:spectral_norm}

It is shown by \cite{barronUniversalApproximationBounds1993} that functions with
finite Fourier moment can be approximated with the superpositions of sigmoidal
functions at a rate independent of the dimensionality $d$. In other words, for
any functions in said class can be approximated with a (shallow) neural network
at error rate $\bigO(m^{-1} \cdot C)$ where $m$ is the number of nodes in the
the single hidden layer and $C$ is some constant dependent on the smoothness of
the target function.

We formulated the class of functions introduced in
\cite{barronNeuralNetApproximation1992, barronUniversalApproximationBounds1993}
below.

\begin{definition}
    Let $X$ be a bounded set in $\mathbb{R}^d$, a function $f: X \to \mathbb{R}$
    is said to be in \textit{Barron class} with constant $C > 0$, if there are
    $x_0$ in $X$ and $c \in [-C, C]$ and a measurable function $F: \mathbb{R}^d
        \to \mathbb{C}$ satisfying:

    \begin{align}
         & \int_{\mathbb{R}^d} \abs{\omega}_{X, x_0} \cdot \abs{F(\omega)} d\omega < C                            \\
         & f(x) = c + \int_{\mathbb{R}^d} (e^{i\spr{x}{\omega}} - e^{i\spr{x_0}{\omega}}) \cdot F(\omega) d\omega
    \end{align}

    where $\abs{\omega}_{X, x_0} := \sup_{x\in X}\abs{\spr{\omega}{x - x_0}}$.
    We refer the class of all functions as $\Gamma_C(X, x_0)$.
\end{definition}

\begin{theorem}\cite[Theorem~1]{barronUniversalApproximationBounds1993}\label{thm:barron_1993_1}
    For every function in $\Gamma_C(X, x_0)$, every sigmoidal function $\phi$,
    every probability measure $\mu$, and every $n \geq 1$, there exists a linear
    combination of sigmoidal function $f_n(x)$ of the form, such that
    \begin{equation}
        \int_X(f(x) - f_n(x))^2 \mu(dx) \leq \frac{(2C)^2}{n}
    \end{equation}
\end{theorem}


\begin{proof}

    The main ideas behind the the proof of Theorem \ref{thm:barron_1993_1} is to
    show functions with finite Fourier moment are in the closure of the convex
    hull of the set of half planes and law of large numbers.

    \textbf{Step 0} (\textit{Fixing $x_0$}): Changing $x_0$ to $x_1$ affects the
    norm, at most, by a factor of $2$. Let $x_0, x_1 \in X$, and $f$ fulfilling
    the assumption above, that is $f \in \Gamma_C(X, x_0)$. For any $\omega \in
        \mathbb{R}^d$, given $x_0, x_1$, we have

    \begin{equation}
        \abs{\omega}_{X, x_0} = \sup_{x\in X}\abs{\spr{\omega}{x-x_1}} \leq \sup_{x\in X}\abs{\spr{\omega}{x-x_0}} + \abs{\spr{\omega}{x_0-x_1}} \leq 2\abs{\omega}_{X, x_1}
    \end{equation}

    Thus we have $\int_{\mathbb{R}^d} \abs{\omega}_{X, x_0} \cdot
        \abs{F(\omega)} d\omega < 2C$. If we have $\tilde{c} = c +
        \int_{\mathbb{R}^d} (e^{\spr{\omega}{x_0}} - e^{\spr{\omega}{x_1}})
        d\omega$, then $f(x) = \tilde{c} + \int_{\mathbb{R}^d} (e^{i\spr{x}{\omega}}
        - e^{i\spr{x_1}{\omega}})$ with $\tilde{c} \leq 2C$.

    This shows that changing $x_0$ would only affect the norm in the RHS of
    Theorem \ref{thm:barron_1993_1} at most by a factor of two, i.e.
    $\Gamma_C(X, x_0) \subset \Gamma_{2C}(X, x_1)$. Therefore, we continue the
    proof assuming $x_0 = 0$ for simplicity.

    \textbf{Step 1} (\textit{Represent $f$ via Inverse Fourier Transform}): From
    the assumption, and the fact that $f$ is real-valued ($X \in \mathbb{R}^d
        \to \mathbb{R}$), the real number part can be written as:

    Note that with polar decomposition $F(\omega) = e^{i\theta(\omega)} \cdot
        \abs{F(\omega)}$ where $\theta(\omega) \in \mathbb{R}$ denote the magnitude
    decomposition.

    \begin{align}
        f(x) - f(0)
         & = \Re \int (e^{i\spr{\omega}{x}} - e^{i\spr{\omega}{0}}) e^{i\theta(\omega)} \cdot F(\omega) d\omega                              \\
         & = \int_{\Omega}\Big(\cos(\spr{\omega}{x} + \theta(\omega)) - \cos(\theta(\omega))\Big)
        F(\omega) d\omega                                                                                                                    \\
         & = \int_{\Omega} \frac{C_{f,X}}{\abs{\omega}_{X, 0}}\Big(\cos(\spr{\omega}{x} + \theta(\omega)) - \cos(\theta(\omega))\Big) d\mu_g \\
         & = \int_{\Omega} g(x, \omega) d\mu_g \label{eq:barron_fouier_int}
    \end{align}

    $C_{f,X}$ is a constant defined with $f$ and $X$: $C_{f,X} =
        \int_{\mathbb{R}^d} \abs{\omega}_{X, 0} \cdot \abs{F(\omega)} d\omega \leq
        C$ and $\mu_g$ is a probability distribution $d\mu_g =
        \abs{\omega}_{X,0}/C_{f,X}\abs{F(\omega)} d\omega$, the integral is
    evaluated on $\Omega = \{\omega \in \mathcal{R}^d: \omega \not = 0\}$ and

    \begin{equation}
        g(x, \omega) = \frac{C_{f,X}}{\abs{\omega}_{X, 0}}\Big(\cos(\spr{\omega}{x} + \theta(\omega)) - \cos(\theta(\omega))\Big)
    \end{equation}

    \textbf{Step 2} (\textit{$f(x) - f(0)$ is in the closure of the convex hull
        of $G_{cos}$}) The integral form in (\ref{eq:barron_fouier_int}) shows that
    $f(x) - f(0)$ can be represented as an infinite convex combination of
    functions in the class

    \begin{equation}
        G_{cos} = \Bigg\{\frac{\abs{\gamma}}{\abs{\omega}_{X, 0}}\Big(\cos(\spr{\omega}{x} + b) - \cos(b)\Big): \omega \not= 0, \abs{\gamma} \leq C, b \in \mathbb{R} \Bigg\}
    \end{equation}

    Suppose we have drawn $n$ samples ($\{\omega_i, i = 1,\dots, n\}$) from
    $\mu_g$, the expected norm in $L_2(\mu_g, X)$ converges to zero as $n \to
        \infty$ by $L_2$ law of large numbers. Therefore, there exist a sequence of
    convex combination in $G_{cos}$ that converges to $f(x) - f(0)$ in $L_2$.


    \textbf{Step 3} (\textit{$G_{cos}$ is in the closure of the convex hull of
        $G_{step}$}): It is sufficient to check $g(z), z = \alpha x, \alpha =
        \omega/\abs{\omega}_{X,0}$ on $[-1, 1]$ for some $\omega$. As $g(z)$ is a
    uniformly continuous sinusoidal function on $[-1, 1]$, it can be uniformly
    approximately by piecewise constant step function.

    Restricting $g(z)$ on $[0, 1]$, for a partition ${0 \leq p_1 \leq p_2 \leq
                \cdots \leq p_k = 1}$, define

    \begin{align}
        g_{k,+}(z) = \sum_{i=1}^{k-1} \Big(g(p_i) - g(p_{i-1})\Big) \cdot
        \stepfunction{z\geq p_i}
    \end{align}

    Similarly, we can construct $g_{k,-}(z) = \sum_{i=1}^{k-1} (g(-p_i) -
        g(-p_{i-1})) \cdot \stepfunction{z\leq -p_i}$, results in a sequence of
    piecewise step function on $[-1, 1]$ uniformly close to $g(z)$. We have
    $g(z) = g_{k,+}+g_{k,-}$, a linear combination of step function (or
    heaviside function) and the sum of the coefficients is bounded by 2C (The
    sum of coefficients of $g_{k,+}$ is bounded by $C$ as a result of the
    derivative of $g$ bounded by $C$, so does $g_{k,-}$ and hence $2C$).

    We can see that functions $g(z)$ are in the closure of the convex hull of
    the step functions (by Lemma 1 in
    \cite{barronUniversalApproximationBounds1993})

    By substituting $z = \omega/\abs{\omega}_{X, 0} x$, we have $G_{cos} \subset
        G_{step}$,
    \begin{equation}
        G_{step} = \Bigg\{ \gamma\stepfunction{\alpha x-t}: \abs{\gamma} \leq 2C, \abs{t} \leq 1, \abs{\alpha}_{X} = 1\Bigg\}
    \end{equation}

    \textbf{Step 4} (\textit{Closure of $G_{\phi}$}) There exists a sequence of
    sigmoidal function $\phi(\abs{c}(\alpha x - t))$, as $\abs{c} \to \infty$,
    it converges to step functions pointwise (except at points where $\alpha x -
        t = 0$). If we introduce a measure $\mu$ that has zero measure at those
    points, previous statement on $G_{cos} \subset G^{\mu}_{step}$ still holds
    on $\{\abs{t} \leq 1: \alpha x - t \not=0\}$ give a particular $\alpha$. We
    subsequently have convergence in $L_2(\mu, X)$ by dominated convergence
    theorem, which implies that functions in $G^{\mu}_{step} \subset G_{\phi}$.

    Finally we have the following relationship since the closure of a convex set
    is also convex.

    \begin{equation*}
        \Gamma_{X, x_0} \subset \closure{G_{cos}} \subset \closure{G_{step}} \subset \closure{G_{\phi}}
    \end{equation*}

    \cite[\textit{Lemma~1}]{barronUniversalApproximationBounds1993}: If $f$ is
    in the closure of the convex hull of a set $G$ in a Hibert space with norm
    $\norm{\cdot}$ and every function $g \in G$ is bounded by some constant
    $C_G$. Then for every $N \geq 1$, and every constant $C' > C_g^2 -
        \norm{f}^2$, there is a sequence $\{f_i, i = 1, \dots, N\}$ in the convex
    hull in $G$ such that:
    \begin{equation}
        \norm{f - f_N}^2 \leq \frac{C'}{n}
    \end{equation}

    $f_N = \sum_{i=1}^N \lambda_i \cdot f_i, \quad \sum_{i=1}^N \lambda_i = 1$

    % Lemma 1 in \cite{barronUniversalApproximationBounds1993} showed that
    % function in a closure of the convex hull of a set in a Hilbert space can
    % be approximated with a sequence of functions from such closure and the
    % norm between the function and the sequences $\{f_i, i = 1, \dots, N\}$ are
    % bounded in a magnitude of $\bigO(N^{-1})$.

    As shown above that function $f(x) - f(0)$ are in the closure of the convex
    hull of $G_{\phi}$ where $\norm{g} \leq (2C)^2$ for every $g \in G_{\phi}$.
    For any choice of $C' > (2C)^2 - \norm{f(x) - f(0)}^2$, we have the $L_2$
    norm of the approximation error is bounded.
    % Suppose we restrict $t$ t

    % Suppose we now restrict $t$ to the continuity point induced by measure
    % $\mu$ in We can check that the functions in $G_{step}$ are in the closure
    % of the convex hull of 

    % \textbf{Step 3} (\textit{Putting it together}): We can further show
    % $G_{cos}$ are in the class of sigmoidal functions. 

    % Theorem 2 in \cite{barronUniversalApproximationBounds1993}, we have that

\end{proof}

% A smoothness property of the function to be approximated is expressed in terms
% of its Fourier representation. In particular, an average of the norm of the
% frequency vector weighted by the Fourier magnitude distribution is used to
% measure the extent to which the function oscillates. In this Introduction, the
% result is presented in the case that the Fourier distribution has a density that
% is integrable as well as having a finite first moment. Somewhat greater
% generality is permitted in the theorem stated and proven in Sections III and IV.

% In addition to the smoothness property ensured via the finite first moment,

In the above result, the approximation error is presented in the case where the
smoothness of the functions is ensured by the fact that functions in such class
has a integrable Fourier distribution, as well as finite first moment.
Naturally, we would like to extend the findings with tighter restriction on the
smoothness at a cost of smaller hypothesis space.

We start with the defining the \textit{Fourier-anlytic norm}, $v_{f, s}\, s \geq
    1$, for function $f$ that has a Fourier representation:
\begin{align}
    f(x)     & = \int_{\mathbb{R}^d} e^{i\spr{x}{\omega}} F(\omega) d\omega \label{eq:fourier_rep}           \\
    v_{f, s} & = \int_{\mathbb{R}^d} \norm{\omega}_1^s \cdot \abs{F(\omega)} d\omega \label{eq:fourier_norm}
\end{align}


\begin{theorem}\label{thm:appr_f2}
    Consider the class of function on $\mathbb{R}^d$ for which there is a
    Fourier representation of the form above. If $v_{f, 2}$ is finite, there
    exists a two-layer ReLU neural network $f_m$ of $m$ nodes $f_m(\mathbf{x})
        = \sum_{i=1}^m a_i \sigma(\mathbf{b_i}^T\mathbf{x} + c_i)$ with $\norm{b_i}
        = 1$ and $\abs{c_i}\leq 1$ such that
    \begin{equation}
        \norm{f
            - \mathbf{x} \cdot \nabla f(0)
            - f(0)
            - f_m}^2 \leq \frac{16 v_{f, 2}^2}{m}
    \end{equation}
\end{theorem}

\textbf{What is the diff between the above and below?}

Other than the norm $\ell^2$, what is the difference and where does the $d$ come
about? This is taken from \cite{klusowskiApproximationCombinationsReLU2018}.

\begin{theorem}
    Consider only the unit ball in $\mathbb{R}^d$, function $f$ that admits a
    Fourier representation as above with finite $v_{f_2}$. There exists a $f_m$:

    \begin{equation}
        f_m(x) = a_0 + \mathbf{b}_0^T \mathbf{x} + \frac{v}{m} \sum_{i=1}^m a_i \sigma(\mathbf{b}_i^T\mathbf{x} + c_i)
    \end{equation}

    with $a_i \in [-1,1], \norm{\mathbf{b}_i}_1 = 1, a_0 = f(0), \mathbf{b}_0^T
        = \nabla f(0), v < 2v_{f,2}$ such that

    % Let $D = \[-1, 1\]^d$, target function $f$ has a Fourier representation as 
    % above with finite $v_{f, 2}$. There exists a $f_m$ 

    \begin{equation}
        \sup_{\mathbf{x} \in D} \abs{f(x) - f_m(x)} \leq c v_{f,2} \sqrt{d+\log{m}} \, m^{-1/2-1/d}
    \end{equation}

    for some universal $c > 0$.
\end{theorem}

Similar bound are also attained with stronger assumption on functions'
smoothness via $v_{f,s}$.

\begin{theorem}\label{thm:appr_f3} Consider the class of function on
    $\mathbb{R}^d$ for which there is a Fourier representation of the form
    above. If $v_{f, 3}$ is finite, there exists a two-layer neural network
    $f_m$ with squared ReLU ridge function as activation function of $m$ nodes
    $f_m(\mathbf{x}) = \sum_{i=1}^m a_i \sigma(\mathbf{b_i}^T\mathbf{x} + c_i)$
    with $\norm{b_i} = 1$ and $\abs{c_i}\leq 1$ such that
    \begin{equation}
        \norm{f
            - \mathbf{x}^T H_f(0) \mathbf{x}/2
            - \mathbf{x} \cdot \nabla f(0)
            - f(0)
            - f_m}^2 \leq \frac{16 v_{f, 3}^2}{m}
    \end{equation}

    where $H_f(0)$ is the Hessian of $f$ at the point zero.
\end{theorem}

\begin{theorem}
    Similar to the settings above. $f$ with finite $v_{f,3}$. There exists a
    $f_m$:
    \begin{equation}
        f_m(x) = a_0 + \mathbf{b}_0^T \mathbf{x} + \mathbf{x}^T A_0 \mathbf{x}+ \frac{v}{m} \sum_{i=1}^m a_i \sigma(\mathbf{b}_i^T\mathbf{x} + c_i)
    \end{equation}

    with $a_i \in [-1,1], \norm{\mathbf{b}_i}_1 = 1, a_0 = f(0), \mathbf{b}_0^T
        = \nabla f(0), A_0 = \nabla\nabla^T f(0), v < 2v_{f,3}$ such that

    \begin{equation}
        \sup_{\mathbf{x} \in D} \abs{f(x) - f_m(x)} \leq c v_{f,3} \sqrt{d+\log{m}} \, m^{-1/2-1/d}
    \end{equation}

    for some universal $c > 0$. Furthermore, if the $a_i$ are restricted to
    $\{-1, 1\}$, the upper bound is of order $v_{f,3} \sqrt{d} \,
        m^{-1/2-1/(d+2)}$
\end{theorem}

% Considering the set of all Borel probability measures of $\Omega = (\mathbb{R}^1
%     \times \mathbb{R}^d \times \mathbb{R}^1)$ (denoted by $\mathcal{P}_d$), given a
% nonempty bounded subset $U \subset \mathbb{R}^d$

% \begin{align*}
%     \mathcal{B}_{p}(U)              & := \left\{f: U \to \mathbb{R} \mid \exists\, \rho \in \mathcal{P}_d, \barronnorm{f}{p} < \infty \, \text{and $f$ is in the form \eqref{eq:barron_represent}}\right\} \\
%     \mathcal{B}_{\mathcal{F}, s}(U) & := \left\{f: U \to \mathbb{R} \mid \exists\, \rho \in \mathcal{P}_d, v_{f, s} < \infty \, \text{and $f$ is in the form \eqref{eq:fourier_rep}}\right\}
% \end{align*}


\newpage

\section{Barron Norm and infinite-width Barron spaces}
\label{sec:barron_norm}

Here in the section introduce the definition of Barron space and its properties.
It should be noted that the term ``Barron space'' was coined by
\cite{ePrioriEstimatesPopulation2019} to honor Prof. Andrew Barron's
contribution in the understanding of neural nets.

For a function $f: \mathcal{X} \subset \mathbb{R}^d \to \mathbb{R}$, we consider
those that admit the following integral representation:

\begin{equation}\label{eq:barron_represent}
    f(\mathbf{x}) = \int_{\Omega} a \sigma(\mathbf{b}^T\mathbf{x} + c) \rho(da, d\mathbf{b}, dc), \quad \mathbf{x} \in \mathcal{X}
\end{equation}

where $\rho$ is a probability distribution on $(\Omega, \Sigma_\Omega)$, $\Omega
    = \mathbb{R}^1 \times \mathbb{R}^d \times \mathbb{R}^1$ and $\sigma(\cdot) =
    \max\{\cdot, 0\}$ is the ReLU activation function.

The representation can be seen as a continuum analogy of the 2NN with $m$ hidden
nodes:

\begin{equation}
    f_m(\mathbf{x}, \Theta) := \frac{1}{m} \sum_{j=1}^m a_j \sigma(\mathbf{b}^T\mathbf{x} + c_j), \quad \Theta = \{(a_j, \mathbf{b}_j, c_j), \ j = 1, \dots, m\}
\end{equation}

\begin{definition}[Barron norm] For functions that admit representation
    \eqref{eq:barron_represent}, its Barron norm is defined as
    \cite{eBarronSpaceFlowinduced2021}:
    \begin{equation}\label{eq:barron_norm}
        \barronnorm{f}{p} := \inf_{\rho} \Big(\ERWi{\rho}{\abs{a}^p (\norm{\mathbf{b}}_1 + \abs{c})^p}\Big)^{1/p}, \quad 1 \leq p \leq +\infty
    \end{equation}
    % where \begin{equation*} \Theta_f = \left\{ (a, \pi) \mid f(x) =
    % \int_{space} a(w) \sigma(\langle w, x \rangle)\dd{\pi(w)} \right\}.
    % \end{equation*}
\end{definition}

The infimum is taken over all probability distribution where
\eqref{eq:barron_represent} holds (\tonote{why} $\rho$ is not unique) for all $\mathbf{x} \in
    \mathcal{X}$. When $p = + \infty$, the Barron norm becomes:

\begin{equation}
    \inf_{\rho} \max_{a, \mathbf{b}, c \in \supp(\rho)} \abs{a} (\norm{\mathbf{b}}_1 + \abs{c})
\end{equation}

Barron spaces are denoted by $\mathcal{B}_p$ and defined as the set of
\textit{}{continuous} functions that admit representation in
\eqref{eq:barron_represent} with finite Barron norm.

By the definition of Barron norm, it is easy to see that:

\begin{equation}
    \mathcal{B}_{\infty} \subset \cdots \subset \mathcal{B}_{2} \subset \mathcal{B}_1
\end{equation}

% https://ocw.mit.edu/courses/18-125-measure-and-integration-fall-2003/6f21af6c40de1eccd70349bd3a3b0095_18125_lec17.pdf


The idea is similar to the inclusion of $L_p$, $L_q$ space.

Applying Hölder's inequality, for any $1 \leq p \leq q < \infty$

\begin{align*}
    \int \abs{a}^p (\norm{\mathbf{b}}_1 + \abs{c})^p d\rho
     & = \int \abs{a}^p (\norm{\mathbf{b}}_1 + \abs{c})^p \cdot 1 d\rho                                                    \\
     & \leq \Big(\int \abs{a}^{pq/p} (\norm{\mathbf{b}}_1 + \abs{c})^{pq/p} d\rho \Big)^{p/q} \Big(\int d\rho\Big)^{1-p/q} \\
     & = \Big(\int \abs{a}^q (\norm{\mathbf{b}}_1 + \abs{c})^q d\rho \Big)^{p/q} \Big(\int d\rho\Big)^{1-p/q}
\end{align*}

Therefore we have the inclusion $\mathcal{B}_{q} \subset \mathcal{B}_p$ for $1
    \leq p \leq q < \infty$. \tonote{Not sure how to justify for the case $+\infty$}

As the reverse also holds in the class of ReLU functions,  we have
$\mathcal{B}_{\infty} = \mathcal{B}_p$, $\barronnorm{\cdot}{\infty} =
    \barronnorm{\cdot}{p}$  for all $1 \leq p \leq +\infty$.

% ~\cite[Proposition 1]{eBarronSpaceFlowinduced2021}
\begin{lemma}\label{lamma:equivalence_barron_space}

    For any $f \in \mathcal{B}_1$, $f
        \,\text{also}\, \in \mathcal{B}_{\infty}$ and $\barronnorm{f}{\infty} =
        \barronnorm{f}{p}$ and hence $ \mathcal{B}_{\infty} = \cdots =
        \mathcal{B}_{2} = \mathcal{B}_1$ when $\sigma(\cdot)$ is ReLU function.
\end{lemma}

\begin{proof}
    As $f \in \mathcal{B}_1$,  there exist a probability distribution $\rho$ on
    $(\Omega, \Sigma_\Omega)$ satisfying the integral form in
    \eqref{eq:barron_represent}.

    For any $\epsilon > 0$, from the definition of Barron norm
    \eqref{eq:barron_norm}:

    \begin{equation}
        \ERWi{\rho}{\abs{a}(\norm{\mathbf{b}}_1 + \abs{c})} \leq \barronnorm{f}{1} + \epsilon
    \end{equation}

    \tonote{Why would this inequality hold? Why do you have to restrict A}

    The key point here is to construct a probability measure and then

    Here we construct two measure $\rho_+$ and $\rho_-$ on satisfying
    \begin{equation*}
        \ERWi{\rho}{\abs{a}(\norm{\mathbf{b}}_1 + \abs{c})}
    \end{equation*}


    where $\Lambda = \{(\mathbf{b}, c): \norm{\mathbf{b}}_1 + \abs{c} = 1\}$
\end{proof}

\section{Direct and Inverse Approximation Theorems}

\begin{theorem} [Direct Approximation]
    For any function $f \in \mathcal{B}$ and $m > 0$, there exists a two-layer
    neural network $f_m = f(\mathbf{x}, \Theta) = \frac{1}{m}\sum_{k=1}^m a_k
        \sigma(\mathbf{b}_k^T \mathbf(x) + c_k)$. $\Theta$ is the set of parameters
    $\Theta = \{(a_k, \mathbf{b}, c_k), k=1,\dots,m\}$ such that
    \begin{equation*}
        \norm{f - f_m}^2 \leq \frac{3\norm{f}_{\mathcal{B}}^2}{m}
    \end{equation*}

    Furthermore, we have
    \begin{equation}
        \norm{\Theta}_{path} := \frac{1}{m} \sum_{k=1}^m \abs{a_k} (\norm{\mathbf{b}_k}_1 + \abs{c_k})
        \leq 3\norm{f}_{\mathcal{B}}
    \end{equation}
\end{theorem}

The $\norm{\Theta}_{path}$ is the per-unit norm control for 2NN defined in
\cite{neyshaburNormBasedCapacityControl2015}. \tonote{add sth about the
    convexity}

\section{Difference and Connection Between Different Barron Spaces}
\label{sec:diff_barron_spaces}


As mentioned in the beginning of the chapter, we will start with the definition
of the different Barron spaces, namely the \textit{Fourier-analytic Barron
    spaces} and the \textit{infinite-width Barron spaces}. Although some
relationship between these spaces has been examined and understood partially in
\cite{eBarronSpaceFlowinduced2021,eMathematicalUnderstandingNeural2020}  and we
hope to clarify this problem in this section.

% todo: change of word

Firstly, we denote the set of all Borel probability measures on $\Omega = \R
    \times \R^d \times \R$ by $\mathcal{P}_d$.

Given a non-empty set $U \subset \R^d$, let us write

\begin{align*}
    \mathcal{B}_{H}(U) :=
     & \{f: U \to \R: \exists\: \mu \in \mathcal{P}_d, \int_{\Omega} \abs{a} d\mu(a, b, c) < \infty                           \\
     & \:\text{and}\: \forall x \in U, f(x) = a (b \cdot x + c)_+ d\mu(a, b, c) \}                                            \\
    \mathcal{B}_{ReLU}(U) :=
     & \{f: U \to \R: \exists\: \mu \in \mathcal{P}_d, \int_{\Omega} \abs{a} \cdot (\abs{b} + \abs{c}) d\mu(a, b, c) < \infty \\
     & \:\text{and}\: \forall x \in U, f(x) = a \sigma(b \cdot x + c) d\mu(a, b, c) \}                                          \\
    \mathcal{B}_{\mathcal{F},s}(U) :=
     & \{f: U \to \R: \exists\: F: \R^d \to \mathbb{C} \text{Fourier norm} < \infty                                           \\
     & \:\text{and}\: \forall x \in U, f(x) \text{has a Fourier representation} \}                                            \\
\end{align*}

As shown in section \ref{sec:barron_norm}, $\mathcal{B}_{ReLU}(U)$ is the Barron
space ($\mathcal{B}_1$) defined above and we can see that the differences
between the $\mathcal{B}_{ReLU}(U)$ and $\mathcal{B}_{H}(U)$ are the activation
function and the respective norm.

We state the following known properties from the literature.

% hookrightarrow or subset
\begin{lemma}
    Give a non-empty subset $U$ from $\R^d$, the following relationship holds:

    1) $\mathcal{B}_{ReLU}(U) \subset \mathcal{B}_{H}(U)$

    2) $\mathcal{B}_{\mathcal{F}, 1}(U) \subset \mathcal{B}_H(U)$

    3) $\mathcal{B}_{\mathcal{F}, 2}(U) \subset \mathcal{B}_{ReLU}(U)$
\end{lemma}


\begin{proof}

\end{proof}

\begin{remark}
    It has been discussed in earlier section that we have the equivalence of
    Barron norm and hence Barron spaces for $1 \leq p\leq +\infty$ (see
    \eqref{eq:barron_fouier_int}) with ReLU as activation function.

    In other words, combined with the fact that $\mathcal{B}_{\infty} =
        \mathcal{B}_p$, we can say that the class of functions where $f$ admits
        a Fourier representation with finite second moment ($v_{f, 2} < \infty$)
        are well ``contained'' inside the \textit{infinite-width Barron spaces}.
        However, $\mathcal{B}_{\mathcal{F}, 1}(U)$ still encapsulates a boarder
        class of functions.
\end{remark}


\begin{lemma}~\cite[Proposition 7.4]{carageaNeuralNetworkApproximation2022}

    Let $U \subset \mathbb{R}^d$ be bounded and have nonempty interior. For
    $s>0$, if $\mathcal{B}_{\mathcal{F},s}(U) \subset \mathcal{B}_{1}(U)$, then
    $s \geq 2$. In particular $\mathcal{B}_{\mathcal{F},1}(U) \not\subset
        \mathcal{B}_{1}(U)$
\end{lemma}

\begin{proof}

\end{proof}



% \section{Minimax Lower Bounds for Two Neural Network Model}

% For simplicity, data are of the form $\{(X_i, Y_i)\}_{i=1}^n$ from a

% For functions $f$ in the class $\mathcal{F}: [-1, 1]^d \to \mathcal{R}$, the
% minimax risk is:

% \begin{equation} R_{n,d} := \inf_{\hat{f}} \sup_{f\in \mathcal{F}}
%     \ERW{\norm{f - \hat{f}}^2} \end{equation}

% It has been shown in \TOCITE(Barron Minimax paper) that the minimax lower
% bound for neural nets is of order $\bigO(\log{d}/n)$ to some fractional power
% when $d$ is of larger order than $n$.

% \section{Risk Bounds for }

% {\itshape Firstly, I need to understand the difference between approximation
% rates and estimation of population risk.

% There exists a vast dictionary of definitions with similar wordings but the
% context is hughly different.

% e.g. What's the estimate of the population risk? In my understanding,

% $f^* - f_m$ where $f_m$ is the best solution in such hypothesis class is the
% approximation rates

% It should be that Population risk is w.r.t. a particular loss function?}

% \begin{equation} f(x) = \sum_{i=1}^m a_i \phi (\spr{\mathbf{w_i}}{x})
%     \end{equation}





%%% Local Variables: %% mode: latex %% TeX-master: "MasterThesisSfS" %% End: 
