\chapter{The approximation properties of two-layer neural networks}

\TODO

In this chapter, we will introduce some basic concepts about artificial
artificial neural networks with an emphasis on the two-layer neural networks
(\gls{2nn}). Firstly, we will define the 2NN model and the problem setup. In
section \ref{sec:uat}, we will state the result of the universal approximation
theorem. Later in section \ref{sec:spectral_norm} and \ref{sec:barron_norm}, we
formally introduce the function spaces associated with 2NN, along with the
definition of the \textit{spectral norm} by
\cite{barronUniversalApproximationBounds1993} and the \textit{Barron norm} by
\cite{eBarronSpaceFlowinduced2021}. Despite the term ``Barron space'' and
``Barron norm'' has been coined and mentioned earlier, the notion of different
Barron spaces and their relationship have not been made obvious. To avoid
confusion, we here borrow the terminology adopted by
\cite{carageaNeuralNetworkApproximation2022}: we refer the spaces associated
with the spectral norm as the \textit{Fourier-analytic Barron spaces} and the
spaces defined using the Barron norm as the \textit{infinite-width Barron
spaces} as it essentially consist of ``infinitely wide'' neural networks (with
some restrictions on the parameters). Finally, the relationship (mainly
inclusion) between the Fourier-analytic Barron spaces and the infinite-width
Barron spaces is examined in section \ref{sec:diff_barron_spaces}. The aim of
this chapter is to provide a concise summary of various well-known results
concerning the approximation properties of 2NN and we recommend work by
\cite{eMathematicalUnderstandingNeural2020,bernerModernMathematicsDeep2021} for
a comprehensive review.

% classical Barron space, or the Fourier-analytic Barron space

% infinitely wide

% infinite-width Barron spaces.

% sparsity-inducing norm ? in bach 2017 paper.


\section{Preliminary results}

\TONOTE{Is it Banach spaces or Hilbert space?}

% Jonas-Barron method

% Maurey's Theorem: with a closure of convex hull suggests bound

% Here I will start classical probabilistic argument of Maurey


Here we provide some frequently used constructions concerning the approximation
of functions in a Hilbert space from by the non-linear $n$-term dictionary. Let
$\mathbb{D} = \{d_1,d_2,\dots\}$ be a uniformly bounded domain in a Banach space
$\mcal{H}$ ($\sup_{d\in \mathbb{D}} \norm{d}_{\mcal{H}} < \infty$) and a
function $f \in \mcal{H}$. For every $n \in \Nat$, we write the collection of
all functions in $\mcal{H}$ which can be expressed as a linear combination of at
most $n$ elements of $\mathbb{D}$ (i.e. elements of \eqref{eq:gm}) as
$\Sigma_m(\mathbb{D})$

\begin{equation}
    \label{eq:dict_represent}
    \Sigma_n(\mathbb{D}) = \Bigg\{
        \sum_{j=1}^n \alpha_j d_j, \quad
        d_j \in \mathbb{D}, \alpha_j \in \R
    \Bigg\}.
\end{equation}

For any function $f_n \in \Sigma_n(\mathbb{D})$ with $n \in \Nat$
\begin{equation}
    \label{eq:gm}
    f_n = \sum_{j=1}^n \alpha_j d_j
\end{equation}

Approximation with $f_n$ is non-linear as it relies on the target function $f$.
These dictionaries \eqref{eq:dict_represent} can be interpreted as splines
approximation with free knots and we will study the approximation results when
$\mathbb{D}$ is comprised of sigmoidal functions, ReLU.

To include the control over the coefficients $\alpha_j$ in the above expansion,
we introduce
\begin{equation}
    \label{eq:dict_sigma}
    \Sigma^{t}_n(\mathbb{D}, M) := \Bigg\{
        f = \sum_{j=1}^n \alpha_j d_j: 
        d_j \in \mathbb{D} \textnormal{ and } 
        \sum_{j=1}^n \abs{\alpha_j}^t \leq M^t, \quad 
        n \in \Nat, \alpha_j \in \R
    \Bigg\}
\end{equation}
for any $t > 0 \in \Nat \bigcup\, \{\infty\}$. Let $K^t_n(\mathbb{D}, M)$ be the
closure of $\Sigma^t_n(\mathbb{D}, M)$ in $\mcal{H}$
\begin{equation}
    K^t_n(\mathbb{D}, M) := \closure{\Sigma^t_n(\mathbb{D}, M)}.
\end{equation}

When $t = 1$, $K^1(\mathbb{D})$ is the class of functions that are a
\textit{convex} combination of functions in $\mathbb{D}$. When $t = \infty$,
$\Sigma^{\infty}_n(\mathbb{D}, M)$ corresponds to the sets whose coefficients
bounded in $\lp{\infty}$. 

Next, we define the union for all $M > 0$
\begin{equation}
    K^t(\mathbb{D}) = \bigcup_{M > 0} K^t(\mathbb{D}, M).
\end{equation}

We define a seminorm for functions $f \in K^t(\mathbb{D})$
\begin{equation}
    \label{eq:general_seminorm}
    \abs{f}_{K^t(\mathbb{D})} = \inf_{M > 0} \{
        f \in K^t_n(\mathbb{D}, M)
    \}.
\end{equation}

For $f \in \mcal{H}$, the approximation error
\begin{equation}
    \label{eq:appro_err_hilbert}
    e_n(f, \mathbb{D}, \mcal{H})
        := \sup_{g\in\Sigma_{\mathbb{D}}} \norm{f - g}_{\mcal{H}}
        = \norm{f - g_n}_{\Sigma_{\mathbb{D}}}
\end{equation}

Assume that approximation holds with a dictionary $\mathbb{D}$ in $H$ with a 
approximation error 
\begin{equation}
    \label{eq:appro_error_general}
    e_n(f, \mathbb{D}, H) \leq n^{-\alpha} C_{\mathbb{D}}.
\end{equation}

We are concerned with the coefficients in the exponents $\alpha$ and what the
$\mathbb{D}$ looks like.

For the special cases where $\mathbb{D}$ is the orthonormal basis of
$H$~\cite{devore_1998}
\begin{equation}
    \label{eq:appro_error_ortho}
    e_n(f, \mathbb{D}, \mcal{H}) 
        \lesssim n^{-\alpha} \abs{f}_{K^t(\mathbb{D})}
\end{equation}

It is proven by~\cite{pisierRemarquesResultatNon1980} that $\norm{f - g_n} =
\bigO(n^{-1/2})$ for general dictionary in $\lp{2}$ space.
\cite{makovozRandomApproximantsNeural1996} provided a refinement and proved the
cases where $X = \lp{p}, p < \infty$ under some general conditions.
\cite{jonesSimpleLemmaGreedy1992} showed ????

\begin{definition}[Covering numbers]
    \label{def:covering_num}
    Let $(\mcal{F}, d)$ be a subset of a normed space with norm $d$. The
    \textit{covering numbers} $N(\epsilon, \mcal{F}, d)$ is the minimal number
    of balls $\{g: d(g, f) < \epsilon\}$ of radius $\epsilon$ required to cover
    the set $\mcal{F}$. The \textit{entropy} $\log N(\epsilon, \mcal{F}, d)$ is
    the logarithm of the covering number. We are usually talking if $\lp{p}(\R)$
    space for some probability measures $\R$
\end{definition}

Approximation with a finite linear combination of elements from a bounded
dictionary in a Hilbert space $\mcal{H}$ has a error rate of $\bigO(n^{-1/2})$
and this holds for general dictionary $\mathbb{D} \subset \mcal{H}$.

\begin{theorem}[Maurey's Theorem]
    \label{thm:maurey}
    Let $\mathbb{D} = \{d_1, d_2, \dots\}$ be a arbitrarily bounded sequence of
    elements in a Hilbert space $\mcal{H}$. For every $f \in \mcal{H}$ of the
    form
    \begin{equation}
        f = \sum_{j=1}^n c_j d_j, \quad
        \sum_{j=1}^n \abs{c_i} < \infty, \quad
        c_j \in \R, n < \infty
    \end{equation}
    then for every $m \in \Nat$, there exists a function $g_n = \sum_{j=1}^n a_j
    d_j$ of the form \eqref{eq:gm} with at most $n$ non-zero coefficients
    \begin{equation}
        \sum_{j=1}^n \abs{a_j} \leq
        \sum_{j=1}^n \abs{c_j}, 
        \quad a_j \in \R
    \end{equation}
    such that
    \begin{equation}
        \norm{f - g_n}_{\mcal{H}} \leq
        2 N(\epsilon, \mathbb{D}, \norm{\cdot}_{\mcal{H}})
        \cdot n^{-1/2}
        \cdot \sum_{j=1}^n \abs{c_j} 
    \end{equation}

\end{theorem}

We recommend readers \cite{devore_1998} for details on nonlinear approximation.
Detailed constructions about can be found in
\cite{vandervaartWeakConvergenceEmpirical1996}.
% Using a classical probabilistic argument of Maurey [45], an approximation rate
% of O(n 2 ) can be obtained for the class B1(D) using non-linear dictionary
% expansions. Moreover, Jones [27] gave a constructive proof of this fact using
% the relaxed greedy algorithm and applied this result to shallow neural networks
% with a cosine activation function. Improvements upon this rate of dictionary
% approximation under an assumption about the behavior of the relaxed greedy
% algorithm appear in [31, 33]. These results yield exponential rates of
% convergence for individual functions in the convex hull of D (but not
% necessarily its closure), which are however not uniform over the class B1(D).
% Further, under compactness [29, 41] or smoothness [50] assumptions on the
% dictionary D improved rates can also be obtained, although for general
% dictionaries the Maurey-Jones rate is the best one can expect [30].

\section{Universal approximation theorem}
\label{sec:uat}

In general, neural networks, even 2NN with one hidden layer, are universal
approximators, which means any continuous functions on a compact set can be
approximated up to a arbitrary precision under some mild restrictions on the
activation functions $\sigma$ and the number of nodes are allowed to grow
arbitrarily. The condition on $\sigma$ is formally stated in
~\cite{cybenkoApproximationSuperpositionsSigmoidal1989}. Despite its obvious
importance, one can not benefit much as it can not provide enough quantitative
information in realist applications. Nevertheless, we will state the main
theorem, the univariate approximation theorem in this section.

% We will state the the universal approximation theorem that the finite linear
% combinations of sigmoidal functions (sum of the functions of the form
% (\eqref{def:sigmoidal})) are dense in the space of continuous functions. This
% approximation result holds for any continuous sigmoidal functions.

% function on a compact set up to arbitrary precision [Cyb89, Fun89, HSW89,
% LLPS93]. modern mathematics of deep learning

\begin{definition}[sigmoidal function]\label{def:sigmoidal}
    A function $\sigma$ is \textbf{sigmoidal} if
    \begin{equation}
        \sigma(t) =
        \begin{cases}
            1 & \text{as} \quad t \to +\infty \\
            0 & \text{as} \quad t \to -\infty.
        \end{cases}
    \end{equation}
\end{definition}

\subsection*{Notations and setup}

Let $I_d = [0,1]^d$ denote the $d$-dimensional unit cube, and the space of
continuous functions over $I_d$ is denoted by $C(I_d)$. We denote the supremum
norm of a function $f \in C(I_d)$ by $\norm{f}_{\infty}$ 
\begin{equation}
    \label{def:sup_norm}
    \norm{f}_{\infty} = \sup_{x\in I_d} \abs{f(x)}.
\end{equation}
We use $M(I_d)$ to denote the space of finite, signed regular Borel measures,
i.e. $\mu(A) \in \R$ for all Borel sets $A \in I_d$ and $\mu(\emptyset)= 0$. We
refer readers to \cite{rudinFunctionalAnalysis1991,
rudinRealComplexAnalysis1987} for a detailed presentation of functional
construction used and we include some basic materials in Appendix
\ref{app:function_measure}.

Let $h(x)$ be the sums of the form:

\begin{equation}
    \label{eq:sum_sigma}
    h(x) = \sum_{j=1}^n a_j \sigma(b_j\tr x + c_j)),
\end{equation}

where $a_j, c_j \in \R$, $b_j \in \R^d$, $n < \infty$, and $\sigma$ is a
univariate function from $\R$ to $\R$.
 
The main contribution of approximation theorem is the statement on the
conditions of $\sigma$ such that the above finite linear combination $h(x)$ is
dense in $C(I_d)$ with respect to the supremum norm. It should also be noted
that there is no restriction for the number of combinations, i.e. the nodes of
the 2NN and hence the parameter space.

\begin{theorem}[Universal approximation theorem]
    \label{thm:uat}
    If $\sigma$ is sigmoidal as defined in Definition $\ref{def:sigmoidal}$,
    then any function $f \in C(I_d)$ be approximated uniformly well by a finite
    linear combination of the form \eqref{eq:sum_sigma}.
\end{theorem}


The main structure of the proof is followed:
\begin{enumerate}
    \item any finite sums of the form \eqref{eq:sum_sigma} with a
    \hyperref[def:dis_func]{discriminatory function} $\sigma$ are dense in
    $C(I_d)$ with respective to the supremum norm.
    \item show that any bounded sigmoidal functions are discriminatory.
\end{enumerate}

\begin{definition}[Discriminatory function]
    \label{def:dis_func}
    A function $\sigma$ is \textbf{discriminatory} if for a measure $\mu \in
    M(I_d)$
    \begin{equation}
        \int_{I_d} \sigma(b\tr x + c) \,d\mu(x) = 0
    \end{equation}
    for all $b \in \R^d$ and $c \in \R$ implies $\mu = 0$
\end{definition}

We can now state that the linear span or the finite linear combinations of
continuous discriminatory functions are dense in the space of $C(I_d)$ equipped
with $\norm{\cdot}_{\infty}$.

\begin{theorem}
    Let $\sigma: \R \to \R$ be a
    continuous \hyperref[def:dis_func]{discriminatory function}, the finite sums
    of the form \eqref{eq:sum_sigma} are dense in the space $(C(I_d),
    \norm{\cdot}_{\infty})$. In other words, for any $\epsilon > 0$ and any
    $f \in C(I_d)$, there exists a sum $h(x)$ of the above form, where
    \begin{equation}
        \norm{f(x) - h(x)}_{\infty} < \epsilon \quad \forall x \in I_d.
    \end{equation}
\end{theorem}

\begin{proof}
    Let $G(\sigma) := \spn(\{\sigma(b\tr x + c): b \in \R^d, c \in \R \})$ be
    the linear span for every $b\in\R^d, c\in\R$. $G(\sigma)$ clearly is a
    linear subspace of $C(I_d)$. We claim that the closure of $G(\sigma)$,
    $\closure{G(\sigma)}$, is all of $C(I_d)$.

    We continue the proof by contradiction. Assuming $\closure{G(\sigma)}$ is
    not $C(I_d)$, then there is a bounded linear functional $L$ on $C(I_d)$ such
    that $L\equiv 0$ on $C(I_d)$ and $L(G(\sigma)) = L(\closure{G(\sigma)}) = 0$
    by the Hahn-Banach Theorem \ref{thm:hahn_banach_2}.

    By the \hyperref[thm:riesz_rep]{Riesz Representation Theorem}, there is a
    unique $\mu \in M(I_d)$ for this $L$ such that
    \begin{equation}
        L(f) = \int_{I_d} f(x) \,d\mu(x) \quad \forall f \in C(I_d)
    \end{equation}

    Since $L$ is zero on $G(\sigma)$, we must have for all $b$ and $c$ that
    \begin{equation}
        \int_{I_d} \sigma(b\tr x + c) \,d\mu(x) = 0
    \end{equation}

    However, the condition that $\sigma$ is discriminatory implies $\mu = 0$ and
    hence subspace $G(\sigma)$ must be dense in $C(I_d)$.
\end{proof}

Now it remains to show that sigmoidal functions are discriminatory.

\begin{lemma}
    Any bounded, measurable sigmoidal function is discriminatory.
\end{lemma}

\begin{proof}

    \textbf{Step 0} \textit{(Assume discriminatory and construct pointwise
    convergence function)}: Let $\sigma: \R \to \R$ be a sigmoidal function.
    Assume that the $\sigma$ is discriminatory with a measure $\mu \in M(I_d)$
    as in Definition \ref{def:dis_func}, we need to show that the measure $\mu =
    0$.

    Fix a arbitrary $b_0 \in \R^d\setminus \{0\}$ and define
    $\sigma_{\lambda}(x) := \sigma(\lambda (b_0\tr x + c) + \varphi)$. Then, for
    any $c, \lambda, \varphi \in \R$ we have
    \begin{equation}
        \sigma_{\lambda}(x)
        = \begin{cases}
            \to 1, \quad &\for b_0\tr x + c > 0 \quad \text{as}\, \lambda \to +\infty \\
            \to 0, \quad &\for b_0\tr x + c < 0 \quad \text{as}\, \lambda \to -\infty \\
            \sigma(\varphi), \quad &\for b_0\tr x + c = 0, \quad\forall \lambda \in \R
        \end{cases}
    \end{equation}

    Therefore, the functions $\sigma_{\lambda}$ converges pointwise to a
    function $\gamma(x): I_d \to \R$
    \begin{equation}
        \gamma(x) = 
        \begin{cases}
            1,               \quad &\for b_0\tr x + c > 0 \\
            0,               \quad &\for b_0\tr x + c < 0 \\
            \sigma(\varphi), \quad &\for b_0\tr x + c = 0
        \end{cases}
    \end{equation}
    pointwise as $\lambda \to + \infty$.
    
    Let $\Pi_{b_0,c}$ denote the hyperplane, $H_{b_0, c}$ denote the
    half-space as below 
    \begin{align}
        \Pi_{b_0,c} &= \{x\in\R^d \mid b_0\tr x + c = 0\} \\
        H_{b_0, c}  &= \{x\in\R^d \mid b_0\tr x + c > 0\}.
    \end{align}
    for all $c \in \R$.
    
    By the Lebesgue Convergence Theorem, we have
    \begin{align*}
        \sigma(\varphi) \mu(\Pi_{b_0, c}) + \mu (H_{b_0, c})
        &= \int_{I_d} \gamma(x) \,d\mu(x) \\
        &= \int_{I_d} \lim_{\lambda\to\infty} \sigma_{\lambda}(x)\,d\mu(x) \\
        &= \lim_{\lambda\to\infty} \int_{I_d} \sigma_{\lambda}(x)\,d\mu(x) = 0
    \end{align*}
    for all $\lambda, \varphi \in \R$.

    Thanks to the function $\sigma$ being sigmoidal, $\lim_{\varphi\to +\infty}
    \sigma(\varphi)= 1$ and $\lim_{\varphi\to -\infty} \sigma(\varphi)=0$
    \begin{equation}
        \label{eq:disc_1}
        \mu(\Pi_{b_0, c}) = 0 \quad \text{and} \quad \mu (H_{b_0, c}) = 0 
        \quad \forall c\in\R
    \end{equation}

    This in turns implies $\mu(I_d) = 0$ as we can choose $c$ arbitrarily large.
    
    We would like to show that the measure of all half-planes being zero implies
    that the measure $\mu$ must be zero. If $\mu$ is a positive Borel measure,
    this would be trivial by \eqref{eq:disc_1} but $\mu$ here is a signed
    measure.
    
    \textbf{Step 1} \textit{(Construct a signed measure)}: Let $\phi$ be a
    finite signed, Borel measure on $\R$
    \begin{equation}
        \label{eq:disc_2}
        \phi(A) = \mu(\{x \in I_d: b_o\tr x \in A\}) \quad \forall A \subseteq \R.
    \end{equation}

    By construction, we have
    \begin{align}
        \forall a < b \in \R, \quad \phi((a,b)) 
        &= \phi((a,\infty)) - \phi([b, \infty)) \\
        &= \mu(\{x \in I_d: b_o\tr x > a\}) \\
        &\quad- \Big(
            \mu(\{x \in I_d: b_o\tr x > b\})
            + \mu(\{x \in I_d: b_o\tr x = b\})
        \Big) \\
        &= \mu(H_{b_0, -a}) - (\mu(H_{b_0, -b}) + \mu(\Pi_{b_0, -b})) \\
        &= 0 - 0 = 0
    \end{align}
    therefore $\phi(A) = 0$ for all Borel sets $A \subseteq \R$.

    % Show a bounded linear functional is zero on such measure
    \textbf{Step 2} \textit{(Define a linear functional $L$}: Let
    $\lp{\infty}(\R)$ denote the space of all measurable bounded functions $f:
    \R \to \R$. For a function $h \in \lp{\infty}(\R)$, we define a functional
    $L: \lp{\infty}(\R) \to \R$:
    \begin{equation}
        L(h) = \int_{I_d} h(b\tr x)\,d\mu(x).
    \end{equation}

    $L$ is linear because for all $\alpha, \beta \in \R, \text{ and } g, h \in
    \lp{\infty}(\R)$
    \begin{align*}
        L(\alpha g + \beta h)
        &= \int_{I_d} (\alpha g + \beta h) (b_0\tr x) \,d\mu(x) \\
        &= \int_{I_d} (\alpha g(b_0\tr x) + \beta h (b_0\tr x)) \, d\mu(x) \\
        &= \alpha \int_{I_d} g(b_0\tr x) \,d\mu(x) 
            + \beta \int_{I_d} h(b_0\tr x) \,d\mu(x) \\
        &= \alpha F(g) + \beta L(h)
    \end{align*}

    Now we look at the indicator function for all Borel sets of $\R$
    \begin{equation}
        \indicator{A}(x) =
        \begin{cases}
            1, \quad \text{ if } x \in A, \\
            0, \quad \text{ if } x \not\in A.
        \end{cases}
    \end{equation}

     Note that
    $\indicator{A} \in \lp{\infty}(\R)$, with \eqref{eq:disc_2} we have 

    \begin{equation}
        L(\indicator{A}) = \int_{I_d} \indicator{A}(b_0\tr x)\,d\mu(x)
        = \mu(\{ x\in I_d: b_0\tr x\in A \}) = \phi(A) = 0
    \end{equation}
    for all Borel sets $A \subseteq \R$.

    Since $L$ is a linear functional, then for finite sum of functions of the
    form (simple functions):
    \begin{equation}
        \label{eq:simple_function}
        s_n(x) = \sum_{j=1}^{n} a_j \indicator{A_j}(x)
    \end{equation}
    is zero for all $n\in\Nat$ where $a_j \in \R$ and ${A_j} \subseteq \R$ are
    measurable and pairwise disjoint sets of $\R$. Since the simple functions
    \eqref{eq:simple_function} are dense in $\lp{\infty}(\R)$, then for a
    function $h \in \lp{\infty}(\R)$, there exists a sequence of function $s_n$
    converges pointwise to $h$ for all $n \in \Nat$ and $x \in \R$. We have
    \begin{align}
        L(h) &= \int_{I_d} h(b_0\tr x) \,d\mu(x) 
             = \int_{I_d} \lim_{n\to\infty} s_n(b_0\tr x) \,d\mu(x) \\
             &= \lim_{n\to\infty} \int_{I_d} s_n(b_0\tr x) \,d\mu(x)
             = \lim_{n\to\infty} L(s_n)
             = 0
    \end{align}
    where the limit in the integral is moved outside by the Lebesgue convergence
    theorem.

    \textbf{Step 3} \textit{(Tidy up)}: Since the sine and cosine functions are
    in $\lp{\infty}(\R)$ and for a bounded measurable function $h'(x) =
    e^{ib_0\tr x} = \cos(b_0\tr x) + i \sin(b_0\tr x)$ we have
    \begin{equation}
        L(h') = \int_{I_d} h'(b_0\tr x) \,d\mu(x) 
              = \int_{I_d} \cos(b_0\tr x) + i \sin(b_0\tr x) \,d\mu(x)
              = 0
    \end{equation}
    because $L(\cos) = L(\sin) = 0$ for all $b_0$.

    \TODO

    \begin{lemma}
        if $\mu$ is a signed finite Borel measure on $\R^d$ such that the Fourier
        transform of $\mu$
        \begin{equation}
            \fourier{\mu}(u) = \int_{\R^d} e^{-iu\tr x} \,\mu(x) = 0,  
        \end{equation}
        for all $x\in\R^d$, then $\mu = 0$ for all measurable sets of $\R^d$.
    \end{lemma}

    As we have chosen $b_0$ arbitrarily from $\R^d \setminus \{0\}$, we can see
    the $\mu = 0$ as the Fourier transform of $\mu$ is zero. Combined with the
    fact that $\mu(I_d) = 0$, we have shown $\sigma$ is discriminatory.

\end{proof}

% Let $f_m(\mathbf{x}, \Theta)$ be the parameterized family of 2NN defined above
% of $m$ nodes that which map input vector $\mathbf{x}$ of dimension $d$.

% \begin{equation}
%     f_m(\mathbf{x}, \Theta) = \sum_{i=1}^m a_i \sigma(\mathbf{b_i}^T\mathbf{x} + c_i)
% \end{equation}

% where $\Theta = (a_1, \mathbf{b}_1, c_1, \dots, a_m, \mathbf{b}_m, c_m)$ denotes
% all the parameters (the total number of parameters is $(d+2)m + 1$). We will
% consider the hypothesis spaces where the activation functions $\sigma$ is ReLU
% \footnote{\TONOTE{The choice of activation function is important for
%         infinite-width Barron spaces}}.

%  homogeneity

\TONOTE{Add something here}

\subsection{Application to the classification problem}

In this section, we will discuss the implications of Theorem \ref{thm:uat} for
classification problems. It should be noted that the decision function defined
below is not continuous on $I_d$ and we would like to check whether such
classification problem can also be well understood as the regression problem.

\begin{definition}[Decision function]
    Let $\{P_1, \dots, P_k\}$ be a partition of $I_d$ where each partition $P_j$
    are pairwise disjoint nonempty Borel sets, i.e. $P_j \not= \emptyset$ and
    $\bigcup_{j=1}^k P_j = I_d$. $f$ is a decision function for $I_d$ of the
    form
    \begin{equation}
        f: I_d \to \{1, \dots, k\}
    \end{equation}
    where $f(x) = j$ for $x \in P_j$. 
\end{definition}

\begin{theorem}
    \label{thm:uat_clas}
    Let $\sigma$ be a continuous sigmoidal function and $f$ be the decision
    function for any finite, measurable partition of $I_d$. Let $\phi$ be a
    Borel measure on $I_d$ and $\phi(I_d) = 1$. Then for any $\epsilon>0$, there
    exists a finite sum of the form
    \begin{equation}
        G(x) = \sum_{j=1}^n a_j \sigma(b_j\tr x + c_j)
    \end{equation}
    where $a_j, c_j \in \R, b_j \in \R^d$ and a set $D\subset I_d$, such that
    the measure of the set $D$, $\phi(D) \geq 1 - \epsilon$ and
    \begin{equation}
        \sup_{x\in D}\abs{G(x) - f(x)} < \epsilon
    \end{equation}
\end{theorem}

The theorem \eqref{thm:uat_clas} is a anoloy of the UAT and the proof is
straightforward using Lusin's Theorem \eqref{thm:lusin}.

\begin{proof}
    By Lusin's Theorem \eqref{thm:lusin}, there exists a continuous function $g
    \in C(I_d)$ such that $\phi(\{x\in I_d \mid f(x) \not= g(x)\}) < \epsilon$.
    Now we have a continuous $g$ and we are able to a find a sum of the form
    above satisfying $\abs{G(x) - g(x)} < \epsilon$ by Theorem \eqref{thm:uat}
    for all $x\in I_d$. Let set $D = \{x\in I_d \mid f(x) = g(x)\}$ and we have
    $\phi(D) \geq 1 - \epsilon$. Then for $x\in D$, we have
    \begin{equation}
        \sup_{x\in D}\abs{G(x)-f(x)} = \sup_{x\in D}\abs{G(x) - g(x)} < \epsilon 
        \quad \forall x\in D.
    \end{equation}
\end{proof}

The above result shows that the total measure of the misclassified points can be made arbitrarily small.

\section{Different Barron spaces}

The notation of Barron spaces is not in consensus within the community and
different terms have been given to describe the same model classes or spaces.
For the function spaces in which functions have finite Fourier moments,
~\cite{xuFiniteNeuronMethod2020} call this model classes \textit{Barron spectral
spaces} while \cite{carageaNeuralNetworkApproximation2022} refers them as
\textit{Fourier-analytic Barron space}.  For function spaces in which the
functions admit a integral representation (to be defined later) with a ReLU
activation function, \cite{eBarronSpaceFlowinduced2021} refer them simply as
\textit{Barron spaces} of different orders $p \in \Nat^+ \bigcup\,\{\infty\}$.
The term \textit{Barron space} was coined by
\cite{ePrioriEstimatesPopulation2019} to honor Prof. Andrew Barron's
contribution in the understanding of neural nets. In some
literature~\cite{carageaNeuralNetworkApproximation2022}, these are given
\textit{infinite-width Barron spaces} associated with different activation
functions and in some rare cases the term \textit{classical Barron space} is
reserved for those associated with Heaviside function.

To avoid confusion and in the meantime emphasize Prof. Andrew Barron's
contribution, we will use two definitions:
\begin{itemize}
    \item Fourier-analytic Barron spaces
    \item infinite-width Barron spaces\footnote{
        We limit the model classes to those associated with ReLU only. In the 
        case of Heaviside function, we denote the space still by the term 
        \hyperref[def:heaviside_space]{\textit{classical Barron space}}.
    }
    \item infinite-width Barron spaces\footnote{
        We limit the model classes to those associated with ReLU only. In the 
        case of Heaviside function, we denote the space still by the term 
        \hyperref[def:heaviside_space]{\textit{classical Barron space}}.
    }
\end{itemize}

In general, the activation function associated with the infinite-width Barron
spaces is ReLU and we will explicitly state when other functions (e.g. squared
ReLU, Heaviside) are used.


\subsection{Fourier-analytic Barron spaces}

Firstly, we define the seminorm and norm~\footnote{
    Often papers omit the ``Barron'' as readers can deduce from the context. 
    In the following chapters, we would like to call them \textit{spectral 
    semi/norm} as another definition of norm in infinite-width Barron spaces is 
    named \textit{Barron norm}.
}by which the smoothness of a function
is controlled. We begin with the Barron spectral seminorm proposed
originally by \cite{barronUniversalApproximationBounds1993}.

% should be L1 norm of \omega

\begin{definition}[Spectral seminorm]
    \label{def:spectral_seminorm}
    \TONOTE{MUST $U$ be nonempy?}
    For any $s \in \Nat$. Let $\emptyset \not= U \subset \R^d$ be a bounded
    domain on $\R^d$. Suppose a function $f: U \to \R$ admits a Fourier
    representation
    \begin{equation}
        f(x) = \int_{\R^d} e^{i\omega\tr x} \fourier{f}(\omega) \,d\omega
    \end{equation}
    where $\fourier{f}: \R^d \to \mathbb{C}$ is the Fourier transform of $f$.

    Let the spectral condition defined as
    \begin{equation}
        v_{f,s} 
            = \int_{\R^d} \norm{\omega}_1^s \abs{\fourier{f}(\omega)}\,d\omega.
    \end{equation}
    The spectral seminorm is defined as
    \begin{equation}
        \specseminorm{f}{s} = \inf_{f_{e\mid U} = f} v_{f,s}
    \end{equation}
    where the infimum is taken over all extensions $f_e$ of $f$ in $\lp1(U)$.
\end{definition}

The notion of spectral norm was first introduced in
~\cite{siegelApproximationRatesNeural2021} since it is more convenient compared
to the seminorm give by \cite{barronUniversalApproximationBounds1993}.



\begin{definition}[Spectral norm]
    \label{def:spectral_norm}
    Under the setup of Definition \ref{def:spectral_seminorm}, we define a
    modified spectral condition
    \begin{equation}
        v'_{f,s} 
            = \int_{\R^d} (1 + \norm{\omega}_1)^s \abs{\fourier{f}(\omega)}
            \,d\omega.
    \end{equation}
    The spectral norm is defined as
    \begin{equation}
        \specnorm{f}{s} = \inf_{f_{e\mid U} = f} v'_{f,s}
    \end{equation}
\end{definition}



The index or coefficients $s \in \Nat$ is referred as the smoothness index in
\cite{siegelHighOrderApproximationRates2021} and it is easy to see that as $s$
increases, the functions with finite $\specnorm{f}{s}$ ($\specseminorm{f}{s}$,
resp.) are becoming ``smoother''. Therefore the \textit{size} a function spaces
of higher smoothness index $s$ is expected to ``shrink'' which might suggests
better approximation rate with 2NN. In the coming sections, we will provide a
accurate statement of the improved approximation error rates with a depiction of
the intricacies inherent within and between these spaces.

\begin{definition}[Fourier-analytic Barron spaces]
    \label{def:fourier_space}
    For any $s \in \Nat$. Let $\emptyset \not= U \subset \R^d$ be a bounded
    domain on $\R^d$. The Fourier-analytic Barron spaces are
    \begin{equation}
        \bspace{\mcal{F},s}(U) := \Big\{
            f: U \to \R: v'_{f,s} < \infty  \textnormal{ and }
            \forall x\in U, 
                f(x) = \int_{\R^d} e^{i\omega\tr x} \fourier{f}(\omega)\,d\omega
        \Big\}
    \end{equation}
    equipped with a norm $\specnorm{f}{s}$, for all $s \in \Nat$.
\end{definition}

\subsection{infinite-width Barron spaces}
\label{sec:barron_norm}

This section introduces the infinite-width Barron space and its elementary
properties. 

\TONOTE{Is $X$ nonempty and bounded?}

For a nonempty and bounded domain in $U \subset \R^d$ and a function $f: U
\to \mathbb{R}$, we consider those that admit the following integral
representation:

\begin{equation}
    \label{eq:barron_represent}
    f(x) = \int_{\Omega} a \sigma(b\tr x + c) \mu(da, db, dc), \quad x \in U
\end{equation}

We define a condition for a measure $\mu$
\begin{align}
    r(f, \mu, p)
    &= \Big(\int_{\R^d} \abs{a}^p  (\norm{b}_1 + \abs{c})^p \,d\mu(a,b,c)\Big)^{1/p} \\
    &= \Big(\ERWi{\mu}{\abs{a}^p  (\norm{b}_1 + \abs{c})^p}\Big)^{1/p},
    \quad 1 \leq p \leq +\infty.
\end{align}

where $\mu$ is a probability distribution on $(\Omega, \Sigma_\Omega)$, $\Omega
= \mathbb{R}^1 \times \mathbb{R}^d \times \mathbb{R}^1$ and $\Sigma_\Omega$ is a
Borel $\sigma$-algebra on $\Omega$ and $\sigma(\cdot)$ is the ReLU activation
function.

We consider another case where the ReLU function is replaced with the Heaviside
function~\footnote{
    also called step function
}

\begin{definition}[Heaviside function]
    \label{eq:heaviside_represent}
    \begin{equation}
        H(x) = 
        \begin{cases}
            1 \quad x > 0,\\
            0 \quad x \leq 0    
        \end{cases}
    \end{equation}
\end{definition}


Similarly, for a $\mu \in \Sigma_{\Omega}$, if a function $f$ admits the
representation as below
\begin{equation}
    \label{eq:heaviside_represent}
    f(x) = \int_{\Omega} a H(b\tr x + c) \mu(da, db, dc), \quad x \in U.
\end{equation}

Accordingly, we define a condition for that particular $\mu$ where
\eqref{eq:heaviside_represent} holds
\begin{align}
    r(f,\mu, H)
    = \int_{\Omega} \abs{a}\,d\mu(a,b,c) = \ERWi{\mu}{\abs{a}}
\end{align}

These representations can be seen as a continuum analogy of the 2NN with $m$
hidden nodes:

\begin{equation}
    f_n(x, \Theta) := \frac{1}{n}
    \sum_{j=1}^n a_j 
        \sigma(b\tr x + c_j), 
    \quad \Theta = \{(a_j, b_j, c_j), \ j = 1, \dots, n\}
\end{equation}

\begin{definition}[Barron norm] For functions that admit representation
    \eqref{eq:barron_represent}, its Barron norm is defined as
    \cite{eBarronSpaceFlowinduced2021}:
    \begin{equation}\label{eq:barron_norm}
        \barronnorm{f}{p} := \inf_{\rho} \Big(\ERWi{\mu}{\abs{a}^p 
        (\norm{b}_1 + \abs{c})^p}\Big)^{1/p},
        \quad 1 \leq p \leq +\infty
    \end{equation}
    % where \begin{equation*} \Theta_f = \left\{ (a, \pi) \mid f(x) =
    % \int_{space} a(w) \sigma(\langle w, x \rangle)\dd{\pi(w)} \right\}.
    % \end{equation*}
\end{definition}

The infimum is taken over all probability distribution where
\eqref{eq:barron_represent} holds for all $x \in U$. When $p = + \infty$, the
Barron norm reads

\begin{equation}
    \label{eq:barron_infinite_norm}
    \inf_{\rho} \max_{a, b, c \in \supp(\rho)} \abs{a} (\norm{b}_1 + \abs{c}).
\end{equation}

Similarly, we can defined a norm associated with Heaviside function where the
infimum is taken for all measure $\mu$ where \eqref{eq:heaviside_represent}
holds
\begin{equation}
    \label{def:heaviside_norm}
    \norm{f}_{\mcal{B}_H} = \inf_{\rho} \ERWi{\mu}{\abs{a}},
    \quad 1 \leq p \leq +\infty.
\end{equation}

\begin{definition}[Infinite-width Barron space]
    \label{def:barron_space}
    Let $U$ be a nonempty unbounded domain in $\R^d$. For functions that admit
    representation \eqref{eq:barron_represent}, the infinite-width Barron space
    with a order of $1 \leq p \leq \infty$
    \begin{equation}
        \mcal{B}_p(U) = \Bigg\{
            f: U \to \R \mid \exists\, \mu \in \Sigma_{\Omega}: 
            r(f, \mu, p) < \infty \textnormal{ and }
            \forall x \in U, f(x) = \int_{\Omega} a \sigma(b\tr x + c) \mu(da, db, dc)
        \Bigg\}
    \end{equation}
\end{definition}

Similarly for Heaviside function, a normed space can be defined

\begin{definition}[Classical Barron space]
    \label{def:heaviside_space}
    Let $U$ be a nonempty unbounded domain in $\R^d$. For functions that admit
    representation \eqref{eq:barron_represent}, the infinite-width Barron space
    with a order of $1 \leq p \leq \infty$
    \begin{equation}
        \label{def:heaviside_space}
        \mcal{B}_H(U) = \Bigg\{
            f: U \to \R \mid \exists\, \mu \in \Sigma_{\Omega}:
            r(f, \mu, H) < \infty \textnormal{ and }
            \forall x \in U, f(x) = \int_{\Omega} a H(b\tr x + c) \mu(da, db, dc)
        \Bigg\}
    \end{equation}
\end{definition}

Barron spaces are denoted by $\mathcal{B}_p$~\footnote{
    Going forward, we will simplify $\mcal{B}_{\mcal{F}, s}(U)$, $\mcal{B}_p(U)$ 
    and $\mcal{B}_H(U)$ as $\mcal{B}_{\mcal{F}, S}$, $\mcal{B}_p$ and 
    $\mcal{B}_H$ to avoid cluttering the notations when $U$ is a bounded domain
    in $\R^d$.
}, consist of all the functions whose $r(f, \mu, p)$ is finite for a measure 
$\mu \in \Sigma_{\Omega}$.

By the definition of Barron norm, it is easy to see that

\begin{equation}
    \mathcal{B}_{\infty} \subset \cdots \subset \mathcal{B}_{2} \subset \mathcal{B}_1
\end{equation}

% https://ocw.mit.edu/courses/18-125-measure-and-integration-fall-2003/6f21af6c40de1eccd70349bd3a3b0095_18125_lec17.pdf

\begin{proof}

The idea is similar to the inclusion of $L_p$, $L_q$ space.

Applying Hölder's inequality, for any $1 \leq p \leq q < \infty$

\begin{align*}
    \int \abs{a}^p (\norm{b}_1 + \abs{c})^p d\rho
     & = \int \abs{a}^p (\norm{b}_1 + \abs{c})^p \cdot 1 d\rho                                                    \\
     & \leq \Big(\int \abs{a}^{pq/p} (\norm{b}_1 + \abs{c})^{pq/p} d\rho \Big)^{p/q} \Big(\int d\rho\Big)^{1-p/q} \\
     & = \Big(\int \abs{a}^q (\norm{b}_1 + \abs{c})^q d\rho \Big)^{p/q} \Big(\int d\rho\Big)^{1-p/q}
\end{align*}

Therefore we have the inclusion $\mathcal{B}_{q} \subset \mathcal{B}_p$ for $1
    \leq p \leq q < \infty$. \TONOTE{Not sure how to justify for the case $+\infty$}
\end{proof}

As the reverse also holds in the class of ReLU functions,  we have
$\mathcal{B}_{\infty} = \mathcal{B}_p$, $\barronnorm{\cdot}{\infty} =
    \barronnorm{\cdot}{p}$  for all $1 \leq p \leq +\infty$.

% ~\cite[Proposition 1]{eBarronSpaceFlowinduced2021}
\begin{lemma}
    \label{lamma:equivalence_barron_space}

    For any $f \in \mathcal{B}_1$, $f
        \,\text{also}\, \in \mathcal{B}_{\infty}$ and $\barronnorm{f}{\infty} =
        \barronnorm{f}{p}$ and hence $ \mathcal{B}_{\infty} = \cdots =
        \mathcal{B}_{2} = \mathcal{B}_1$ when $\sigma(\cdot)$ is ReLU function.
\end{lemma}

\begin{proof}
    As $f \in \mathcal{B}_1$,  there exist a probability distribution $\rho$ on
    $(\Omega, \Sigma_\Omega)$ satisfying the integral form in
    \eqref{eq:barron_represent}.

    For any $\epsilon > 0$, from the definition of Barron norm
    \eqref{eq:barron_norm}:

    \begin{equation}
        \ERWi{\rho}{\abs{a}(\norm{b}_1 + \abs{c})} \leq \barronnorm{f}{1} + \epsilon
    \end{equation}

    \TONOTE{Why would this inequality hold? Why do you have to restrict A}

    The key point here is to construct a probability measure and then

    Here we construct two measure $\rho_+$ and $\rho_-$ on satisfying
    \begin{equation*}
        \ERWi{\rho}{\abs{a}(\norm{\mathbf{b}}_1 + \abs{c})}
    \end{equation*}


    where $\Lambda = \{(\mathbf{b}, c): \norm{\mathbf{b}}_1 + \abs{c} = 1\}$
\end{proof}


\section{Approximation in Fourier-analytic Barron spaces}
\label{sec:spectral_norm}

It is shown by \cite{barronUniversalApproximationBounds1993} that functions with
finite Fourier moment can be approximated with the superpositions of sigmoidal
functions at a rate independent of the dimensionality $d$ with some restriction
on the smoothness of the functions, In other words, any functions in that class
can be approximated with a 2NN at an error rate of $\bigO(m^{-1} \cdot C)$ where
$m$ is the number of nodes in the the single hidden layer and $C$ is a constant
dependent \textit{only} the smoothness of the target function. Even the
convergence rate itself is independent of dimension (i.e. the dimensionality of
input vector $X$), the constant $C$ could be dimension-dependent as the Fourier
transform is imposed here and the terms of derivatives might grow exponentially
required to make \TONOTE{?}. We will begin with functions in the model class
introduced by Prof. Andrew \cite{barronUniversalApproximationBounds1993}.

To begin with, we formulated the class of functions introduced in
\cite{barronNeuralNetApproximation1992, barronUniversalApproximationBounds1993}
below.

\begin{definition}
    \label{def:fourier_class}
    Let $U$ be a bounded set in $\mathbb{R}^d$, a function $f: U \to \mathbb{R}$
    is said to be in \textit{Barron class} with constant $C > 0$, if there is a
    $x_0$ in $U$ and $c \in [-C, C]$ and a measurable function $f: \mathbb{R}^d
    \to \mathbb{C}$ satisfying:

    \begin{align}
        & \int_{\mathbb{R}^d} \abs{\omega}_{U, x_0} 
        \cdot \abs{\fourier{f}(\omega)} \,d\omega < C \\
        & f(x) = c + \int_{\mathbb{R}^d} (
            e^{i\omega\tr x} - e^{i\omega\tr x_0}
        ) \cdot \fourier{f}(\omega)\,d\omega
    \end{align}

    where $\abs{\omega}_{X, x_0} := \sup_{x\in U}\abs{\spr{\omega}{x - x_0}}$
    and we denote by $\abs{\omega}_U$ when $x_0 = 0$ for simplicity. We refer
    the class of all functions as $\Gamma_C(U, x_0)$.
\end{definition}

\begin{theorem}\cite[Theorem~1]{barronUniversalApproximationBounds1993}\label{thm:barron_1993_1}
    For every function in $\Gamma_C(U, x_0)$, every sigmoidal function $\phi$,
    every probability measure $\mu$, and every $n \geq 1$, there exists a linear
    combination of sigmoidal function $f_n(x)$ of the form, such that
    \begin{equation}
        \int_U(f(x) - f_n(x))^2 \mu(dx) \leq \frac{(2C)^2}{n}
    \end{equation}
\end{theorem}


\begin{proof}

    The main ideas behind the the proof of Theorem \ref{thm:barron_1993_1} is to
    show functions with finite Fourier moment are in the closure of the convex
    hull of the set of half planes and law of large numbers.

    \textbf{Step 0} (\textit{Fixing $x_0$}): Changing $x_0$ to $x_1$ affects the
    norm, at most, by a factor of $2$. Let $x_0, x_1 \in U$, and $f$ fulfilling
    the assumption above, that is $f \in \Gamma_C(U, x_0)$. For any $\omega \in
        \mathbb{R}^d$, given $x_0, x_1$, we have

    \begin{equation}
        \abs{\omega}_{U, x_0} 
            = \sup_{x\in U}\abs{\spr{\omega}{x-x_1}} 
            \leq \sup_{x\in U}\abs{\spr{\omega}{x-x_0}} + \abs{\spr{\omega}{x_0-x_1}} 
            \leq 2\abs{\omega}_{U, x_1}
    \end{equation}

    Thus we have $\int_{\mathbb{R}^d} \abs{\omega}_{U, x_0} \cdot a < 2C$. If we
    have $\tilde{c} = c + \int_{\mathbb{R}^d} (e^{\omega\tr x_0} - e^{\omega\tr
    x_1}) d\omega$, then $f(x) = \tilde{c} + \int_{\mathbb{R}^d} (e^{i\omega\tr
    x} - e^{i\omega\tr x_1})$ with $\tilde{c} \leq 2C$.

    This shows that changing $x_0$ would only affect the norm in the RHS of
    Theorem \ref{thm:barron_1993_1} at most by a factor of two, i.e.
    $\Gamma_C(U, x_0) \subset \Gamma_{2C}(U, x_1)$. Therefore, we continue the
    proof assuming $x_0 = 0$ for simplicity.

    \textbf{Step 1} (\textit{Represent $f$ via Inverse Fourier Transform}): From
    the assumption, and the fact that $f$ is real-valued ($U \subset
    \mathbb{R}^d \to \mathbb{R}$), the real number part can be written as:

    Note that with polar decomposition $\fourier{f}(\omega) =
    e^{i\theta(\omega)} \cdot \abs{\fourier{f}(\omega)}$ where $\theta(\omega)
    \in \mathbb{R}$ denote the magnitude decomposition.

    \begin{align}
        f(x) - f(0)
         & = \Re \int (e^{i\omega\tr x} - e^{i\omega\tr 0}) e^{i\theta(\omega)} \cdot 
         \fourier{f}(\omega)\,d\omega \\
         & = \int_{\Omega}\Big(\cos(\omega\tr x + \theta(\omega)) - \cos(\theta(\omega))\Big)
         \fourier{f}(\omega)\,d\omega \\
         & = \int_{\Omega} \frac{C_{f,U}}{\abs{\omega}_{U, 0}}\Big(\cos(\omega\tr x + \theta
         (\omega)) - \cos(\theta(\omega))\Big)\,d\mu_g \\
         & = \int_{\Omega} g(x, \omega)\,d\mu_g \label{eq:barron_fouier_int}
    \end{align}

    $C_{f,U}$ is a constant defined with $f$ and $U$: $C_{f,U} =
        \int_{\mathbb{R}^d} \abs{\omega}_{U, 0} \cdot \abs{\fourier{f}(\omega)} d\omega \leq
        C$ and $\mu_g$ is a probability distribution $d\mu_g =
        \abs{\omega}_{U,0}/C_{f,U}\abs{\fourier{f}(\omega)} d\omega$, the integral is
    evaluated on $\Omega = \{\omega \in \R^d: \omega \not = 0\}$ and

    \begin{equation}
        g(x, \omega) = \frac{C_{f,U}}{\abs{\omega}_{U, 0}}\Big(\cos(\omega\tr x + \theta(\omega)) - \cos(\theta(\omega))\Big)
    \end{equation}

    \textbf{Step 2} (\textit{$f(x) - f(0)$ is in the closure of the convex hull
        of $G_{cos}$}) The integral form in (\ref{eq:barron_fouier_int}) shows that
    $f(x) - f(0)$ can be represented as an infinite convex combination of
    functions in the class

    \begin{equation}
        G_{cos} = \Bigg\{\frac{\abs{\gamma}}{\abs{\omega}_{U, 0}}\Big(\cos(\omega\tr x + b) - \cos(b)\Big): \omega \not= 0, \abs{\gamma} \leq C, b \in \mathbb{R} \Bigg\}
    \end{equation}

    Suppose we have drawn $n$ samples ($\{\omega_i, i = 1,\dots, n\}$) from
    $\mu_g$, the expected norm in $\lp{2}(\mu_g, U)$ converges to zero as $n \to
    \infty$ by $\lp{2}$ law of large numbers. Therefore, there exist a sequence
    of convex combination in $G_{cos}$ that converges to $f(x) - f(0)$ in
    $\lp{2}$.


    \textbf{Step 3} (\textit{$G_{cos}$ is in the closure of the convex hull of
        $G_{step}$}): It is sufficient to check $g(z), z = \alpha x, \alpha =
        \omega/\abs{\omega}_{U,0}$ on $[-1, 1]$ for some $\omega$. As $g(z)$ is a
    uniformly continuous sinusoidal function on $[-1, 1]$, it can be uniformly
    approximately by piecewise constant step function.

    Restricting $g(z)$ on $[0, 1]$, for a partition ${0 \leq p_1 \leq p_2 \leq
                \cdots \leq p_k = 1}$, define

    \begin{align}
        g_{k,+}(z) = \sum_{i=1}^{k-1} \Big(g(p_i) - g(p_{i-1})\Big) \cdot
        \indicator{\{z\geq p_i\}}(z)
    \end{align}

    Similarly, we can construct $g_{k,-}(z) = \sum_{i=1}^{k-1} (g(-p_i) -
    g(-p_{i-1})) \cdot \indicator{\{z\leq -p_i\}}(z)$, results in a sequence of
    piecewise step function on $[-1, 1]$ uniformly close to $g(z)$. We have
    $g(z) = g_{k,+}+g_{k,-}$, a linear combination of step function (or
    heaviside function) and the sum of the coefficients is bounded by 2C (The
    sum of coefficients of $g_{k,+}$ is bounded by $C$ as a result of the
    derivative of $g$ bounded by $C$, so does $g_{k,-}$ and hence $2C$).

    We can see that functions $g(z)$ are in the closure of the convex hull of
    the step functions (by Lemma 1 in
    \cite{barronUniversalApproximationBounds1993})

    By substituting $z = \omega/\abs{\omega}_{U, 0} x$, we have $G_{cos} \subset
        G_{step}$,
    \begin{equation}
        G_{step} = \Bigg\{
            \gamma\indicator{\{\alpha x-t\}}(x):
            \abs{\gamma} \leq 2C,
            \abs{t} \leq 1,
            \abs{\alpha}_{U} = 1
        \Bigg\}
    \end{equation}

    \textbf{Step 4} (\textit{Closure of $G_{\phi}$}) There exists a sequence of
    sigmoidal function $\phi(\abs{c}(\alpha x - t))$, as $\abs{c} \to \infty$,
    it converges to step functions pointwise (except at points where $\alpha x -
        t = 0$). If we introduce a measure $\mu$ that has zero measure at those
    points, previous statement on $G_{cos} \subset G^{\mu}_{step}$ still holds
    on $\{\abs{t} \leq 1: \alpha x - t \not=0\}$ give a particular $\alpha$. We
    subsequently have convergence in $L_2(\mu, U)$ by dominated convergence
    theorem, which implies that functions in $G^{\mu}_{step} \subset G_{\phi}$.

    Finally we have the following relationship since the closure of a convex set
    is also convex.

    \begin{equation*}
        \Gamma_{U, x_0} \subset \closure{G_{cos}} \subset \closure{G_{step}} \subset \closure{G_{\phi}}
    \end{equation*}

    \cite[\textit{Lemma~1}]{barronUniversalApproximationBounds1993}: If $f$ is
    in the closure of the convex hull of a set $G$ in a Hibert space with norm
    $\norm{\cdot}$ and every function $g \in G$ is bounded by some constant
    $C_G$. Then for every $N \geq 1$, and every constant $C' > C_g^2 -
        \norm{f}^2$, there is a sequence $\{f_i, i = 1, \dots, N\}$ in the convex
    hull in $G$ such that:
    \begin{equation}
        \norm{f - f_N}^2 \leq \frac{C'}{n}
    \end{equation}

    $f_N = \sum_{i=1}^N \lambda_i \cdot f_i, \quad \sum_{i=1}^N \lambda_i = 1$

    % Lemma 1 in \cite{barronUniversalApproximationBounds1993} showed that
    % function in a closure of the convex hull of a set in a Hilbert space can
    % be approximated with a sequence of functions from such closure and the
    % norm between the function and the sequences $\{f_i, i = 1, \dots, N\}$ are
    % bounded in a magnitude of $\bigO(N^{-1})$.

    As shown above that function $f(x) - f(0)$ are in the closure of the convex
    hull of $G_{\phi}$ where $\norm{g} \leq (2C)^2$ for every $g \in G_{\phi}$.
    For any choice of $C' > (2C)^2 - \norm{f(x) - f(0)}^2$, we have the $L_2$
    norm of the approximation error is bounded.
    % Suppose we restrict $t$ t

    % Suppose we now restrict $t$ to the continuity point induced by measure
    % $\mu$ in We can check that the functions in $G_{step}$ are in the closure
    % of the convex hull of 

    % \textbf{Step 3} (\textit{Putting it together}): We can further show
    % $G_{cos}$ are in the class of sigmoidal functions. 

    % Theorem 2 in \cite{barronUniversalApproximationBounds1993}, we have that

\end{proof}



\subsection{Improved rate with Heaviside activation function}



Here, \cite{makovozRandomApproximantsNeural1996} showed that the error can be
improved to $\bigO(m^{\frac{1}{2} - \frac{1}{2d}})$

\begin{theorem}\cite[Theorem 3]{makovozRandomApproximantsNeural1996}
    \label{thm:improve_barron}

    \TODO

    Let $U$ be bounded set on $\R^d$. Let $f: U \to \R$ be a function and $V$ be
    the set of functions whose spectral Barron seminorm is finite
    \begin{equation}
        V =\{f: U\to\R : \specseminorm{f}{1} < \infty \text{ and } 
        \forall x\in U,
        f(x) = \int_{\R^d} e^{i\omega\tr x}\fourier{f}(\omega)\,d\omega\}.
    \end{equation}
    Then there exists a finite linear combination of the form for any $f \in V$
    \begin{equation}
        f_n(x) = \sum_{j=1}^n a_j H{b_j\tr x + c_j} \quad 
            a_j, c_j \in \R, b_j \in \R^d, 0 < n < \infty 
    \end{equation}
    such that 
    \begin{equation}
        \norm{f(x) - f_n(x)}_{q} \leq C n^{-\frac{1}{2} - \frac{1}{q \cdot d}}
        \quad 1 \leq q < \infty
    \end{equation}
    where $C$ is a constant dependent only $q$ and the Heaviside function
    $H(\cdot)$.
\end{theorem}

% In some literature, V is represented as the dictionary
\TONOTE{use which notation for dictionary}

Note that this theorem does not cover the case $q = \infty$. However,
\cite{barronUniversalApproximationBounds1993} has already showed that $f\in V$ are
dense w.r.t. supremum norm ($\norm{f - f_n}_{\infty} = \bigO(n^{-1/2})$), which
implies $\norm{f-f_n}_{q}$ for all $\lp{q}, q<\infty$.

As the dimensionality $d$ increases, this rate approaches to the original rate
and therefore this theorem is significant for low dimensionality. The above
conclusion can be extended to more general bounded sigmoidal functions. It is
easy to check $H(\lambda t) \to \sigma(t)$ as $\lambda \to \infty$ and
$H(\lambda t) \to \sigma{t}$ as $\lambda \to -\infty$. On a closed interval
$[-u, u]$ on $\R$, note that the difference $\sigma(t) - H(\lambda t)$ is still
bounded everywhere. Therefore, we can see that distance between a bounded
sigmoidal function and Heaviside function can be made arbitrarily small on the
space $\lp{p}(\R, \mu)$.

\begin{proof}
    We denote the set of Heaviside functions with bounded weights
    \begin{equation}
        A = \{\indicator{b, c}: \indicator{b, c} = 
        \indicator{b\tr x + c}, b\in\R^d, c\in\R\}
    \end{equation}

    As we already knew from \cite{barronUniversalApproximationBounds1993} that
    $V$ is the closure of the convex, symmetric hull of $A$ in $\lp{p}(D)$
    \begin{equation}
        V = \closure{\conv(A \bigcup - A)}.
    \end{equation}

    \begin{equation}
        D^d_{\indicator{}} = \{\sum_{j=1}^m a_j \indicator{b\tr x + c}, 
        \quad \sum_j \abs{a_i} \leq 1, \abs{b_j} = 1,
        b\in\R^d,
        a, c\in\R\}
    \end{equation}
    and we denote by $D$ to avoid clutters. We then need to get an estimate of
    $N(\epsilon, D, d)$ where $d$ is the $\lp{p}$ norm for $2 \leq p < \infty$.

    We consider Heaviside with $\abs{b} = 1$. $D$ is inside a ball with radius
    $r$, then this implies $\abs{c} \leq r$ as $\indicator{b\tr x + c}$ would
    be ones or zeros over all $D$. Suppose $b_0, b_1, c_0, c_1$ and $\abs{b_0 -
    b_1} \leq \epsilon$ and $\abs{c_0 - c_1} \leq \epsilon$ for some $\epsilon <
    0$. It is easy to check that 
    \begin{equation}
        \norm{\indicator{b_0, c_0} - \indicator{b_1, c_1}}_{D}
            \leq C \sqrt{\epsilon}
    \end{equation}
    in $\lp{2}(D)$ with a positive constant $C>0$.
    
    We obtain a $\bigO(\sqrt{\epsilon})$-net for $A$ in $\lp{2}(D)$ if we are able
    to find a $\epsilon$-net for the set $P:= \{(b,c)\in \R^{d+1}: \abs{b} = 1,
    \abs{c} \leq r\}$. To build a $\epsilon$-net for the sphere $\abs{b}=1$,
    $\bigO(\frac{1}{\epsilon}^{d-1})$ elements is needed. An interval $[-r,r]$
    requires $\bigO(\epsilon^{-d})$ elements. Therefore, a $\epsilon$-net of
    $\bigO(\epsilon^{-2c})$ elements can be constructed for $A$ and hence the
    covering number for $A$ is of the order $\bigO(\epsilon^{-\frac{1}{2d}})$

    The statement then follows the corollary from~\cite[p.
    104]{makovozRandomApproximantsNeural1996}.
\end{proof}


% \section{Approximation in $L_2(\Omega)$}

% \section{Approximation in $L_2([0,1])$}

% \section{Add a section for F transform}

\section{Improved approximation in Fourier-analytic Barron spaces}

In this section, we consider approximation by 2NN with ReLU$^k$ as activation
function
\begin{equation}
    \sigma_k(x) = [\max(0, x)]^k, \quad k \in \Nat^0
\end{equation}
when $k = 0$, $\sigma_0$ is the Heaviside function in Definition
\ref{eq:heaviside_represent}.

% A smoothness property of the function to be approximated is expressed in terms
% of its Fourier representation. In particular, an average of the norm of the
% frequency vector weighted by the Fourier magnitude distribution is used to
% measure the extent to which the function oscillates. In this Introduction, the
% result is presented in the case that the Fourier distribution has a density that
% is integrable as well as having a finite first moment. Somewhat greater
% generality is permitted in the theorem stated and proven in Sections III and IV.

% In addition to the smoothness property ensured via the finite first moment,

In the previous section, the smoothness of a function is expressed through its
\textit{first} Fourier representation. In particular, how ``oscillating'' or
``fluctuating'' of is a function $f$ is measured by the an mean of the norm of
the frequency vector weighted by the Fourier magnitude distribution. Naturally,
we would like to extend the findings with tighter restriction on the smoothness
and ideally improve on the $\bigO(n^{-1/2})$. Tighter rates of approximation is
made possible with stricter conditions on the Barron spectral norm while
bounding the parameter $b\in\R^d$ as in \TOCITE w.r.t. $\lp{1}$ norm or $\lp{2}$
norm. 

We consider the dictionary when researching for shallow neural networks
similar to \eqref{eq:dict_represent}
\begin{equation}
    \mathbb{D}_{k} = \Bigg\{
        \sigma_k(b\tr x + c), \quad b\in\R^d, c\in\R
    \Bigg\}
\end{equation}
where $\sigma_k, k \in \Nat^0$ is the ReLU$^k$ function described above.

Specifically, we will consider the sets similar to \eqref{eq:dict_sigma}
\begin{equation}
    \Sigma_n(\mathbb{D}_k)= \Bigg\{
        \sum_{j=1}^n a_j \sigma_k (b_j\tr x + c_j): 
        b_j \in \R^d, a_i \in \R
    \Bigg\} 
\end{equation}

We define the sets of functions whose coefficients $a_j$ are bounded in $\lp{1}$
and $\lp{\infty}$ for all $0 < n < \infty$.

\begin{align*}
    \Sigma_n^1(\mathbb{D}_k, M) &:= \Big\{
        \sum_{j=1}^n a_j d_j:
        d_j \in D^d_{\sigma_k}, \sum_{j=1}^n \norm{a_j}_1 \leq M 
    \Big\} \\
    \Sigma_n^{\infty}(\mathbb{D}_k, M) &:= \Big\{
        \sum_{j=1}^n a_j d_j:
        d_j \in D^d_{\sigma_k}, \max_{j} \abs{a_j} \leq M 
    \Big\}
\end{align*}

We will consider the cases where the $\lp{1}$-norm of $a_i$ is bounded or
unbounded.

\subsection{Approximation in $L_{\infty}$ with bounded weights}

We start with the main result when the approximation with 2NN with $m$ nodes and 
the parameters bounded in $L_1$.

\begin{theorem}
    \label{thm:appro_bound_l1}
    Let $U = [-1,1]^d$. Suppose $f: U \to \R$ admits a Fourier representation
    and the spectral norm of order $2$ of $f$ is finite, i.e.
    \begin{equation}
        v_{f,2} = \int_{\R^d} \norm{\omega}_1^s \abs{\fourier{f}(\omega)} 
        d\omega < \infty.
    \end{equation}
    There exists a 2NN of the form with ReLU activation function $\sigma$
    \begin{equation}
        f_n(x) = f(0) + \nabla f(0) \cdot x + v \cdot 
        \frac{1}{n} \sum_{j=1}^n a_j \sigma(b_j\tr x + c_j)
    \end{equation}
    with $a_j\in[-1,1]$, $\norm{b_j}_1 = 1$, $c_j\in[0,1]$ and $v \leq
    2v_{f,2}$ such that
    \begin{equation}
        \sup_{\mathbf{x} \in D} \norm{f(x) - f_m(x)}_{\infty} 
        \leq c v_{f,2} \sqrt{d+\log{n}} \, n^{-1/2-1/d}
    \end{equation}

    for some universal $c > 0$.
\end{theorem}

\begin{theorem}
    Let $U = [-1,1]^d$. Suppose $f: U \to \R$ admits a Fourier representation
    and the spectral norm of order $2$ of $f$ is finite, i.e.
    \begin{equation}
        v_{f,3} = \int_{\R^d} \norm{\omega}_1^3 \abs{\fourier{f}(\omega)} 
        d\omega < \infty.
    \end{equation}
    There exists a 2NN of the form with squared ReLU activation function
    \begin{equation}
        f_n(x) = f(0) + \nabla f(0) \cdot x + v \cdot 
        \frac{1}{n} \sum_{j=1}^n a_j \sigma(b_j\tr x + c_j)
    \end{equation}
    with $a_j\in[-1,1]$, $\norm{b_j}_1 = 1$, $c_j\in[0,1]$ and $v \leq
    2v_{f,2}$ such that
    \begin{equation}
        \sup_{\mathbf{x} \in D} \abs{f(x) - f_m(x)} \leq 
        c v_{f,3} \sqrt{d} n^{-1/2-1/d}
    \end{equation}

    for some universal $c > 0$.
\end{theorem}

\subsection{Approximation in $L_{\infty}$ with unbounded weights}

\subsection{Approximation in $L_2$ with bounded weights}

\begin{theorem}
    Let $U=[-1,1]^d$. Suppose $f: U \to \R$ is a function that admits an
    integral representation
    \begin{equation}
        f(x) = v \int \,d\mu(t,a),
    \end{equation}

\end{theorem}


\begin{theorem}
    Under similar setup as the theorem before, suppose $f$ has a finite spectral
    norm of order $3$, i.e. $v_{f,3} < \infty$. Then there exists a finite
    linear combinations of ridge function of the form
    \begin{equation}
        f_{m, b_0}(x) = \frac{v}{m}\sum_{j=1}^m a_j \sigma_{\text{}}(b_j\tr x + c_j)
        \quad \forall x \in U,
    \end{equation}

\end{theorem}

\subsection{Approximation in $L_2$ with unbounded weights}


\section{Approximation in infinite-width Barron spaces}
\label{sec:appro_barron_space}

\begin{theorem} [Direct Approximation]
    For any function $f \in \mathcal{B}$ and $m > 0$, there exists a two-layer
    neural network $f_m = f(\mathbf{x}, \Theta) = \frac{1}{m}\sum_{k=1}^m a_k
        \sigma(\mathbf{b}_k^T \mathbf(x) + c_k)$. $\Theta$ is the set of parameters
    $\Theta = \{(a_k, \mathbf{b}, c_k), k=1,\dots,m\}$ such that
    \begin{equation*}
        \norm{f - f_m}^2 \leq \frac{3\norm{f}_{\mathcal{B}}^2}{m}
    \end{equation*}

    Furthermore, we have
    \begin{equation}
        \norm{\Theta}_{path} := \frac{1}{m} \sum_{k=1}^m \abs{a_k} (\norm{\mathbf{b}_k}_1 + \abs{c_k})
        \leq 3\norm{f}_{\mathcal{B}}
    \end{equation}
\end{theorem}

The $\norm{\Theta}_{path}$ is the per-unit norm control for 2NN defined in
\cite{neyshaburNormBasedCapacityControl2015}. \TONOTE{add sth about the
    convexity}

\section{Difference and Connection Between Different Barron Spaces}
\label{sec:diff_barron_spaces}


As mentioned in the beginning of the chapter, we will start with the definition
of the different Barron spaces, namely the \textit{Fourier-analytic Barron
spaces} and the \textit{infinite-width Barron spaces}. Although some
relationships between these spaces has been examined and understood partially in
\cite{eBarronSpaceFlowinduced2021,eMathematicalUnderstandingNeural2020}, we hope
to clarify this problem in this section inspirerd by the work from
\cite{carageaNeuralNetworkApproximation2022}.

Firstly, we denote the set of all Borel probability measures on $\Omega = \R^1
\times \R^d \times \R^1$ by $\Sigma_{\Omega}$ and we write functions that admits
the integral form below
\begin{equation}
    f(x) = \int_{\omega} a\sigma(b\tr x + c) \,d\mu(a,b,c), \quad
    \forall x \in \R^d
\end{equation}

Given a nonempty, bounded domain $U \subset \R^d$, we have defined various
Barron spaces:

\begin{itemize}
    \item $\bspace{\mcal{F}, s}$, $s \in \{1,2\}$ Fourier-analytic Barron spaces 
        \eqref{def:fourier_space} with $\norm{\cdot}_{\mcal{F},s}$ 
        \eqref{def:spectral_norm}
    \item $\mcal{B}$ infinite-width Barron spaces \eqref{def:barron_space} with
        $\norm{\cdot}_{\mcal{B}}$ \eqref{eq:barron_norm}
    {
        \setlength\itemindent{25pt}
        \item $\bspace{H}$ classical Barron space \eqref{def:heaviside_space}
            with $\norm{\cdot}_{\mcal{B}_H}$ \eqref{def:heaviside_norm}
    }
\end{itemize}

We \textit{could} include the classical Barron space within the infinite-width
Barron spaces when $\bspace{p}(U), p=0$ with a ReLU$^0$ activation function
which is essentially the Heaviside function but we decide against it to
emphasize on $\bspace{H}(U)$ as it is frequently visited. We denote
infinite-width Barron spaces with $\bspace{}(U)$ after the equivalence of
$\bspace{p}(U), 1\leq p \leq\infty$ has been shown in Lemma
\ref{lamma:equivalence_barron_space}.

We state the following known properties from the literature.

% hookrightarrow or subset
\begin{lemma}
    Given the constructions of spaces above and a nonempty bounded domain $U$ in
    $\R^d$, then the following relationships holds:

    1) $\mcal{B}(U) \subset \bspace{H}(U)$

    2) $\bspace{\mathcal{F}, 1}(U) \subset \bspace{H}(U)$

    3) $\bspace{\mathcal{F}, 2}(U) \subset \mcal{B}(U)$
\end{lemma}


\begin{proof}

% As shown in weinan, functions in the $\mathcal{B}_{ReLU}$ i.e. Barron space, are
% Lipschitz.

% https://math.stackexchange.com/questions/2319301/why-must-bounded-sets-be-contained-within-a-closed-ball
\textbf{1):} 
Firstly, we try to connect ReLU and the indicator function through integral. As
the set $U$ is nonempty and bounded in $\R^d$, this implies that there exists a
a open ball, $B_r(\cdot)$, with a radius $r > 0$ whose closure contains $U$ such
that
\begin{equation}
    U \subset \closure{B_r(x)}
\end{equation}
for some $x \in \R^d$. Assume that $U \subset \closure{B_r(0)}$. Let $C:=1+r$
and then we have:

\begin{equation}
    \sigma(x) = \int_0^C H(y- t) \,dt \quad \forall y \in \R 
\end{equation}

For all function $f \in \mcal{B}(U)$. Let $\beta_{b,c} = \abs{b} + \abs{c}$ for
any $b\in\R^d, c\in\R$ and note that $\abs{b\tr x + c} \leq C\cdot\beta_{b,c}$.
Thanks to the positive homogeneity of ReLU function $\sigma$, i.e.
$\sigma(\lambda x) = \lambda \sigma(x)$ for $x \in \R$.

For a measure $\mu \in \Sigma_{\Omega}$ and all $x \in U$
\begin{align*}
    \mu_{ReLU}(x) 
    &= \int_{\Omega} a \sigma(b\tr x + c )\,d(a,b,c) \\
    &= \int_{\Omega} \beta_{b,c} \sigma(\frac{b\tr x}{\beta_{b,c}}+ \frac{c}{\beta_{b,c}}) \,d\mu(a,b,c) \\
    &= \int_{\Omega} \int_0^C a \beta_{b,c} H(\frac{b\tr x}{\beta_{b,c}} + \frac{c}{\beta_{b,c}} -t)\,dt\,d\mu
    (a,b,c) \quad \text{(Fubini's Theorem)}\\
    &= \int_{\Omega} a H(b\tr x + c') \,d\nu(a,b,c') = \nu_H(x)
\end{align*}
where $\nu$ is the pushforward of the measure $\mu \otimes \lambda$ and
$\lambda$ is the Lebesgue measure on $[0, C]$.

\begin{equation}
    T: \Omega \times [0,C] \to \Omega, \quad 
    ((a,b,c), t) \mapsto
    (a\beta_{b,c}, \frac{b}{\beta_{b,c}}, \frac{c}{\beta_{b,c}} - t )
\end{equation}

We have 
\begin{equation}
    r(f, \mu, h) = \int_{\Omega} \abs{a}\,d\mu(a,b,c) 
    = \int_{\Omega}\int_0^C 
\end{equation}

Therefore, it shows that for any function $f \in \norm{f}_{\mathcal{B}_H}$
\begin{equation}
    \norm{f}_{\mathcal{B}_H} \leq C \norm{f}_{\mathcal{B}_{ReLU}} < \infty
\end{equation}
hence the inclusion holds.

\textbf{2):} This is an direct consequence of \cite[Theorem
2]{barronNeuralNetApproximation1992} and we include the proof for completeness.

\begin{theorem}
    Let $f$ be a function that admits a Fourier representation with finite
    spectral norm of order $1$, i.e.
    \begin{equation}
        v_{f,1} < \infty
    \end{equation}
    then $f(x) - f(0)$ can be expressed as an infinite convex combination of
    indicator multiplied by a constant
    \begin{equation}
        f(x) - f(0) = \int_{\R^d}\int_0^1 (
            \indicator{} - \indicator{}
        ) \,d\omega\,dt
    \end{equation}
\end{theorem}

By Fourier transform, note that
\begin{equation}
    f(x) - f(0) = \int (e^{i\omega\tr x} - 1) \fourier{f}(\omega)\,d\omega
\end{equation}

We also have
\begin{equation}
    e^{iz} - 1 =
    \begin{cases}
        i \int_0^c \indicator{\{z>u\}} e^{iu} \,du \quad
            &\text{ when } z \in [0,c] \\
        -i \int_0^c \indicator{\{z<-u\}} e^{iu} \,du \quad
            &\text{ when } z \in [c,0] 
    \end{cases}
\end{equation}

It follows that
\begin{equation}
    e^{iz} - 1 = i \int_0^c (
        \indicator{\{z>u\}} - \indicator{\{z<-u\}}  
    )
    e^{iu} \,du 
\end{equation}

Integrating when $z = \omega x$ and $c=\abs{\omega}_{[0,1]}$ defined in
\eqref{def:fourier_class} gives
\begin{equation}
    f(x) -f(0) = i\int_{\R^d} (\int_0^{c} (
        \indicator{\{\omega x>u\}} - \indicator{\{\omega x<-u\}}
    ) e^{iu}\,du)
    \fourier{f}(\omega)\,d\omega
\end{equation}

Only taking the real part of the LHS and RHS and integrating using Fubini's theorem
\begin{equation}
    f(x) = f(0) + \int_{\R^d} \int_0^1 (
        \indicator{\{\omega x<-t\}} - \indicator{\{\omega x>t\}}
    ) \abs{\omega} \sin(t\omega + \theta_{\omega})
    \fourier{f}(\omega)\,d\omega\,d\omega
\end{equation}

A similar argument as the proof in \TONOTE{proof of Fourier} is employed here
where the ``drawing'' parameters from the distribution shows that the $f(x) -
f(0)$ is the closure of the convex hull of finite linear combinations.

Hence we show the inclusion in 2).

\textbf{3):} As $U$ is nonempty and bounded subset in $\R^d$, we can fix a point
$x_0 \in \R^d$ and a radius $r > 0$ such that $U \subset x_0 + [0, r]^d$. Let
$f$ be a function in $\mathcal{B}_{\mathcal{F},2}$ with norm $v_{f,2}\leq 1$.
We define two mapping from $\R^d$ to $\mathbb{C}$ $G, H$.
\begin{align*}
    G(\omega) &= \frac{1}{2} (\fourier{f}(\omega) 
                    + \bar{\fourier{f}(-\omega)}) \\
    H(\omega) &= \frac{1}{r^d} \cdot 
                    e^{i \frac{x_0\tr}{r} \omega} \cdot 
                    G(\omega / r)
\end{align*}
where $\fourier{f}(\omega)$ is the Fourier transform of $f$.

We calculate their respective norm $G_{\mathcal{F},2} \leq 2$ and
$H_{\mathcal{F},2} \leq 2r^2$. We then define two functions with $G,H$ being
their Fourier transform, respectively.
\begin{align*}
    g(x) := \int_{\R^d} e^{ix\omega} G(\omega)\,d\omega\\
    h(x) := \int_{\R^d} e^{ix\omega} h(\omega)\,d\omega\\
\end{align*}
and it is easy to see that for all $x \in U$, $f(x)=g(x)=h(\frac{x-x_0}{r})$.

By construction, the spectral norm of $h$ is finite. We have
$\norm{h}_{\mathcal{B}_{ReLU}}$ is finite with some constant $C_h$ thanks to
Theorem 9 in \cite{eMathematicalUnderstandingNeural2020}. Therefore, $h(y)$ can
be represented as the integral form
\begin{equation}
    h(y) = \int_{[0,1]^d} a \sigma(b\tr x + c) \,d\mu(sa,b,c) \quad \forall y \in [0,1]^d
\end{equation}
where $\mu \in \Sigma_{\Omega}$ and $\norm{\mu}_{ReLU} \leq C_h$.

Since $y=\frac{x-x_0}{r}$ for all $x\in U$, the results above implies that 
\begin{equation}
    f(x) = h(\frac{x-x_0}{r}) \int_{\Omega} a 
    \sigma(\frac{b\tr x}{r} + c - \frac{b\tr x_0}{r})\,d\mu(a,b,c)
    = \mu'_{ReLU}(x)
\end{equation}
where $\mu'$ is the pushforward of $\mu$ under the map:
\begin{equation}
    \Phi: \Omega \to \Omega, (a,b,c) \mapsto (a, \frac{b}{r}, c - \frac{b\tr x_0}{r})
\end{equation}

We can then calculate $\norm{\mu'}_{ReLU} \leq (1+\norm{x_0}_2)
\norm{\mu}_{ReLU}$ is smaller than some constant $C$ w.r.t. the choice of
$d,r,x_0$. Hence $f$ is in $\mathcal{B}_{ReLU}$ with some measure $\mu'$ if $f
\in \mathcal{B}_{\mathcal{F},2}$ and hence the inclusion

\end{proof}

It has been argued in \cite{eRepresentationFormulasPointwise2020} that
$\bspace{\mcal{F},1}$ embeds into the $\mcal{B}$ but
\cite{carageaNeuralNetworkApproximation2022} claims that this embedment wrongly
interpret the results of
\cite{barronUniversalApproximationBounds1993,barronNeuralNetApproximation1992}.
We will show next that $\bspace{\mathcal{F},1}\not\subset\mcal{B}$. In other
words, the original model class proposed by
\cite{barronNeuralNetApproximation1992} is not \textit{contained} in the novel
Barron space $\mcal{B}$ introduced recently by
\cite{eBarronSpaceFlowinduced2021}. 


\begin{theorem}~\cite[Proposition 7.4]{carageaNeuralNetworkApproximation2022}

    Let $U \subset \mathbb{R}^d$ be bounded and have nonempty interior. For
    $s>0$, if $\mathcal{B}_{\mathcal{F},s}(U) \subset \mathcal{B}_{1}(U)$, then
    $s \geq 2$. In particular $\mathcal{B}_{\mathcal{F},1}(U) \not\subset
        \mathcal{B}_{1}(U)$
\end{theorem}

\begin{remark}
    It has been discussed in earlier section that we have the equivalence of
    Barron norm and hence Barron spaces for $1 \leq p\leq +\infty$ (see
    \eqref{eq:barron_fouier_int}) with ReLU as activation function.

    In other words, combined with the fact that
    $\mathcal{B}_{\infty}=\mathcal{B}_p$, we can say that the class of functions
    where $f$ admits a Fourier representation with finite second moment ($v_{f,
    2} < \infty$) are well ``contained'' inside the \textit{infinite-width
    Barron spaces}. However, $\mathcal{B}_{\mathcal{F}, 1}(U)$ still
    encapsulates a boarder class of functions.
\end{remark}



% \section{Minimax Lower Bounds for Two Neural Network Model}

% For simplicity, data are of the form $\{(U_i, Y_i)\}_{i=1}^n$ from a

% For functions $f$ in the class $\mathcal{F}: [-1, 1]^d \to \mathcal{R}$, the
% minimax risk is:

% \begin{equation} R_{n,d} := \inf_{\hat{f}} \sup_{f\in \mathcal{F}}
%     \ERW{\norm{f - \hat{f}}^2} \end{equation}

% It has been shown in \TOCITE(Barron Minimax paper) that the minimax lower
% bound for neural nets is of order $\bigO(\log{d}/n)$ to some fractional power
% when $d$ is of larger order than $n$.

% \section{Risk Bounds for }

% {\itshape Firstly, I need to understand the difference between approximation
% rates and estimation of population risk.

% There exists a vast dictionary of definitions with similar wordings but the
% context is hughly different.

% e.g. What's the estimate of the population risk? In my understanding,

% $f^* - f_m$ where $f_m$ is the best solution in such hypothesis class is the
% approximation rates

% It should be that Population risk is w.r.t. a particular loss function?}

% \begin{equation} f(x) = \sum_{i=1}^m a_i \phi (\spr{\mathbf{w_i}}{x})
%     \end{equation}





%%% Local Variables: %% mode: latex %% TeX-master: "MasterThesisSfS" %% End: 
