\chapter{Two-layer neural networks and nonlinear approximation}
\label{ch:preliminary}

This chapter introduces the 2NN model and explores fundamental findings
concerning $n$-term approximation using a dictionary. Section
\ref{sec:what_is_2nn} presents an overview of the 2NN model and its basic
properties. In section \ref{sec:preliminary}, we delve into the well-known
results regarding $n$-term approximation from a dictionary. Finally, section
\ref{sec:uat} outlines the problem of \textit{density} around approximation with
2NN.

The aim of this chapter is to summarize well-established results that will be
utilized in subsequent sections of this thesis. For an in-depth introduction, we
recommend reading \cite{devore_1998,pinkusApproximationTheoryMLP1999}.


\section{What is a two-layer neural network}
\label{sec:what_is_2nn}

This section provides an introduction to \textit{feed-forward neural networks}
and their basic properties, with a focus on 2NN. While recurrent neural networks
and other architectures are commonly used for specific applications, fully
connected feed-forward neural networks offer a convenient way to balance model
complexity and approximation efficiency. This is especially the case in 2NN:
complexity is solely determined by the width or the number of nodes $n$. We will
begin by defining 2NN and then discuss their elementary properties.

This section is based on \cite{devoreNeuralNetworkApproximation2021,
shalev-shwartzUnderstandingMachineLearning2014}.

\subsection{Feed-forward neural network}

The class of artificial neural networks (\gls{ann}) known as feed-forward neural
networks, denoted by $\nn$, is characterized by a directed acyclic graph
(\gls{dag})
\begin{equation*}
    \mathcal{G} = (\mathcal{V}, \mathcal{E}).
\end{equation*}

The \textit{architecture} of $\nn$ is represented by the associated graph
$\mathcal{G}$, which consists of a finite collection of vertices $\mathcal{V}$
and a finite set of edges $\mathcal{E}$. A computation unit associated with each
$v \in \mathcal{V} \setminus \mathcal{I}$ is known as a \textit{node} or
\textit{neuron}. The variables $\mathcal{I}$ and $\mathcal{O}$ represent the
input and output layers of the neural network, respectively. The following basic
properties holds for $\nn$: 
\begin{enumerate}
    \item For each vertex $v \in \mathcal{V} \setminus \mathcal{I}$, there is an
    associated activation function $\sigma: \R \to \R$.
    \item For each edge $e \in \mathcal{E}$, there is an associated scalar
    weight $w_e \in \R$.
\end{enumerate}

The nodes in the input layer receive scalar inputs, which are then observed by
downstream nodes. Apart from the nodes in $\mathcal{I}$, each node in $\nn$
takes a superposition of inputs from its upstream nodes, mediated by the
associated weights $w_e$. The edges $e = (v, v')$ between a vertex $v$ and its
upstream vertices $v'$ correspond to these weights.

The output function of $\nn$ can be seen as a mapping from $\R^d$ to $\R^{d'}$,
$d = \abs{\mathcal{I}}, d' = \abs{\mathcal{O}}$. The family of functions
produced by $\nn$, given a fixed architecture $\mathcal{G}$, is determined by
the trainable parameters, denoted by $\Theta := \{w_e,b_v\}, e \in \mathcal{E}, v
\in \mathcal{V} \setminus \mathcal{I}$.

\subsection{Fully connected neural networks}

The general definition previously presented covers virtually all networks
architecture encountered in practice. However, in the following chapters, we
will focus specifically on a specialized class of neural networks known as
\textit{fully connected neural networks}, where the vertices are organized into
layers.

In a fully connected network, each vertex is connected exclusively to all
vertices in the next layer via edges and to no other layers.  The input layer,
$\mathcal{I}$, consists of $d$ input vertices, where each vertex receives an
external scalar signal $x_i \in \R, i \in \{1, \dots, d\}$. The combined input
$x := (x_1, \dots, x_d) \in \R^d$ serves as the independent variable for the
output function $f_{\nn}$. The input layer is followed by $L$ hidden layers,
with $L$ representing the \textit{depth} of the network. Each hidden layer
contains $n_j$ hidden vertices, where $n_j$ is the \textit{width} of the $j$th
layer, $j = 1, 2,\dots, L$.


Each hidden node is associated with an activation function $\sigma$, while nodes
at the output layer $\mathcal{O}$ take the identity function to ensure that
$f_{\nn}(x)$ is a linear combination of nodes at the $L$th layer plus a bias
term. Consequently, for a fixed architecture, the \textit{weight matrices} and
\textit{bias vectors} alone suffice to describe the output function
$f_{\nn}(x)$,
\begin{equation}
    W^{(l)} \in \R^l \times \R^{l-1}, b^{(l)} \in \R^l 
    \quad l = 1, 2, \dots, L+1.
\end{equation}

Let $X^{(l)} \in \R^l$ be the outputs of layer $l$, the output function
is 
\begin{equation}
    f_{\nn}(x) := W^{L+1} X^{L} + b^{L+1}
\end{equation}
and each layer is calculated recursively
\begin{align}
    X^{(0)} 
        &= x \in \R^d, \quad d = \abs{\mathcal{I}} \\
    X^{(l)} 
        &= \sigma(W^{(l)} X^{(l-1)} + b^{(l)}), 
        \quad l = 1, 2, \dots, L+1, 
\end{align}

Here the activation function $\sigma: \R \to \R$ is assumed to be
coordinate-wise or element-wise independent:
\begin{equation*}
    \sigma(x) = (\sigma(x_1), \sigma(x_2), \dots, \sigma(x_d)),
\end{equation*}
for $x = (x_1, x_2, \dots, x_d) \in \R^d, d \in \Nat$.

\subsection{The family of functions from neural networks}

Throughout the following discussion, we will assume that each layer of the fully
connected neural network has an identical width, that is,
\begin{equation*}
    n_1 = n_2 = \cdots = n_L = W.
\end{equation*}
A NN of different width can be embedded into a wider NN of fixed width
$$
    W := \max_{j=1,\dots,L} n_j.
$$
This can be achieved simply by inserting zero bias terms in each bias vector
$b^{(l)}$ in each $l$th layer and adding zero-weighted edges between layers. To
account for the newly added vertices, new edges with a weight of $0$ are added,
connecting to the forthcoming $(j + 1)$th layer. The fully connected neural
networks we discuss will be of a fixed width.

Consider a fully connected neural network with width $W$ and depth $L$, and let
$\sigma$ be the activation function. If $d$ and $d'$ are the input and output
dimension, respectively, we denote the set of all possible functions $f: \R^d
\to \R^{d'}$ produced by a fixed architecture with trainable parameters as
\begin{equation}
    \nnFunc{W}{L}{\sigma}.
\end{equation}

The number of trainable parameters is given by 
\begin{align}
    n(W, L) := (d + 1)W + W(W + 1)(L - 1) + d'(W + 1).
\end{align}

\begin{remark}
    It is worth noting that $\nnFunc{W}{L}{\sigma}$ is not a linear space, even
    when $L = 1$, as it is not closed under addition of functions. For example,
    there exist functions $f_1, f_2 \in \nnFunc{W}{L}{\sigma}$ such that their
    sum $f_1 + f_2$ does not belong to $\nnFunc{W}{L}{\sigma}$.
\end{remark}

In what follows, we only consider the specialized case of 2NN ($L=1$), and the
set of functions by 2NN is denoted by
\begin{equation}
    \label{eq:set_of_all_functions_2nn}
    \nnFunc{W}{1}{\sigma} = \mcal{M}(\sigma) =  \Bigg\{
        b_0 + \sum_{j=1}^W a_j \sigma(b_j\tr x + c_j), \quad
        a_j, c_j \in \R,  b_j \in \R^d
    \Bigg\}.
\end{equation}
Here, $W$ refers to the width of the 2NN model, i.e. the number of nodes. When
discussing complexity in approximation, $n$ is usually used to represent the
number of nodes required for a certain level of accuracy.

\section{$n$-term approximation}
\label{sec:preliminary}

% Jonas-Barron method

% Maurey's Theorem: with a closure of convex hull suggests bound

% Here I will start classical probabilistic argument of Maurey

This section introduces some fundamental findings regarding $n$-term
approximations derived from a dictionary. Nonlinear approximation problems are
typically presented as follows: given a target function $f\in\mcal{H}$, select a
basis and an $n$-term approximation to $f$ from that basis. The class of bases
is known as $library$, as seen in wavelets or splines approximation. Our focus
is on a closely related form of approximation: $n$-term approximation from a
dictionary $\mathbb{D} \subset \mcal{H}$. The dictionary $\mathbb{D}$ can be an
arbitrary subset but its choice is subject to practical computational
limitations. 

This section is mainly based on \cite[Section 8]{devore_1998} and
\cite{vandervaartWeakConvergenceEmpirical1996}, unless stated otherwise.

Let $\mathbb{D} = \{d_1,d_2,\dots\}$ be a uniformly bounded domain in a Hilbert
space $(\mcal{H}, \norm{\cdot}_{\mcal{H}})$
\begin{equation}
    \sup_{d_j\in \mathbb{D}} \norm{d_j}_{\mcal{H}} < \infty \quad 
    \forall d_j \in \mathbb{D}.
\end{equation}

For every $n \in \Nat$, the collection of all functions in $\mcal{H}$
which can be expressed as a linear combination of at most $n$ elements of
$\mathbb{D}$ is denoted by
\begin{equation}
    \label{eq:dict_represent}
    \Sigma_n(\mathbb{D}) = \Bigg\{
        \sum_{j=1}^n a_j d_j, \quad
        d_j \in \mathbb{D}, a_j \in \R
    \Bigg\}.
\end{equation}

Any function $f_n \in \Sigma_n(\mathbb{D})$ with $n \in \Nat$ is represented as
\begin{equation}
    \label{eq:gm}
    f_n = \sum_{j=1}^n a_j d_j, \quad d_j \in \mathbb{D}.
\end{equation}

To include the control over the
coefficients, also called \textit{outer parameters}, $\alpha_j$ in the above
expansion, we introduce
\begin{equation}
    \label{eq:dict_sigma}
    \Sigma^{t}_n(\mathbb{D}, M) := \Bigg\{
        f = \sum_{j=1}^n \alpha_j d_j: 
        d_j \in \mathbb{D} \textnormal{ and } 
        \sum_{j=1}^n \abs{\alpha_j}^t \leq M^t, \quad 
        n \in \Nat, \alpha_j \in \R
    \Bigg\}
\end{equation}
for any $t \in \Nat \bigcup\, \{\infty\}$ and any $M > 0$. 

Let $K^t_n(\mathbb{D}, M)$ be the closure of $\Sigma^t_n(\mathbb{D}, M)$ in
$\mcal{H}$
\begin{equation}
    K^t_n(\mathbb{D}, M) := \closure{\Sigma^t_n(\mathbb{D}, M)}.
\end{equation}

It should be noted that $t = 1$, $K^1_n(\mathbb{D}, M)$ is the class of
functions that are a signed \textit{convex} combination of elements in
$\mathbb{D}$. When $t = \infty$, $K^{\infty}_n(\mathbb{D}, M)$ corresponds to
the sets whose coefficients are bounded in $\lp{\infty}$. 

Next, we define the union for all $M > 0$
\begin{equation}
    K^t_n(\mathbb{D}) = \bigcup_{M > 0} K^t_n(\mathbb{D}, M).
\end{equation}

We define a seminorm for functions $f \in K^t(\mathbb{D})$
\begin{equation}
    \label{eq:general_seminorm}
    \abs{f}_{K^t(\mathbb{D})} = \inf \{
        M >0: f \in K^t_n(\mathbb{D}, M)
    \}.
\end{equation}


For $f \in \mcal{H}$, the approximation error is defined as
\begin{equation}
    \label{eq:appro_err_hilbert}
    e_n(f, \mathbb{D}, \mcal{H})
        := \sup_{g\in\Sigma_n(\mathbb{D})} \norm{f - g}_{\mcal{H}}.
\end{equation}
where $\Sigma_n(\mathbb{D})$ is the collection of all functions in $\mcal{H}$
that can be expressed as a linear combination of at most $n$ elements of
$\mathbb{D}$ \eqref{eq:dict_represent}.

Assuming that approximation holds with a dictionary $\mathbb{D}$ in $\mcal{H}$
such that for any function $f \in \mcal{H}$, the approximation error is
\begin{equation}
    \label{eq:appro_error_general}
    e_n(f, \mathbb{D}, H) \leq n^{-\alpha} C_{\mathbb{D}}.
\end{equation}

We are concerned with the coefficients in the exponents $\alpha$ and what
$C_{\mathbb{D}}$ is dependent on.

\subsection{Maurey's Theorem}
\label{subsec:maurey}

Approximation with a finite linear combination of elements from a bounded
dictionary in a Hilbert space $\mcal{H}$ achieves an error rate of
$\bigO(n^{-1/2})$ by~\cite{pisierRemarquesResultatNon1980} and this holds for
any bounded $\mathbb{D} \subset \mcal{H}$.

\begin{definition}[Covering numbers and entropy]
    % http://www.stat.yale.edu/~yw562/teaching/598/lec14.pdf
    \label{def:covering_num}
    Let $(\mcal{F}, \norm{\cdot}_{\mcal{F}})$ be a subset of a normed space. The
    \textit{covering number} $N(\epsilon, \mcal{F}, \norm{\cdot}_{\mcal{F}})$ is
    the minimal number of balls $\{g: \norm{f - g}_{\mcal{F}} < \epsilon\}$ of
    radius $\epsilon$ required to cover the set $\mcal{F}$. 
\end{definition}

\begin{definition}[$n$-th dyadic entropy number]
    \label{def:entropy} The \textit{entropy} is the logarithm of the covering
    number. For $n \in \Nat$, the $n$-th (dyadic) entropy number is given by
    \begin{equation}
        \entropyNum{\mcal{F}}
        = \inf \{
            \epsilon > 0: \mcal{F} \text{ is covered by } 2^n 
            \text{ balls of radius } \epsilon
        \}.
    \end{equation}
    % https://arxiv.org/pdf/1211.1559.pdf
\end{definition}



\begin{theorem}[Maurey's Theorem]
    \label{thm:maurey}
    Let $\mathbb{D} = \{d_1, d_2, \dots\}$ be a subset of a Hilbert space
    $(\mcal{H}, \norm{\cdot}_{\mcal{H}})$. $\mathbb{D}$ is uniformly bounded
    \begin{equation}
        \sup_{d\in\mathbb{D}} \norm{d}_{\mcal{H}} < \infty.
    \end{equation}
    For every $f \in \mcal{H}$ of the form
    \begin{equation}
        \label{eq:maurey_eq_1}
        f = \sum_{j=1}^m c_j d_j, \quad
        \sum_{j=1}^m \abs{c_i} < \infty, \quad
        c_j \in \R, m > 0
    \end{equation}
    and every $n \in \Nat$, there exists a $g_n = \sum_{j=1}^n a_j
    d_j$ with at most $n$ non-zero coefficients with
    \begin{equation}
        \sum_{j=1}^n \abs{a_j} \leq
        \sum_{j=1}^n \abs{c_j}, 
        \quad a_j \in \R
    \end{equation}
    such that
    \begin{equation}
        \norm{f - g_n}_{\mcal{H}} \leq
        2 \entropyNum{\mathbb{D}}
        \cdot n^{-\frac{1}{2}}
        \cdot \sum_{j=1}^n \abs{c_j}.
    \end{equation}

\end{theorem}

\begin{proof}
    Without loss of generality, we can assume $m < \infty$ and the coefficients
    $c_j$ in \eqref{eq:maurey_eq_1} are non-negative and the sum $\sum_{j=1}^m
    \abs{c_j} = 1$.

    We partition the index set $I = \{1,2,\cdots,m\}$ into $n$ nonempty subsets
    $I_p, \,p=1,2,\cdots,n$ such that each subset $\mathbb{D}_p$ of $\mathbb{D}$
    has a diameter smaller than $\epsilon$.

    For each partition $p = 1, 2,\cdots, n$, $n_p := \abs{I_p}$. Let $S_p = \sum_{i\in I_p} c_i$
    and 
    \begin{align}
        \hat{f}_p := \frac{S_p}{n_p} (\hat{d}_1^{(p)} + \cdots + \hat{d}_{n_p}^{(p)}) \\
        \hat{f} := \sum_{p=1}^n \hat{f}_p
    \end{align}
    where each $\hat{d}_{k}^{(p)}$ is identically distributed and pairwise
    independent from $\mathbb{D}_p$ and the probability of equalling to $d_i \in
    \mathbb{D}_p$ is $\frac{c_i}{S_p}$.
    \begin{equation}
        \PR{\hat{d}_{k}^{(p)} = d_i} = \frac{c_i}{S_p}, 
        \quad i \in I_p, p = 1,2,\cdots, n.
    \end{equation}

    Next, we can show that 
    \begin{equation}
        \ERW{\hat{f}_p} = \frac{S_p}{n_p} \sum_{k}^{n_p}\ERW{\hat{d}_{k}^{(p)}}
        = \frac{S_p}{n_p} \sum_{i\in I_p} \frac{c_i}{S_p} d_i = f_p
    \end{equation}

    \begin{equation}
        \ERW{\norm{f - \hat{f}}^2} = \sum^n_p \VAR{f_p - \hat{f}_p} = \sum^n_p \VAR{\hat{f}_p}
    \end{equation}

    As each partition $I_p$, the $\mathbb{D}_p$ has a diameter of smaller than
    $\epsilon$ which implies variance of $\hat{d}_p^{(p)}$ is controlled by
    $\epsilon^2$, we have
    \begin{equation}
        \ERW{\norm{f - \hat{f}}^2} =\sum^n_p \VAR{\hat{f}_p} \leq \sum^n_p \frac{\epsilon^2}{n} S_p = \epsilon^2/n.
    \end{equation}

    In conclusion, there must exist some realization $\hat{f}$ such that
    $\norm{f-\hat{f}}^2$ is smaller than $\epsilon^2/n$ and $\epsilon$ can be
    chosen close to the entropy. Such realization is a linear combination of at
    most $2n$ elements from $\mathbb{D}$.
\end{proof}

\begin{remark}
    This is refinement of the result in \cite{pisierRemarquesResultatNon1980} in
    a Hilbert space. The approximation rate by Maurey is not always sharp and
    one can use the compactness of the dictionary $\mathbb{D}$ to improve it.
\end{remark}

\begin{corollary}
    \label{cor:maurey}
    If $f$ is in the closure of the convex hull of a set $\mathbb{D}$ in a
    Hilbert space $(\mcal{H}, \norm{\cdot}_{\mcal{H}})$ and every function $d
    \in \mathbb{D}$ is bounded, then for every $n \in \Nat$, there is an $f_n$
    in the convex hull of $n$ elements in $\mathbb{D}$ such that
    \begin{equation}
        \norm{f - f_n}_{\mcal{H}}\leq C n^{-\frac{1}{2}}
    \end{equation}

    where $f_n = \sum_{j=1}^n \alpha_j \cdot d_j, \quad d_j \in \mathbb{D},
    \quad \sum_{j=1}^n \alpha_j = 1$, and $C^2 \geq \sup_{d\in \mathbb{D}}
    \norm{g}_{\mcal{H}}^2 - \norm{f}_{\mcal{H}}^2$.

\end{corollary}

\subsection{Iterative approximation}
\label{subsec:iterative}

\cite{jonesSimpleLemmaGreedy1992} has also shown the same approximation error
$\bigO(n^{-1/2})$ using an iterative argument. One can find a proof in
\cite[p. 611]{jonesSimpleLemmaGreedy1992} and a refinement of Jone's result in
\cite[Theorem 5]{barronUniversalApproximationBounds1993}. 

Let $\mathbb{D}$ be a nonempty bounded subset of a Hilbert space $\mcal{H}$.
Suppose a sequence of function $f_n$ were used to approximate a element $f$ in
$\mcal{H}$ with the following algorithm
\begin{equation}
    \label{eq:seq_appro_jones}
    f_n = \alpha_n + f_{n-1} + (1-\alpha_n) d_n, \quad
    d_n \in \mathbb{D}, \alpha_n \in [0,1], n \geq 1.
\end{equation}
The iteration starts with $f_1 = d_1 \in \mathbb{D}$ and $\alpha_1 = 0$ and the
sequence $\{f_n\}$ defined as above is in the convex hull of the points
$\{d_1,\dots, d_n\}$.

\begin{theorem}[Jone's Theorem]
    Let $\mathbb{D}$ be a nonempty bounded domain of a Hilbert space $\mcal{H}$.
    Let $C_{\mathbb{D}} = \sup_{d \in \mathbb{D}} \norm{d}^2 < \infty$ and set
    $C_f = C_{\mathbb{D}} - \norm{f}^2$. If $f$ is in the closure of the convex
    hull of $\mathbb{D}$, i.e. $f \in \conv(\mathbb{D})$. Then for every $n \in
    \Nat$, $f_n$ is chosen iteratively as
    \begin{equation}
        \norm{f_n - f}^2 \leq \inf_{\alpha \in [0,1]} \inf_{d \in \mathbb{D}}
        \norm{\alpha f_{n-1} + (1-\alpha)d - f}^2 + e_n
    \end{equation}
    where $e_n \leq \frac{C}{n(n+C/C_f -1 )}$ for some $C > C_f$. Then
    approximation error for each $n \in \Nat$ is $\bigO(n^{-1/2})$.
\end{theorem}

\begin{proof}

     Set $f_{\delta} = \sum_{j=1}^m a_j d_j$ be a convex combination of $m$
    elements from $\mathbb{D}$ for some $\delta > 0$ such that $f$ and
    $f_{\delta}$ is close
    \begin{equation}
        \norm{f - f_{\delta}} \leq \delta.
    \end{equation}
    We have
    \begin{align}
        \norm{\alpha (f_{n-1} - f) + (1-\alpha) (d - f_{\delta})}^2
        &= \alpha^2 \norm{f_{n-1} - f}^2 + (1-\alpha)^2 \norm{d - f_{\delta}}^2 \\
        &+ 2 \alpha (1-\alpha) \spr{f_{n-1}-f}{d - f_{\delta}}
    \end{align}

    We can then calculate the average of terms containing $d$
    \begin{align}
        &\sum_{j=1}^m a_j (1-\alpha)^2 \norm{d_j - f_{\delta}}^2 + 
        2 a_j \alpha (1-\alpha) \spr{f_{n-1}-f}{d_j - f_{\delta}} \\
        &= (1-\alpha)^2 \sum_{j=1}^m a_j \norm{d_j - f_{\delta}}^2 
        + 2 \alpha (1-\alpha) \spr{f_{n-1}-f}{\sum_{j=1}^m a_j d_j - f_{\delta}}\\
        &= (1-\alpha)^2 \sum_{j=1}^m a_j \norm{d_j - f_{\delta}}^2 + 0 \\
        &= (1-\alpha)^2 \sum_{j=1}^m a_j \norm{d_j}^2 - \norm{f_{\delta}}^2 \\
        &\leq (1-\alpha)^2 (C_{\mathbb{D}} - \norm{f_{\delta}}^2).
    \end{align}

    This implies that there must exist a $d \in \mathbb{D}$ such that
    \begin{align}
        \norm{\alpha (f_{n-1} - f) + (1-\alpha) (d - f_{\delta})}^2
        \leq \alpha^2 \norm{f_{n-1} - f}^2 + 
        (1-\alpha)^2 \norm{(d - f_{\delta})}^2 + \delta.
    \end{align}

    By triangle inequality and setting $\delta \to 0$, we have
    \begin{align}
        \inf_{g\in \mathbb{D}} \norm{\alpha f_{n-1} + (1-\alpha) d - f}^2
        &= \inf_{g\in \mathbb{D}} \norm{\alpha (f_{n-1} - f) + (1-\alpha) (d - f)}^2 \\
        &\leq \alpha^2 \norm{f_{n-1} - f}^2 + (1-\alpha)^2 C_f.
    \end{align}

    Then proof is complete when we set $\alpha = \frac{C_f}{C_f + \norm{f_{n-1}
    - f}^2}$.
\end{proof}

% in siegelHighOrderApproximationRates2021
% Using a classical probabilistic argument of Maurey [45], an approximation rate
% of O(n 2 ) can be obtained for the class B1(D) using non-linear dictionary
% expansions. Moreover, Jones [27] gave a constructive proof of this fact using
% the relaxed greedy algorithm and applied this result to shallow neural networks
% with a cosine activation function. Improvements upon this rate of dictionary
% approximation under an assumption about the behavior of the relaxed greedy
% algorithm appear in [31, 33]. These results yield exponential rates of
% convergence for individual functions in the convex hull of D (but not
% necessarily its closure), which are however not uniform over the class B1(D).
% Further, under compactness [29, 41] or smoothness [50] assumptions on the
% dictionary D improved rates can also be obtained, although for general
% dictionaries the Maurey-Jones rate is the best one can expect [30].

\section{Universal approximation theorem}
\label{sec:uat}

Neural networks, including those with a single hidden layer, such as 2NN, have
the ability to approximate any continuous function $f$ on a compact set up to an
arbitrary precision. This is known as universal approximation, which can be
achieved with mild restrictions on the activation functions $\sigma$. For
functions of $d$-variables, a linear combination of functions from the
dictionary
\begin{equation}
    \label{eq:uat_dict}
    \mathbb{D} = \{\sigma(b\tr x + c): b \in \R^d, c\in \R \}
\end{equation}
can be used to obtain a good approximation to $f$. These functions in
$\mathbb{D}$, also known as ridge functions or planar waves, are subject to
the requirements on $\sigma$.

However, the problem of complexity in approximation remains a challenge, as it
is unclear how many nodes $n$ in the hidden layer are required to achieve a
desired degree of approximation. While $n$ is allowed to grow uncontrolled, one
is unable to obtain quantitative information about the error rate. Despite this
limitation, this section will present the univariate approximation theorem based
on \cite{cybenkoApproximationSuperpositionsSigmoidal1989}, unless otherwise
specified.

% https://arxiv.org/abs/1601.00013
% use the notion of complexity problem

% We will state the the universal approximation theorem that the finite linear
% combinations of sigmoidal functions (sum of the functions of the form
% (\eqref{def:sigmoidal})) are dense in the space of continuous functions. This
% approximation result holds for any continuous sigmoidal functions.

% function on a compact set up to arbitrary precision [Cyb89, Fun89, HSW89,
% LLPS93]. modern mathematics of deep learning

\begin{definition}[sigmoidal function]\label{def:sigmoidal}
    A function $\sigma$ is \textbf{sigmoidal} if
    \begin{equation}
        \sigma(t) =
        \begin{cases}
            1 & \text{as} \quad t \to +\infty \\
            0 & \text{as} \quad t \to -\infty.
        \end{cases}
    \end{equation}
\end{definition}

\subsection*{Notations and setup}

Let $I_d = [0,1]^d$ denote the $d$-dimensional unit cube, and the space of
continuous functions over $I_d$ is denoted by $C(I_d)$. We denote the supremum
norm of a function $f \in C(I_d)$ by
\begin{equation}
    \label{def:sup_norm}
    \norm{f}_{\infty} = \sup_{x\in I_d} \abs{f(x)}.
\end{equation}
We use $M(I_d)$ to denote the space of finite, signed regular Borel measures,
i.e. $\mu(A) \in \R$ for all Borel sets $A \in I_d$ and $\mu(\emptyset)= 0$. We
refer readers to \cite{rudinFunctionalAnalysis1991,
rudinRealComplexAnalysis1987} for a detailed presentation of functional
construction used and we include some basic materials in Appendix
\ref{app:function_measure}.

Let $h(x)$ be a linear combinations of elements from the dictionary $\mathbb{D}$
\eqref{eq:uat_dict}

\begin{align}
    \label{eq:sum_sigma}
    h(x) \in 
    \Sigma_n(\mathbb{D}_{\sigma}) = \Sigma_n^{\sigma} := \Bigg\{
        \sum_{j=1}^n a_j \sigma(b_j\tr x + c_j), \quad
        a_j, c_j \in \R,  b_j \in \R^d
    \Bigg\}
\end{align}

where  $n < \infty$, and $\sigma$ is a univariate function from $\R$ to $\R$.
 
The main contribution of approximation theorem is the statement on the
conditions of $\sigma$ such that the above finite linear combination $h(x)$ is
dense in $C(I_d)$ with respect to the supremum norm. It should also be stressed
that there is no restriction for the number of combinations. Here the set
$\Sigma_n(\mathbb{D}_{\sigma})$ is the set $\mcal{M}(\sigma)$ in
\eqref{eq:set_of_all_functions_2nn} where $n = W$ and $b_0$ is omitted.

\begin{theorem}[Universal approximation theorem]
    \label{thm:uat}
    If $\sigma$ is sigmoidal as defined in Definition $\ref{def:sigmoidal}$,
    then any function $f \in C(I_d)$ be approximated uniformly well by a finite
    linear combination of ridge functions of the form \eqref{eq:sum_sigma}.

    In other words, for any function $f: I_d \to \R$ and any $\epsilon > 0$,
    there exists a $f_n \in \Sigma_n^{\sigma}$ such that
    \begin{align}
        \norm{f - f_n}_{\lp{\infty}(I_d)} < \epsilon, \quad n \in \Nat.
    \end{align}
\end{theorem}

\begin{theorem}
    Let $\sigma: \R \to \R$ be a continuous function. We consider the set
    \begin{equation}
        \Sigma_n^{\sigma} = \Bigg\{
            \sum_{j=1}^n \alpha_j \sigma(b\tr x + c), \quad
            b_j \in \R^d, \alpha_j, c_j \in \R, n\in\Nat
        \Bigg\}.
    \end{equation}

    For any function $f \in C(\R^d)$, and any compact set $U$ of $\R^d$, and any
    $\epsilon > 0$, there exists a $f_n \in \Sigma_n^{\sigma}$ such that 
    \begin{equation}
        \norm{f - f_n}_{\lp{\infty}(U)} < \epsilon
    \end{equation}
    if and only if $\sigma$ is not a polynomial. In other words,
    $\Sigma_n^{\sigma}$ is dense in the space $C(\R^d)$ in the topology of
    uniform convergence on compact sets.
\end{theorem}

\begin{remark}
    The requirements for the set $\Sigma_n^{\sigma}$ constructed using a
    function $\sigma$ is surprisingly simple and it is proven in Theorem 1
    \cite[p. 10]{leshnoMultilayerFeedforwardNetworks1993} that $\sigma$ simply
    need not to be a polynomial.
\end{remark}

The main structure of the proof for Theorem \ref{thm:uat} is as follows:
\begin{enumerate}
    \item Any finite sums of the form \eqref{eq:sum_sigma} with a
    \hyperref[def:dis_func]{discriminatory function} $\sigma$ are dense in
    $C(I_d)$ with respective to the supremum norm;
    \item Any bounded sigmoidal function is discriminatory.
\end{enumerate}

\begin{definition}[Discriminatory function]
    \label{def:dis_func}
    A function $\sigma$ is \textbf{discriminatory} if for every measure $\mu \in
    M(I_d)$,
    \begin{equation}
        \int_{I_d} \sigma(b\tr x + c) \,d\mu(x) = 0 \quad 
        \forall \,b \in \R^d \textnormal{ and } c \in \R
    \end{equation}
    implies $\mu = 0$.
\end{definition}

We continue to show that the linear span of any continuous discriminatory
functions are dense in the space of $(C(I_d), \norm{\cdot}_{\infty})$.

\begin{theorem}
    Let $\sigma: \R \to \R$ be a continuous
    \hyperref[def:dis_func]{discriminatory function}, the finite sums of the
    form \eqref{eq:sum_sigma} are dense in the space $(C(I_d),
    \norm{\cdot}_{\infty})$. In other words, for any $\epsilon > 0$ and any $f
    \in C(I_d)$, there exists a $n \in \Nat$ and a sum $h$ of the above form
    \eqref{eq:sum_sigma}, where
    \begin{equation}
        \norm{f - h}_{\lp{\infty}(I_d)} < \epsilon.
    \end{equation}
\end{theorem}

\begin{proof}
    Let $G := \spn(\{\sigma(b\tr x + c): b \in \R^d, c \in \R \})$ be the linear
    span for every $b\in\R^d, c\in\R$. $G$ clearly is a linear subspace of
    $C(I_d)$. We claim that the closure of $G$, $\closure{G}$, is all of
    $C(I_d)$.

    We continue the proof by contradiction. Assuming $\closure{G}$ is not
    $C(I_d)$, then there is a bounded linear functional $L$ on $C(I_d)$ such
    that $L \not\equiv 0$ on $C(I_d)$ and $L(G) = L(\closure{G}) = 0$ by the
    Hahn-Banach Theorem \ref{thm:hahn_banach_1}.

    By the \hyperref[thm:riesz_rep]{Riesz Representation Theorem}, there is a
    unique $\mu \in M(I_d)$ for this $L$ such that
    \begin{equation}
        L(f) = \int_{I_d} f(x) \,d\mu(x) \quad \forall f \in C(I_d)
    \end{equation}

    Since $L$ is identically zero on $G$, we must have for all $b$ and $c$ that
    \begin{equation}
        \int_{I_d} \sigma(b\tr x + c) \,d\mu(x) = 0
    \end{equation}

    However, the condition that $\sigma$ is discriminatory implies $\mu = 0$ and
    consequently $L = 0$. By \eqref{thm:hahn_banach_2}, subspace $G$ must be
    dense in $C(I_d)$.
\end{proof}

Now it remains to show that sigmoidal functions are discriminatory with the help
of Lemma \ref{lemma:zero_measure}.

\begin{lemma}~\cite{rudinFunctionalAnalysis1991}
    \label{lemma:zero_measure}
    if $\mu$ is a signed finite Borel measure on $\R^d$ such that the Fourier
    transform of $\mu$
    \begin{equation}
        \fourier{\mu}(u) = \int_{\R^d} e^{-iu\tr x} \,\mu(x) = 0,  
    \end{equation}
    for all $x\in\R^d$, then $\mu = 0$ for all measurable sets of $\R^d$.
\end{lemma}

\begin{lemma}
    Any bounded, measurable sigmoidal function is discriminatory.
\end{lemma}

\begin{proof}

    \textbf{Step 0} \textit{(Assume discriminatory and construct pointwise
    convergence function)}: Let $\sigma: \R \to \R$ be a sigmoidal function.
    Assume that the $\sigma$ is discriminatory with a measure $\mu \in M(I_d)$
    as in Definition \ref{def:dis_func} and the goal is to show that $\mu = 0$.

    Fix an arbitrary $b_0 \in \R^d\setminus \{0\}$ and define
    $\sigma_{\lambda}(x) := \sigma(\lambda (b_0\tr x + c) + \varphi)$. For
    any $c, \lambda, \varphi \in \R$, one can write
    \begin{equation}
        \sigma_{\lambda}(x)
        = \begin{cases}
            \to 1, \quad &\for b_0\tr x + c > 0 \quad \text{as}\, \lambda \to +\infty \\
            \to 0, \quad &\for b_0\tr x + c < 0 \quad \text{as}\, \lambda \to -\infty \\
            \sigma(\varphi), \quad &\for b_0\tr x + c = 0, \quad\forall \lambda \in \R
        \end{cases}.
    \end{equation}

    Therefore, the function $\sigma_{\lambda}$ converges pointwise to a function
    $\gamma(x): I_d \to \R$ as $\lambda \to + \infty$.

    \begin{equation}
        \gamma(x) = 
        \begin{cases}
            1,               \quad &\for b_0\tr x + c > 0 \\
            0,               \quad &\for b_0\tr x + c < 0 \\
            \sigma(\varphi), \quad &\for b_0\tr x + c = 0
        \end{cases}
    \end{equation}
    
    Let $\Pi_{b_0,c}$ denote the hyperplane and $H_{b_0, c}$ denote the
    half-space as: 
    \begin{align}
        \Pi_{b_0,c} &= \{x\in\R^d \mid b_0\tr x + c = 0\} \\
        H_{b_0, c}  &= \{x\in\R^d \mid b_0\tr x + c > 0\}
    \end{align}
    for all $c \in \R$.
    
    By the Lebesgue Convergence Theorem, we have
    \begin{align*}
        \sigma(\varphi) \mu(\Pi_{b_0, c}) + \mu (H_{b_0, c})
        &= \int_{I_d} \gamma(x) \,d\mu(x) \\
        &= \int_{I_d} \lim_{\lambda\to\infty} \sigma_{\lambda}(x)\,d\mu(x) \\
        &= \lim_{\lambda\to\infty} \int_{I_d} \sigma_{\lambda}(x)\,d\mu(x) = 0
    \end{align*}
    for all $\lambda, \varphi \in \R$.

    Thanks to the function $\sigma$ being sigmoidal, we have $\lim_{\varphi\to
    +\infty} \sigma(\varphi)= 1$ and $\lim_{\varphi\to -\infty}
    \sigma(\varphi)=0$ and consequently it is easy to see
    \begin{equation}
        \label{eq:disc_1}
        \mu(\Pi_{b_0, c}) = 0 \quad \text{and} \quad \mu (H_{b_0, c}) = 0 
        \quad \forall c\in\R.
    \end{equation}

    This in turn implies $\mu(I_d) = 0$ as $c$ can be chosen arbitrarily large.
    
    We would like to show that the measure of all half-planes being zero implies
    that the measure $\mu$ must be zero. If $\mu$ is a positive Borel measure,
    this would be trivial by \eqref{eq:disc_1} but $\mu$ here is a signed
    measure.
    
    \textbf{Step 1} \textit{(Construct a signed measure)}: Let $\phi$ be a
    finite signed, Borel measure on $\R$
    \begin{equation}
        \label{eq:disc_2}
        \phi(A) = \mu(\{x \in I_d: b_o\tr x \in A\}) \quad \forall A \subseteq \R.
    \end{equation}

    By construction, we have
    \begin{align}
        \forall a < b \in \R, \quad \phi((a,b)) 
        &= \phi((a,\infty)) - \phi([b, \infty)) \\
        &= \mu(\{x \in I_d: b_o\tr x > a\}) \\
        &\quad- \Big(
            \mu(\{x \in I_d: b_o\tr x > b\})
            + \mu(\{x \in I_d: b_o\tr x = b\})
        \Big) \\
        &= \mu(H_{b_0, -a}) - (\mu(H_{b_0, -b}) + \mu(\Pi_{b_0, -b})) \\
        &= 0 - 0 = 0.
    \end{align}
    Therefore $\phi(A) = 0$ for all Borel sets $A \subseteq \R$.

    % Show a bounded linear functional is zero on such measure
    \textbf{Step 2} \textit{(Define a linear functional $L$)}: Let
    $\lp{\infty}(\R)$ denote the space of all measurable bounded functions $f:
    \R \to \R$. For a function $h \in \lp{\infty}(\R)$, we define a functional
    $L: \lp{\infty}(\R) \to \R$:
    \begin{equation}
        L(h) = \int_{I_d} h(b\tr x)\,d\mu(x).
    \end{equation}

    $L$ is linear because for all $\alpha, \beta \in \R, \text{ and } g, h \in
    \lp{\infty}(\R)$
    \begin{align*}
        L(\alpha g + \beta h)
        &= \int_{I_d} (\alpha g + \beta h) (b_0\tr x) \,d\mu(x) \\
        &= \int_{I_d} (\alpha g(b_0\tr x) + \beta h (b_0\tr x)) \, d\mu(x) \\
        &= \alpha \int_{I_d} g(b_0\tr x) \,d\mu(x) 
            + \beta \int_{I_d} h(b_0\tr x) \,d\mu(x) \\
        &= \alpha F(g) + \beta L(h)
    \end{align*}

    Now we look at the indicator function for all Borel sets of $\R$
    \begin{equation}
        \indicator{A}(x) =
        \begin{cases}
            1, \quad \text{ if } x \in A, \\
            0, \quad \text{ if } x \not\in A.
        \end{cases}
    \end{equation}

    Combining $\indicator{A} \in \lp{\infty}(\R)$ and \eqref{eq:disc_2}, we
    have 

    \begin{equation}
        L(\indicator{A}) = \int_{I_d} \indicator{A}(b_0\tr x)\,d\mu(x)
        = \mu(\{ x\in I_d: b_0\tr x\in A \}) = \phi(A) = 0
    \end{equation}
    for all Borel sets $A \subseteq \R$.

    Since $L$ is a linear functional, the finite sum of functions of the form
    (simple functions):
    \begin{equation}
        \label{eq:simple_function}
        s_n(x) = \sum_{j=1}^{n} a_j \indicator{A_j}(x)
    \end{equation}
    is zero for all $n\in\Nat$ where $a_j \in \R$, and ${A_j} \subseteq \R$ are
    measurable and pairwise disjoint sets of $\R$. Since the simple functions
    \eqref{eq:simple_function} are dense in $\lp{\infty}(\R)$ \citep[Proposition
    6.7]{alma991031140799705251} , then for a function $h \in \lp{\infty}(\R)$,
    there exists a sequence of functions $s_n$ converge pointwise to $h$ for all
    $n \in \Nat$ and $x \in \R$
    \begin{align}
        L(h) &= \int_{I_d} h(b_0\tr x) \,d\mu(x) 
             = \int_{I_d} \lim_{n\to\infty} s_n(b_0\tr x) \,d\mu(x) \\
             &= \lim_{n\to\infty} \int_{I_d} s_n(b_0\tr x) \,d\mu(x)
             = \lim_{n\to\infty} L(s_n)
             = 0
    \end{align}
    where the limit in the integral is moved outside the integral by the
    Lebesgue Convergence Theorem.

    \textbf{Step 3} \textit{(Tidy up)}: sine and cosine functions are in
    $\lp{\infty}(\R)$, which implies $L(\cos) = L(\sin) = 0$ for all $b_0$. For a
    bounded measurable function $h'(x) = e^{ib_0\tr x} = \cos(b_0\tr x) + i
    \sin(b_0\tr x)$, we have
    \begin{equation}
        \label{eq:bounded_functional}
        L(h') = \int_{I_d} h'(b_0\tr x) \,d\mu(x) 
              = \int_{I_d} \cos(b_0\tr x) + i \sin(b_0\tr x) \,d\mu(x)
              = 0.
    \end{equation}

    It is easy to verify that $\mu = 0$ on $\R^d \setminus \{0\}$ as the Fourier
    transform of $\mu$ is zero from Lemma \ref{lemma:zero_measure}. Combining
    with the fact that $\mu(I_d) = 0$, \eqref{eq:bounded_functional} establishes
    that $\sigma$ is discriminatory.

\end{proof}

% Let $f_m(\mathbf{x}, \Theta)$ be the parameterized family of 2NN defined above
% of $m$ nodes that which map input vector $\mathbf{x}$ of dimension $d$.

% \begin{equation}
%     f_m(\mathbf{x}, \Theta) = \sum_{i=1}^m a_i \sigma(\mathbf{b_i}^T\mathbf{x} + c_i)
% \end{equation}

% where $\Theta = (a_1, \mathbf{b}_1, c_1, \dots, a_m, \mathbf{b}_m, c_m)$ denotes
% all the parameters (the total number of parameters is $(d+2)m + 1$). We will
% consider the hypothesis spaces where the activation functions $\sigma$ is ReLU
% \footnote{\TONOTE{The choice of activation function is important for
%         infinite-width Barron spaces}}.

%  homogeneity

\subsection{Application to the classification problem}

This section will explain the implications of Theorem \ref{thm:uat} for
classification problems. It should be noted that the decision function defined
below is not continuous on $I_d$. We would like to check whether such
classification problem can also be well understood as the regression problem.

\begin{definition}[Decision function]
    Let $\{P_1, \dots, P_k\}$ be a partition of $I_d$ where each partition $P_j$
    are pairwise disjoint nonempty Borel sets, i.e. $P_j \not= \emptyset$ and
    $\bigcup_{j=1}^k P_j = I_d$. $f$ is a decision function for $I_d$ of the
    form
    \begin{equation}
        f: I_d \to \{1, \dots, k\}
    \end{equation}
    where $f(x) = j$ for $x \in P_j$. 
\end{definition}

\begin{theorem}
    \label{thm:uat_clas}
    Let $\sigma$ be a continuous sigmoidal function and $f$ be the decision
    function for any finite, measurable partition of $I_d$. Let $\phi$ be a
    Borel measure on $I_d$ and $\mu(I_d) = 1$. Then for any $\epsilon>0$, there
    exists a finite sum of the form
    \begin{equation}
        h(x) = \sum_{j=1}^n a_j \sigma(b_j\tr x + c_j)
    \end{equation}
    where $a_j, c_j \in \R, b_j \in \R^d$ and a set $D\subset I_d$, such that
    the measure of the set $D$, $\mu(D) \geq 1 - \epsilon$ and
    \begin{equation}
        \sup_{x\in D}\abs{f(x) - h(x)} < \epsilon
    \end{equation}
\end{theorem}

Theorem \ref{thm:uat_clas} is a anoloy of the UAT and the proof is
straightforward using Lusin's Theorem \eqref{thm:lusin}.

\begin{proof}
    By Lusin's Theorem \eqref{thm:lusin}, there exists a continuous function $g
    \in C(I_d)$ such that $\mu(\{x\in I_d \mid f(x) \not= g(x)\}) < \epsilon$.
    Now we have a continuous $g$ and we are able to a find a sum of the form
    above satisfying $\abs{h(x) - g(x)} < \epsilon$ by Theorem \eqref{thm:uat}
    for all $x\in I_d$. Let set $D = \{x\in I_d \mid f(x) = g(x)\}$ and we have
    $\mu(D) \geq 1 - \epsilon$. Then for $x\in D$, we have
    \begin{equation}
        \sup_{x\in D}\abs{h(x)-f(x)} = \sup_{x\in D}\abs{h(x) - g(x)} < \epsilon 
        \quad \forall x\in D.
    \end{equation}
\end{proof}

The above result shows that the total measure of the misclassified points can be
made arbitrarily small when $n$ is allowed to grow.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "MasterThesisSfS"
%%% End: 
