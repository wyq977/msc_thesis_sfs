@misc{abbePolytimeUniversalityLimitations2020,
  title = {Poly-Time Universality and Limitations of Deep Learning},
  author = {Abbe, Emmanuel and Sandon, Colin},
  year = {2020},
  month = jan,
  number = {arXiv:2001.02992},
  eprint = {2001.02992},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.02992},
  abstract = {The goal of this paper is to characterize function distributions that deep learning can or cannot learn in poly-time. A universality result is proved for SGD-based deep learning and a non-universality result is proved for GD-based deep learning; this also gives a separation between SGD-based deep learning and statistical query algorithms: (1) \{\textbackslash it Deep learning with SGD is efficiently universal.\} Any function distribution that can be learned from samples in poly-time can also be learned by a poly-size neural net trained with SGD on a poly-time initialization with poly-steps, poly-rate and possibly poly-noise. Therefore deep learning provides a universal learning paradigm: it was known that the approximation and estimation errors could be controlled with poly-size neural nets, using ERM that is NP-hard; this new result shows that the optimization error can also be controlled with SGD in poly-time. The picture changes for GD with large enough batches: (2) \{\textbackslash it Result (1) does not hold for GD:\} Neural nets of poly-size trained with GD (full gradients or large enough batches) on any initialization with poly-steps, poly-range and at least poly-noise cannot learn any function distribution that has super-polynomial \{\textbackslash it cross-predictability,\} where the cross-predictability gives a measure of ``average'' function correlation -- relations and distinctions to the statistical dimension are discussed. In particular, GD with these constraints can learn efficiently monomials of degree \$k\$ if and only if \$k\$ is constant. Thus (1) and (2) point to an interesting contrast: SGD is universal even with some poly-noise while full GD or SQ algorithms are not (e.g., parities).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/TFP9X4RR/Abbe and Sandon - 2020 - Poly-time universality and limitations of deep lea.pdf;/home/wyq977/Zotero/storage/3IJP73A2/2001.html}
}

@misc{andrealorkeCybenkoTheoremCapability,
  title = {Cybenko's {{Theorem}} and the Capability of a Neural Network as Function Approximator},
  author = {{Andrea L\"orke} and {Fabian Schneider} and {Johannes Heck} and {Patrick Nitter}},
  file = {/home/wyq977/Zotero/storage/KRD4JLDJ/Andrea Lörke et al. - Cybenko’s Theorem and the capability of a neural n.pdf}
}

@article{bachBreakingCurseDimensionality2016,
  title = {Breaking the {{Curse}} of {{Dimensionality}} with {{Convex Neural Networks}}},
  author = {Bach, Francis},
  year = {2016},
  month = oct,
  journal = {arXiv:1412.8690 [cs, math, stat]},
  eprint = {1412.8690},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We consider neural networks with a single hidden layer and non-decreasing homogeneous activa-tion functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of ob-servations. In addition, we provide a simple geometric interpretation to the non-convex problem of addition of a new unit, which is the core potentially hard computational element in the framework of learning from continuously many basis functions. We provide simple conditions for convex relaxations to achieve the same generalization error bounds, even when constant-factor approxi-mations cannot be found (e.g., because it is NP-hard such as for the zero-homogeneous activation function). We were not able to find strong enough convex relaxations and leave open the existence or non-existence of polynomial-time algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Mathematics - Statistics Theory},
  file = {/home/wyq977/Zotero/storage/UG8ZUQH7/Bach - 2016 - Breaking the Curse of Dimensionality with Convex N.pdf;/home/wyq977/Zotero/storage/JYLYQE6U/1412.html}
}

@article{banerjeeStatistics612Lp,
  title = {Statistics 612: {{Lp}} Spaces, Metrics on Spaces of Probabilites, and Connections to Estimation},
  author = {Banerjee, Moulinath},
  pages = {11},
  langid = {english},
  file = {/home/wyq977/Zotero/storage/LGPVNTG8/Banerjee - Statistics 612 Lp spaces, metrics on spaces of pr.pdf}
}

@article{barronApproximationEstimationBounds1994,
  title = {Approximation and Estimation Bounds for Artificial Neural Networks},
  author = {Barron, Andrew R.},
  year = {1994},
  month = jan,
  journal = {Machine Learning},
  volume = {14},
  number = {1},
  pages = {115--133},
  issn = {1573-0565},
  doi = {10.1007/BF00993164},
  abstract = {For a common class of artificial neural networks, the mean integrated squared error between the estimated network and a target functionf is shown to be bounded by\$\$O\textbackslash left( \{\textbackslash frac\{\{\textbackslash mathop c\textbackslash nolimits\_f\^2 \}\}\{n\}\} \textbackslash right) + O\textbackslash left( \{\textbackslash frac\{\{nd\}\}\{N\}\textbackslash log  N\} \textbackslash right)\$\$wheren is the number of nodes,d is the input dimension of the function,N is the number of training observations, andCfis the first absolute moment of the Fourier magnitude distribution off. The two contributions to this total risk are the approximation error and the estimation error. Approximation error refers to the distance between the target function and the closest neural network function of a given architecture and estimation error refers to the distance between this ideal network function and an estimated network function. Withn {$\sim$} Cf(N/(d logN))1/2 nodes, the order of the bound on the mean integrated squared error is optimized to beO(Cf((d/N) logN)1/2). The bound demonstrates surprisingly favorable properties of network estimation compared to traditional series and nonparametric curve estimation techniques in the case thatd is moderately large. Similar bounds are obtained when the number of nodesn is not preselected as a function ofCf(which is generally not knowna priori), but rather the number of nodes is optimized from the observed data by the use of a complexity regularization or minimum description length criterion. The analysis involves Fourier techniques for the approximation error, metric entropy considerations for the estimation error, and a calculation of the index of resolvability of minimum complexity estimation of the family of networks.},
  langid = {english},
  file = {/home/wyq977/Zotero/storage/CS2MZZXX/Barron - 1994 - Approximation and estimation bounds for artificial.pdf}
}

@article{barronApproximationEstimationHighDimensional2018,
  title = {Approximation and {{Estimation}} for {{High-Dimensional Deep Learning Networks}}},
  author = {Barron, Andrew R. and Klusowski, Jason M.},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.03090 [cs, stat]},
  eprint = {1809.03090},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {It has been experimentally observed in recent years that multi-layer artificial neural networks have a surprising ability to generalize, even when trained with far more parameters than observations. Is there a theoretical basis for this? The best available bounds on their metric entropy and associated complexity measures are essentially linear in the number of parameters, which is inadequate to explain this phenomenon. Here we examine the statistical risk (mean squared predictive error) of multi-layer networks with \$\textbackslash ell\^1\$-type controls on their parameters and with ramp activation functions (also called lower-rectified linear units). In this setting, the risk is shown to be upper bounded by \$[(L\^3 \textbackslash log d)/n]\^\{1/2\}\$, where \$d\$ is the input dimension to each layer, \$L\$ is the number of layers, and \$n\$ is the sample size. In this way, the input dimension can be much larger than the sample size and the estimator can still be accurate, provided the target function has such \$\textbackslash ell\^1\$ controls and that the sample size is at least moderately large compared to \$L\^3\textbackslash log d\$. The heart of the analysis is the development of a sampling strategy that demonstrates the accuracy of a sparse covering of deep ramp networks. Lower bounds show that the identified risk is close to being optimal.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/LQAQTE5L/Barron and Klusowski - 2018 - Approximation and Estimation for High-Dimensional .pdf;/home/wyq977/Zotero/storage/2DXPA558/1809.html}
}

@inproceedings{barronNeuralNetApproximation1992,
  title = {Neural Net Approximation},
  booktitle = {Proc. 7th {{Yale}} Workshop on Adaptive and Learning Systems},
  author = {Barron, Andrew R.},
  year = {1992},
  volume = {1},
  pages = {69--72},
  file = {/home/wyq977/Zotero/storage/FS6I4UR9/neural net approximation.pdf}
}

@article{barronUniversalApproximationBounds1993,
  title = {Universal Approximation Bounds for Superpositions of a Sigmoidal Function},
  author = {Barron, A. R.},
  year = {1993},
  month = may,
  journal = {IEEE Transactions on Information Theory},
  volume = {39},
  number = {3},
  pages = {930--945},
  issn = {1557-9654},
  doi = {10.1109/18.256500},
  abstract = {Approximation properties of a class of artificial neural networks are established. It is shown that feedforward networks with one layer of sigmoidal nonlinearities achieve integrated squared error of order O(1/n), where n is the number of nodes. The approximated function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. The nonlinear parameters associated with the sigmoidal nodes, as well as the parameters of linear combination, are adjusted in the approximation. In contrast, it is shown that for series expansions with n terms, in which only the parameters of linear combination are adjusted, the integrated squared approximation error cannot be made smaller than order 1/n/sup 2/d/ uniformly for functions satisfying the same smoothness assumption, where d is the dimension of the input to the function. For the class of functions examined, the approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings.{$<>$}},
  keywords = {Approximation error,Artificial neural networks,Feedforward neural networks,Feeds,Fourier transforms,Information theory,Linear approximation,Neural networks,Statistical distributions,Statistics},
  file = {/home/wyq977/Zotero/storage/3E86QZQI/Barron - 1993 - Universal approximation bounds for superpositions .pdf;/home/wyq977/Zotero/storage/8TLMAXBS/256500.html}
}

@article{bartlettLocalRademacherComplexities2005,
  title = {Local {{Rademacher}} Complexities},
  author = {Bartlett, Peter L. and Bousquet, Olivier and Mendelson, Shahar},
  year = {2005},
  month = aug,
  journal = {The Annals of Statistics},
  volume = {33},
  number = {4},
  eprint = {math/0508275},
  eprinttype = {arxiv},
  issn = {0090-5364},
  doi = {10.1214/009053605000000282},
  abstract = {We propose new bounds on the error of learning algorithms in terms of a data-dependent notion of complexity. The estimates we establish give optimal rates and are based on a local and empirical version of Rademacher averages, in the sense that the Rademacher averages are computed from the data, on a subset of functions with small empirical error. We present some applications to classification and prediction with convex function classes, and with kernel classes in particular.},
  archiveprefix = {arXiv},
  keywords = {62G08; 68Q32 (Primary),Mathematics - Statistics Theory},
  file = {/home/wyq977/Zotero/storage/EZ98ZI5Q/Bartlett et al. - 2005 - Local Rademacher complexities.pdf;/home/wyq977/Zotero/storage/BUSDCDE9/0508275.html}
}

@misc{berlindRademacherComplexity,
  title = {Rademacher {{Complexity}}},
  author = {Berlind, Chris},
  file = {/home/wyq977/Zotero/storage/V5J72EJI/Berlind - Rademacher Complexity.pdf}
}

@misc{bernerModernMathematicsDeep2021,
  title = {The {{Modern Mathematics}} of {{Deep Learning}}},
  author = {Berner, Julius and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
  year = {2021},
  month = may,
  number = {arXiv:2105.04026},
  eprint = {2105.04026},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.04026},
  abstract = {We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/FT3STUMW/Berner et al. - 2021 - The Modern Mathematics of Deep Learning.pdf}
}

@misc{carageaNeuralNetworkApproximation2022,
  title = {Neural Network Approximation and Estimation of Classifiers with Classification Boundary in a {{Barron}} Class},
  author = {Caragea, Andrei and Petersen, Philipp and Voigtlaender, Felix},
  year = {2022},
  month = mar,
  number = {arXiv:2011.09363},
  eprint = {2011.09363},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  abstract = {We prove bounds for the approximation and estimation of certain binary classification functions using ReLU neural networks. Our estimation bounds provide a priori performance guarantees for empirical risk minimization using networks of a suitable size, depending on the number of training samples available. The obtained approximation and estimation rates are independent of the dimension of the input, showing that the curse of dimensionality can be overcome in this setting; in fact, the input dimension only enters in the form of a polynomial factor. Regarding the regularity of the target classification function, we assume the interfaces between the different classes to be locally of Barron-type. We complement our results by studying the relations between various Barron-type spaces that have been proposed in the literature. These spaces differ substantially more from each other than the current literature suggests.},
  archiveprefix = {arXiv},
  keywords = {68T07; 41A25; 41A46; 42B35; 46E15,Mathematics - Functional Analysis,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/2UZ24WB4/Caragea et al. - 2022 - Neural network approximation and estimation of cla.pdf}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  year = {1989},
  month = dec,
  journal = {Mathematics of Control, Signals and Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  langid = {english},
  keywords = {Approximation,Completeness,Neural networks},
  file = {/home/wyq977/Zotero/storage/BBT8674Z/Cybenko - 1989 - Approximation by superpositions of a sigmoidal fun.pdf}
}

@article{donohoHighDimensionalDataAnalysis,
  title = {High-{{Dimensional Data Analysis}}: {{The Curses}} and {{Blessings}} of {{Dimensionality}}},
  author = {Donoho, David L},
  pages = {33},
  abstract = {The coming century is surely the century of data. A combination of blind faith and serious purpose makes our society invest massively in the collection and processing of data of all kinds, on scales unimaginable until recently. Hyperspectral Imagery, Internet Portals, Financial tick-by-tick data, and DNA Microarrays are just a few of the betterknown sources, feeding data in torrential streams into scientific and business databases worldwide.},
  langid = {english},
  file = {/home/wyq977/Zotero/storage/D8EA2V7Z/Donoho - High-Dimensional Data Analysis The Curses and Ble.pdf}
}

@inproceedings{donohoHighdimensionalDataAnalysis2000,
  title = {High-Dimensional Data Analysis: {{The}} Curses and Blessings of Dimensionality},
  shorttitle = {High-Dimensional Data Analysis},
  booktitle = {Ams {{Conference}} on {{Math Challenges}} of the 21st {{Century}}},
  author = {Donoho, David L.},
  year = {2000},
  abstract = {The coming century is surely the century of data. A combination of blind faith and serious purpose makes our society invest massively in the collection and processing of data of all kinds, on scales unimaginable until recently. Hyperspectral Imagery, Internet Portals, Financial tick-by-tick data, and DNA Microarrays are just a few of the betterknown sources, feeding data in torrential streams into scientific and business databases worldwide. In traditional statistical data analysis, we think of observations of instances of particular phenomena (e.g. instance {$\leftrightarrow$} human being), these observations being a vector of values we measured on several variables (e.g. blood pressure, weight, height,...). In traditional statistical methodology, we assumed many observations and a few, wellchosen variables. The trend today is towards more observations but even more so, to radically larger numbers of variables \textendash{} voracious, automatic, systematic collection of hyper-informative detail about each observed instance. We are seeing examples where the observations gathered on individual instances are curves, or spectra, or images, or},
  file = {/home/wyq977/Zotero/storage/2ZMTH9W7/Donoho - 2000 - High-dimensional data analysis The curses and ble.pdf;/home/wyq977/Zotero/storage/WNWD6BNM/summary.html}
}

@article{eBanachSpacesAssociated2020,
  title = {On the {{Banach}} Spaces Associated with Multi-Layer {{ReLU}} Networks: {{Function}} Representation, Approximation Theory and Gradient Descent Dynamics},
  shorttitle = {On the {{Banach}} Spaces Associated with Multi-Layer {{ReLU}} Networks},
  author = {E, Weinan and Wojtowytsch, Stephan},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.15623 [cs, math, stat]},
  eprint = {2007.15623},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We develop Banach spaces for ReLU neural networks of finite depth \$L\$ and infinite width. The spaces contain all finite fully connected \$L\$-layer networks and their \$L\^2\$-limiting objects under bounds on the natural path-norm. Under this norm, the unit ball in the space for \$L\$-layer networks has low Rademacher complexity and thus favorable generalization properties. Functions in these spaces can be approximated by multi-layer neural networks with dimension-independent convergence rates. The key to this work is a new way of representing functions in some form of expectations, motivated by multi-layer neural networks. This representation allows us to define a new class of continuous models for machine learning. We show that the gradient flow defined this way is the natural continuous analog of the gradient descent dynamics for the associated multi-layer neural networks. We show that the path-norm increases at most polynomially under this continuous gradient flow dynamics.},
  archiveprefix = {arXiv},
  keywords = {68T07; 46E15; 26B35; 26B40,Computer Science - Machine Learning,Mathematics - Functional Analysis,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/U5TPALKY/E and Wojtowytsch - 2020 - On the Banach spaces associated with multi-layer R.pdf;/home/wyq977/Zotero/storage/TZFI8HDC/2007.html}
}

@article{eBarronSpaceFlowinduced2021,
  title = {The {{Barron Space}} and the {{Flow-induced Function Spaces}} for {{Neural Network Models}}},
  author = {E, Weinan and Ma, Chao and Wu, Lei},
  year = {2021},
  month = mar,
  journal = {arXiv:1906.08039 [cs, math, stat]},
  eprint = {1906.08039},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {One of the key issues in the analysis of machine learning models is to identify the appropriate function space and norm for the model. This is the set of functions endowed with a quantity which can control the approximation and estimation errors by a particular machine learning model. In this paper, we address this issue for two representative neural network models: the two-layer networks and the residual neural networks. We define the Barron space and show that it is the right space for two-layer neural network models in the sense that optimal direct and inverse approximation theorems hold for functions in the Barron space. For residual neural network models, we construct the so-called flow-induced function space, and prove direct and inverse approximation theorems for this space. In addition, we show that the Rademacher complexity for bounded sets under these norms has the optimal upper bounds.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Probability,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/QHIFWQDB/E et al. - 2021 - The Barron Space and the Flow-induced Function Spa.pdf}
}

@article{eComparativeAnalysisOptimization2020,
  title = {A {{Comparative Analysis}} of the {{Optimization}} and {{Generalization Property}} of {{Two-layer Neural Network}} and {{Random Feature Models Under Gradient Descent Dynamics}}},
  author = {E, Weinan and Ma, Chao and Wu, Lei},
  year = {2020},
  month = jul,
  journal = {Science China Mathematics},
  volume = {63},
  number = {7},
  eprint = {1904.04326},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  pages = {1235--1258},
  issn = {1674-7283, 1869-1862},
  doi = {10.1007/s11425-019-1628-5},
  abstract = {A fairly comprehensive analysis is presented for the gradient descent dynamics for training two-layer neural network models in the situation when the parameters in both layers are updated. General initialization schemes as well as general regimes for the network width and training data size are considered. In the over-parametrized regime, it is shown that gradient descent dynamics can achieve zero training loss exponentially fast regardless of the quality of the labels. In addition, it is proved that throughout the training process the functions represented by the neural network model are uniformly close to that of a kernel method. For general values of the network width and training data size, sharp estimates of the generalization error is established for target functions in the appropriate reproducing kernel Hilbert space.},
  archiveprefix = {arXiv},
  keywords = {41A99; 49M99,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/YYTDVG9F/E et al. - 2020 - A Comparative Analysis of the Optimization and Gen.pdf;/home/wyq977/Zotero/storage/ZEDVPGKA/1904.html}
}

@article{eMathematicalUnderstandingNeural2020,
  title = {Towards a {{Mathematical Understanding}} of {{Neural Network-Based Machine Learning}}: What We Know and What We Don't},
  shorttitle = {Towards a {{Mathematical Understanding}} of {{Neural Network-Based Machine Learning}}},
  author = {E, Weinan and Ma, Chao and Wojtowytsch, Stephan and Wu, Lei},
  year = {2020},
  month = dec,
  journal = {arXiv:2009.10713 [cs, math, stat]},
  eprint = {2009.10713},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {The purpose of this article is to review the achievements made in the last few years towards the understanding of the reasons behind the success and subtleties of neural network-based machine learning. In the tradition of good old applied mathematics, we will not only give attention to rigorous mathematical results, but also the insight we have gained from careful numerical experiments as well as the analysis of simplified models. Along the way, we also list the open problems which we believe to be the most important topics for further study. This is not a complete overview over this quickly moving field, but we hope to provide a perspective which may be helpful especially to new researchers in the area.},
  archiveprefix = {arXiv},
  keywords = {68T07 (primary); 26B40; 41A30; 35Q68,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/JVSZJFM4/E et al. - 2020 - Towards a Mathematical Understanding of Neural Net.pdf;/home/wyq977/Zotero/storage/NQP5W5AR/2009.html}
}

@article{eObservationsPartialDifferential2020,
  title = {Some Observations on Partial Differential Equations in {{Barron}} and Multi-Layer Spaces},
  author = {E, Weinan and Wojtowytsch, Stephan},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.01484 [cs, math]},
  eprint = {2012.01484},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {We use explicit representation formulas to show that solutions to certain partial differential equations lie in Barron spaces or multilayer spaces if the PDE data lie in such function spaces. Consequently, these solutions can be represented efficiently using artificial neural networks, even in high dimension. Conversely, we present examples in which the solution fails to lie in the function space associated to a neural network under consideration.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T07; 35C15; 65M80,Computer Science - Machine Learning,Mathematics - Analysis of PDEs},
  file = {/home/wyq977/Zotero/storage/LUFYJVBI/E and Wojtowytsch - 2020 - Some observations on partial differential equation.pdf}
}

@article{ePrioriEstimatesPopulation2019,
  title = {A {{Priori Estimates}} of the {{Population Risk}} for {{Two-layer Neural Networks}}},
  author = {E, Weinan and Ma, Chao and Wu, Lei},
  year = {2019},
  journal = {Communications in Mathematical Sciences},
  volume = {17},
  number = {5},
  eprint = {1810.06397},
  eprinttype = {arxiv},
  pages = {1407--1425},
  issn = {15396746, 19450796},
  doi = {10.4310/CMS.2019.v17.n5.a11},
  abstract = {New estimates for the population risk are established for two-layer neural networks. These estimates are nearly optimal in the sense that the error rates scale in the same way as the Monte Carlo error rates. They are equally effective in the over-parametrized regime when the network size is much larger than the size of the dataset. These new estimates are a priori in nature in the sense that the bounds depend only on some norms of the underlying functions to be fitted, not the parameters in the model, in contrast with most existing results which are a posteriori in nature. Using these a priori estimates, we provide a perspective for understanding why two-layer neural networks perform better than the related kernel methods.},
  archiveprefix = {arXiv},
  keywords = {41A46; 41A63; 62J02; 65D05,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/2SMQ4IAB/E et al. - 2019 - A Priori Estimates of the Population Risk for Two-.pdf;/home/wyq977/Zotero/storage/89W733GF/1810.html}
}

@article{ePrioriEstimatesPopulation2019a,
  title = {A {{Priori Estimates}} of the {{Population Risk}} for {{Residual Networks}}},
  author = {E, Weinan and Ma, Chao and Wang, Qingcan},
  year = {2019},
  month = may,
  journal = {arXiv:1903.02154 [cs, stat]},
  eprint = {1903.02154},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Optimal a priori estimates are derived for the population risk, also known as the generalization error, of a regularized residual network model. An important part of the regularized model is the usage of a new path norm, called the weighted path norm, as the regularization term. The weighted path norm treats the skip connections and the nonlinearities differently so that paths with more nonlinearities are regularized by larger weights. The error estimates are a priori in the sense that the estimates depend only on the target function, not on the parameters obtained in the training process. The estimates are optimal, in a high dimensional setting, in the sense that both the bound for the approximation and estimation errors are comparable to the Monte Carlo error rates. A crucial step in the proof is to establish an optimal bound for the Rademacher complexity of the residual networks. Comparisons are made with existing norm-based generalization error bounds.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/ZFYMGJVC/E et al. - 2019 - A Priori Estimates of the Population Risk for Resi.pdf;/home/wyq977/Zotero/storage/WJ3ZNUL8/1903.html}
}

@article{eRepresentationFormulasPointwise2020,
  title = {Representation Formulas and Pointwise Properties for {{Barron}} Functions},
  author = {E, Weinan and Wojtowytsch, Stephan},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.05982 [cs, math, stat]},
  eprint = {2006.05982},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We study the natural function space for infinitely wide two-layer neural networks and establish different representation formulae. In two cases, we describe the space explicitly up to isomorphism. Using a convenient representation, we study the pointwise properties of two-layer networks and show that functions whose singular set is fractal or curved (for example distance functions from smooth submanifolds) cannot be represented by infinitely wide two-layer networks with finite path-norm.},
  archiveprefix = {arXiv},
  keywords = {68T07; 46E15; 26B35; 26B40,Computer Science - Machine Learning,Mathematics - Analysis of PDEs,Mathematics - Functional Analysis,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/3ACSK94A/E and Wojtowytsch - 2020 - Representation formulas and pointwise properties f.pdf;/home/wyq977/Zotero/storage/FYRRN8LS/2006.html}
}

@article{halabiCombinatorialPenaltiesWhich2018,
  title = {Combinatorial {{Penalties}}: {{Which}} Structures Are Preserved by Convex Relaxations?},
  shorttitle = {Combinatorial {{Penalties}}},
  author = {Halabi, Marwa El and Bach, Francis and Cevher, Volkan},
  year = {2018},
  month = mar,
  journal = {arXiv:1710.06273 [cs, stat]},
  eprint = {1710.06273},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We consider the homogeneous and the nonhomogeneous convex relaxations for combinatorial penalty functions defined on support sets. Our study identifies key differences in the tightness of the resulting relaxations through the notion of the lower combinatorial envelope of a setfunction along with new necessary conditions for support identification. We then propose a general adaptive estimator for convex monotone regularizers, and derive new sufficient conditions for support recovery in the asymptotic setting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/AYM3T5MW/Halabi et al. - 2018 - Combinatorial Penalties Which structures are pres.pdf}
}

@article{hanClassDimensionalityfreeMetrics2021,
  title = {A {{Class}} of {{Dimensionality-free Metrics}} for the {{Convergence}} of {{Empirical Measures}}},
  author = {Han, Jiequn and Hu, Ruimeng and Long, Jihao},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.12036 [cs, math, stat]},
  eprint = {2104.12036},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {This paper concerns the convergence of empirical measures in high dimensions. We propose a new class of metrics and show that under such metrics, the convergence is free of the curse of dimensionality (CoD). Such a feature is critical for high-dimensional analysis and stands in contrast to classical metrics (\{\textbackslash it e.g.\}, the Wasserstein distance). The proposed metrics originate from the maximum mean discrepancy, which we generalize by proposing specific criteria for selecting test function spaces to guarantee the property of being free of CoD. Therefore, we call this class of metrics the generalized maximum mean discrepancy (GMMD). Examples of the selected test function spaces include the reproducing kernel Hilbert space, Barron space, and flow-induced function spaces. Three applications of the proposed metrics are presented: 1. The convergence of empirical measure in the case of random variables; 2. The convergence of \$n\$-particle system to the solution to McKean-Vlasov stochastic differential equation; 3. The construction of an \$\textbackslash varepsilon\$-Nash equilibrium for a homogeneous \$n\$-player game by its mean-field limit. As a byproduct, we prove that, given a distribution close to the target distribution measured by GMMD and a certain representation of the target distribution, we can generate a distribution close to the target one in terms of the Wasserstein distance and relative entropy. Overall, we show that the proposed class of metrics is a powerful tool to analyze the convergence of empirical measures in high dimensions without CoD.},
  archiveprefix = {arXiv},
  keywords = {60B10; 60E15; 60K35; 91A16; 60H10,Computer Science - Machine Learning,Mathematics - Probability,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/Z78ZRZAD/Han et al. - 2021 - A Class of Dimensionality-free Metrics for the Con.pdf;/home/wyq977/Zotero/storage/BTHQW4WR/2104.html}
}

@misc{heissHowImplicitRegularization2021,
  title = {How {{Implicit Regularization}} of {{ReLU Neural Networks Characterizes}} the {{Learned Function}} -- {{Part I}}: The 1-{{D Case}} of {{Two Layers}} with {{Random First Layer}}},
  shorttitle = {How {{Implicit Regularization}} of {{ReLU Neural Networks Characterizes}} the {{Learned Function}} -- {{Part I}}},
  author = {Heiss, Jakob and Teichmann, Josef and Wutte, Hanna},
  year = {2021},
  month = jul,
  number = {arXiv:1911.02903},
  eprint = {1911.02903},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.02903},
  abstract = {Today, various forms of neural networks are trained to perform approximation tasks in many fields. However, the estimates obtained are not fully understood on function space. Empirical results suggest that typical training algorithms favor regularized solutions. These observations motivate us to analyze properties of the neural networks found by gradient descent initialized close to zero, that is frequently employed to perform the training task. As a starting point, we consider one dimensional (shallow) ReLU neural networks in which weights are chosen randomly and only the terminal layer is trained. First, we rigorously show that for such networks ridge regularized regression corresponds in function space to regularizing the estimate's second derivative for fairly general loss functionals. For least squares regression, we show that the trained network converges to the smooth spline interpolation of the training data as the number of hidden nodes tends to infinity. Moreover, we derive a correspondence between the early stopped gradient descent and the smoothing spline regression. Our analysis might give valuable insight on the properties of the solutions obtained using gradient descent methods in general settings.},
  archiveprefix = {arXiv},
  keywords = {41Axx; 93Exx; 68T05; 68Q32,Computer Science - Machine Learning,G.3,I.2.6,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/RCEQUQ5R/Heiss et al. - 2021 - How Implicit Regularization of ReLU Neural Network.pdf;/home/wyq977/Zotero/storage/F325NW4L/1911.html}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {08936080},
  doi = {10.1016/0893-6080(89)90020-8},
  langid = {english},
  file = {/home/wyq977/Zotero/storage/QDN96WDA/Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf}
}

@misc{javanmardAnalysisTwoLayerNeural2019,
  title = {Analysis of a {{Two-Layer Neural Network}} via {{Displacement Convexity}}},
  author = {Javanmard, Adel and Mondelli, Marco and Montanari, Andrea},
  year = {2019},
  month = aug,
  number = {arXiv:1901.01375},
  eprint = {1901.01375},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  abstract = {Fitting a function by using linear combinations of a large number \$N\$ of `simple' components is one of the most fruitful ideas in statistical learning. This idea lies at the core of a variety of methods, from two-layer neural networks to kernel regression, to boosting. In general, the resulting risk minimization problem is non-convex and is solved by gradient descent or its variants. Unfortunately, little is known about global convergence properties of these approaches. Here we consider the problem of learning a concave function \$f\$ on a compact convex domain \$\textbackslash Omega\textbackslash subseteq \{\textbackslash mathbb R\}\^d\$, using linear combinations of `bump-like' components (neurons). The parameters to be fitted are the centers of \$N\$ bumps, and the resulting empirical risk minimization problem is highly non-convex. We prove that, in the limit in which the number of neurons diverges, the evolution of gradient descent converges to a Wasserstein gradient flow in the space of probability distributions over \$\textbackslash Omega\$. Further, when the bump width \$\textbackslash delta\$ tends to \$0\$, this gradient flow has a limit which is a viscous porous medium equation. Remarkably, the cost function optimized by this gradient flow exhibits a special property known as displacement convexity, which implies exponential convergence rates for \$N\textbackslash to\textbackslash infty\$, \$\textbackslash delta\textbackslash to 0\$. Surprisingly, this asymptotic theory appears to capture well the behavior for moderate values of \$\textbackslash delta, N\$. Explaining this phenomenon, and understanding the dependence on \$\textbackslash delta,N\$ in a quantitative manner remains an outstanding challenge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory},
  file = {/home/wyq977/Zotero/storage/C3G32MF3/Javanmard et al. - 2019 - Analysis of a Two-Layer Neural Network via Displac.pdf;/home/wyq977/Zotero/storage/5LPAB7AJ/1901.html}
}

@misc{jinLogicalFallacyDetection2022,
  title = {Logical {{Fallacy Detection}}},
  author = {Jin, Zhijing and Lalwani, Abhinav and Vaidhya, Tejas and Shen, Xiaoyu and Ding, Yiwen and Lyu, Zhiheng and Sachan, Mrinmaya and Mihalcea, Rada and Sch{\"o}lkopf, Bernhard},
  year = {2022},
  month = may,
  number = {arXiv:2202.13758},
  eprint = {2202.13758},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Reasoning is central to human intelligence. However, fallacious arguments are common, and some exacerbate problems such as spreading misinformation about climate change. In this paper, we propose the task of logical fallacy detection, and provide a new dataset (Logic) of logical fallacies generally found in text, together with an additional challenge set for detecting logical fallacies in climate change claims (LogicClimate). Detecting logical fallacies is a hard problem as the model must understand the underlying logical structure of the argument. We find that existing pretrained large language models perform poorly on this task. In contrast, we show that a simple structure-aware classifier outperforms the best language model by 5.46\% on Logic and 4.51\% on LogicClimate. We encourage future work to explore this task as (a) it can serve as a new reasoning challenge for language models, and (b) it can have potential applications in tackling the spread of misinformation. Our dataset and code are available at https://github.com/causalNLP/logical-fallacy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Logic in Computer Science,Computer Science - Machine Learning},
  file = {/home/wyq977/Zotero/storage/NW2KRTX2/Jin et al. - 2022 - Logical Fallacy Detection.pdf;/home/wyq977/Zotero/storage/2G4E3GGE/2202.html}
}

@article{klusowskiApproximationCombinationsReLU2018,
  title = {Approximation by {{Combinations}} of {{ReLU}} and {{Squared ReLU Ridge Functions}} with \$ \textbackslash ell\^1 \$ and \$ \textbackslash ell\^0 \$ {{Controls}}},
  author = {Klusowski, Jason M. and Barron, Andrew R.},
  year = {2018},
  month = may,
  journal = {arXiv:1607.07819 [math, stat]},
  eprint = {1607.07819},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {We establish \$ L\^\{\textbackslash infty\} \$ and \$ L\^2 \$ error bounds for functions of many variables that are approximated by linear combinations of ReLU (rectified linear unit) and squared ReLU ridge functions with \$ \textbackslash ell\^1 \$ and \$ \textbackslash ell\^0 \$ controls on their inner and outer parameters. With the squared ReLU ridge function, we show that the \$ L\^2 \$ approximation error is inversely proportional to the inner layer \$ \textbackslash ell\^0 \$ sparsity and it need only be sublinear in the outer layer \$ \textbackslash ell\^0 \$ sparsity. Our constructions are obtained using a variant of the Jones-Barron probabilistic method, which can be interpreted as either stratified sampling with proportionate allocation or two-stage cluster sampling. We also provide companion error lower bounds that reveal near optimality of our constructions. Despite the sparsity assumptions, we showcase the richness and flexibility of these ridge combinations by defining a large family of functions, in terms of certain spectral conditions, that are particularly well approximated by them.},
  archiveprefix = {arXiv},
  keywords = {62M45; 41A15,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/IJ275KKA/Klusowski and Barron - 2018 - Approximation by Combinations of ReLU and Squared .pdf;/home/wyq977/Zotero/storage/SC7P9V4P/1607.html}
}

@misc{klusowskiMinimaxLowerBounds2017,
  title = {Minimax {{Lower Bounds}} for {{Ridge Combinations Including Neural Nets}}},
  author = {Klusowski, Jason M. and Barron, Andrew R.},
  year = {2017},
  month = feb,
  number = {arXiv:1702.02828},
  eprint = {1702.02828},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1702.02828},
  abstract = {Estimation of functions of \$ d \$ variables is considered using ridge combinations of the form \$ \textbackslash textstyle\textbackslash sum\_\{k=1\}\^m c\_\{1,k\} \textbackslash phi(\textbackslash textstyle\textbackslash sum\_\{j=1\}\^d c\_\{0,j,k\}x\_j-b\_k) \$ where the activation function \$ \textbackslash phi \$ is a function with bounded value and derivative. These include single-hidden layer neural networks, polynomials, and sinusoidal models. From a sample of size \$ n \$ of possibly noisy values at random sites \$ X \textbackslash in B = [-1,1]\^d \$, the minimax mean square error is examined for functions in the closure of the \$ \textbackslash ell\_1 \$ hull of ridge functions with activation \$ \textbackslash phi \$. It is shown to be of order \$ d/n \$ to a fractional power (when \$ d \$ is of smaller order than \$ n \$), and to be of order \$ (\textbackslash log d)/n \$ to a fractional power (when \$ d \$ is of larger order than \$ n \$). Dependence on constraints \$ v\_0 \$ and \$ v\_1 \$ on the \$ \textbackslash ell\_1 \$ norms of inner parameter \$ c\_0 \$ and outer parameter \$ c\_1 \$, respectively, is also examined. Also, lower and upper bounds on the fractional power are given. The heart of the analysis is development of information-theoretic packing numbers for these classes of functions.},
  archiveprefix = {arXiv},
  keywords = {62J02; 62G08; 68T05,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/94MED2BL/Klusowski and Barron - 2017 - Minimax Lower Bounds for Ridge Combinations Includ.pdf;/home/wyq977/Zotero/storage/8HILZQCG/1702.html}
}

@misc{klusowskiRiskBoundsHighdimensional2018,
  title = {Risk {{Bounds}} for {{High-dimensional Ridge Function Combinations Including Neural Networks}}},
  author = {Klusowski, Jason M. and Barron, Andrew R.},
  year = {2018},
  month = oct,
  number = {arXiv:1607.01434},
  eprint = {1607.01434},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1607.01434},
  abstract = {Let \$ f\^\{\textbackslash star\} \$ be a function on \$ \textbackslash mathbb\{R\}\^d \$ with an assumption of a spectral norm \$ v\_\{f\^\{\textbackslash star\}\} \$. For various noise settings, we show that \$ \textbackslash mathbb\{E\}\textbackslash |\textbackslash hat\{f\} - f\^\{\textbackslash star\} \textbackslash |\^2 \textbackslash leq \textbackslash left(v\^4\_\{f\^\{\textbackslash star\}\}\textbackslash frac\{\textbackslash log d\}\{n\}\textbackslash right)\^\{1/3\} \$, where \$ n \$ is the sample size and \$ \textbackslash hat\{f\} \$ is either a penalized least squares estimator or a greedily obtained version of such using linear combinations of sinusoidal, sigmoidal, ramp, ramp-squared or other smooth ridge functions. The candidate fits may be chosen from a continuum of functions, thus avoiding the rigidity of discretizations of the parameter space. On the other hand, if the candidate fits are chosen from a discretization, we show that \$ \textbackslash mathbb\{E\}\textbackslash |\textbackslash hat\{f\} - f\^\{\textbackslash star\} \textbackslash |\^2 \textbackslash leq \textbackslash left(v\^3\_\{f\^\{\textbackslash star\}\}\textbackslash frac\{\textbackslash log d\}\{n\}\textbackslash right)\^\{2/5\} \$. This work bridges non-linear and non-parametric function estimation and includes single-hidden layer nets. Unlike past theory for such settings, our bound shows that the risk is small even when the input dimension \$ d \$ of an infinite-dimensional parameterized dictionary is much larger than the available sample size. When the dimension is larger than the cube root of the sample size, this quantity is seen to improve the more familiar risk bound of \$ v\_\{f\^\{\textbackslash star\}\}\textbackslash left(\textbackslash frac\{d\textbackslash log (n/d)\}\{n\}\textbackslash right)\^\{1/2\} \$, also investigated here.},
  archiveprefix = {arXiv},
  keywords = {62J02; 62G08; 68T05,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/TX578AW9/Klusowski and Barron - 2018 - Risk Bounds for High-dimensional Ridge Function Co.pdf;/home/wyq977/Zotero/storage/M3XEZFAK/1607.html}
}

@article{klusowskiRiskBoundsHighdimensional2018a,
  title = {Risk {{Bounds}} for {{High-dimensional Ridge Function Combinations Including Neural Networks}}},
  author = {Klusowski, Jason M. and Barron, Andrew R.},
  year = {2018},
  month = oct,
  journal = {arXiv:1607.01434 [math, stat]},
  eprint = {1607.01434},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {Let \$ f\^\{\textbackslash star\} \$ be a function on \$ \textbackslash mathbb\{R\}\^d \$ with an assumption of a spectral norm \$ v\_\{f\^\{\textbackslash star\}\} \$. For various noise settings, we show that \$ \textbackslash mathbb\{E\}\textbackslash |\textbackslash hat\{f\} - f\^\{\textbackslash star\} \textbackslash |\^2 \textbackslash leq \textbackslash left(v\^4\_\{f\^\{\textbackslash star\}\}\textbackslash frac\{\textbackslash log d\}\{n\}\textbackslash right)\^\{1/3\} \$, where \$ n \$ is the sample size and \$ \textbackslash hat\{f\} \$ is either a penalized least squares estimator or a greedily obtained version of such using linear combinations of sinusoidal, sigmoidal, ramp, ramp-squared or other smooth ridge functions. The candidate fits may be chosen from a continuum of functions, thus avoiding the rigidity of discretizations of the parameter space. On the other hand, if the candidate fits are chosen from a discretization, we show that \$ \textbackslash mathbb\{E\}\textbackslash |\textbackslash hat\{f\} - f\^\{\textbackslash star\} \textbackslash |\^2 \textbackslash leq \textbackslash left(v\^3\_\{f\^\{\textbackslash star\}\}\textbackslash frac\{\textbackslash log d\}\{n\}\textbackslash right)\^\{2/5\} \$. This work bridges non-linear and non-parametric function estimation and includes single-hidden layer nets. Unlike past theory for such settings, our bound shows that the risk is small even when the input dimension \$ d \$ of an infinite-dimensional parameterized dictionary is much larger than the available sample size. When the dimension is larger than the cube root of the sample size, this quantity is seen to improve the more familiar risk bound of \$ v\_\{f\^\{\textbackslash star\}\}\textbackslash left(\textbackslash frac\{d\textbackslash log (n/d)\}\{n\}\textbackslash right)\^\{1/2\} \$, also investigated here.},
  archiveprefix = {arXiv},
  keywords = {62J02; 62G08; 68T05,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/IBSSFDVL/Klusowski and Barron - 2018 - Risk Bounds for High-dimensional Ridge Function Co.pdf;/home/wyq977/Zotero/storage/SGBT65GT/1607.html}
}

@article{kundurMagnitudePhase,
  title = {Magnitude and {{Phase}}},
  author = {Kundur, Professor Deepa},
  pages = {5},
  langid = {english},
  file = {/home/wyq977/Zotero/storage/IXES29C2/Kundur - Magnitude and Phase.pdf}
}

@misc{leeAbilityNeuralNets2021,
  title = {On the Ability of Neural Nets to Express Distributions},
  author = {Lee, Holden and Ge, Rong and Ma, Tengyu and Risteski, Andrej and Arora, Sanjeev},
  year = {2021},
  month = apr,
  number = {arXiv:1702.07028},
  eprint = {1702.07028},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Deep neural nets have caused a revolution in many classification tasks. A related ongoing revolution -- also theoretically not understood -- concerns their ability to serve as generative models for complicated types of data such as images and texts. These models are trained using ideas like variational autoencoders and Generative Adversarial Networks. We take a first cut at explaining the expressivity of multilayer nets by giving a sufficient criterion for a function to be approximable by a neural network with \$n\$ hidden layers. A key ingredient is Barron's Theorem \textbackslash cite\{Barron1993\}, which gives a Fourier criterion for approximability of a function by a neural network with 1 hidden layer. We show that a composition of \$n\$ functions which satisfy certain Fourier conditions ("Barron functions") can be approximated by a \$n+1\$-layer neural network. For probability distributions, this translates into a criterion for a probability distribution to be approximable in Wasserstein distance -- a natural metric on probability distributions -- by a neural network applied to a fixed base distribution (e.g., multivariate gaussian). Building up recent lower bound work, we also give an example function that shows that composition of Barron functions is more expressive than Barron functions alone.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/wyq977/Zotero/storage/KECHNJ2C/Lee et al. - 2021 - On the ability of neural nets to express distribut.pdf;/home/wyq977/Zotero/storage/5Y5HSIEL/1702.html}
}

@article{liangJustInterpolateKernel2019,
  title = {Just {{Interpolate}}: {{Kernel}} "{{Ridgeless}}" {{Regression Can Generalize}}},
  shorttitle = {Just {{Interpolate}}},
  author = {Liang, Tengyuan and Rakhlin, Alexander},
  year = {2019},
  month = feb,
  journal = {arXiv:1808.00387 [cs, math, stat]},
  eprint = {1808.00387},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  doi = {10.1214/19-AOS1849},
  abstract = {In the absence of explicit regularization, Kernel ``Ridgeless'' Regression with nonlinear kernels has the potential to fit the training data perfectly. It has been observed empirically, however, that such interpolated solutions can still generalize well on test data. We isolate a phenomenon of implicit regularization for minimum-norm interpolated solutions which is due to a combination of high dimensionality of the input data, curvature of the kernel function, and favorable geometric properties of the data such as an eigenvalue decay of the empirical covariance and kernel matrices. In addition to deriving a data-dependent upper bound on the out-of-sample error, we present experimental evidence suggesting that the phenomenon occurs in the MNIST dataset.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/YMXK5Y2T/Liang and Rakhlin - 2019 - Just Interpolate Kernel Ridgeless Regression Ca.pdf}
}

@article{liComplexityMeasuresNeural2020,
  title = {Complexity {{Measures}} for {{Neural Networks}} with {{General Activation Functions Using Path-based Norms}}},
  author = {Li, Zhong and Ma, Chao and Wu, Lei},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.06132 [cs, stat]},
  eprint = {2009.06132},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {A simple approach is proposed to obtain complexity controls for neural networks with general activation functions. The approach is motivated by approximating the general activation functions with one-dimensional ReLU networks, which reduces the problem to the complexity controls of ReLU networks. Specifically, we consider two-layer networks and deep residual networks, for which path-based norms are derived to control complexities. We also provide preliminary analyses of the function spaces induced by these norms and a priori estimates of the corresponding regularized estimators.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/ZI4KLY69/Li et al. - 2020 - Complexity Measures for Neural Networks with Gener.pdf;/home/wyq977/Zotero/storage/IJHPA2EL/2009.html}
}

@article{liUnprecedentedGenomicDiversity2015,
  title = {Unprecedented Genomic Diversity of {{RNA}} Viruses in Arthropods Reveals the Ancestry of Negative-Sense {{RNA}} Viruses},
  author = {Li, Ci-Xiu and Shi, Mang and Tian, Jun-Hua and Lin, Xian-Dan and Kang, Yan-Jun and Chen, Liang-Jun and Qin, Xin-Cheng and Xu, Jianguo and Holmes, Edward C. and Zhang, Yong-Zhen},
  year = {2015},
  month = jan,
  journal = {eLife},
  volume = {4},
  issn = {2050-084X},
  doi = {10.7554/eLife.05378},
  abstract = {Although arthropods are important viral vectors, the biodiversity of arthropod viruses, as well as the role that arthropods have played in viral origins and evolution, is unclear. Through RNA sequencing of 70 arthropod species we discovered 112 novel viruses that appear to be ancestral to much of the documented genetic diversity of negative-sense RNA viruses, a number of which are also present as endogenous genomic copies. With this greatly enriched diversity we revealed that arthropods contain viruses that fall basal to major virus groups, including the vertebrate-specific arenaviruses, filoviruses, hantaviruses, influenza viruses, lyssaviruses, and paramyxoviruses. We similarly documented a remarkable diversity of genome structures in arthropod viruses, including a putative circular form, that sheds new light on the evolution of genome organization. Hence, arthropods are a major reservoir of viral genetic diversity and have likely been central to viral evolution.},
  langid = {english},
  pmcid = {PMC4384744},
  pmid = {25633976},
  keywords = {Animals,arthropods,Arthropods,Biodiversity,evolution,Evolution; Molecular,Genome,infectious disease,microbiology,negative-sense,phylogeny,Phylogeny,RNA virus,RNA Viruses,segmentation,viruses},
  file = {/home/wyq977/Zotero/storage/B34W32VX/Li et al. - 2015 - Unprecedented genomic diversity of RNA viruses in .pdf}
}

@misc{maFunctionSpaceTheory2020,
  title = {A {{Function Space Theory}} and {{Generalization Error Estimates}} for {{Neural Network Models}}},
  author = {Ma, Chao},
  year = {2020},
  langid = {english},
  file = {/home/wyq977/Zotero/storage/TB6UJM4G/Ma - A Function Space Theory and Generalization Error E.pdf}
}

@phdthesis{meiComputationalStatisticalTheories2020,
  title = {Computational and Statistical Theories for Large-Scale Neural Networks},
  author = {Mei, Song and Montanari, Andrea and Johnstone, Iain and Ying, Lexing},
  year = {2020},
  address = {{Stanford, California}},
  abstract = {Deep learning methods operate in regimes that defy the traditional computational and statistical mindsets. Despite the non-convexity of empirical risks and the huge complexity of neural network architectures, stochastic gradient algorithms can often find an approximate global minimizer of the training loss and achieve small generalization error on test data. In recent years, an important research direction is to theoretically explain these observed optimization efficiency and generalization efficacy of neural network systems. This thesis tries to tackle these challenges in the model of two-layers neural networks, by analyzing its computational and statistical properties in various scaling limits. On the computational aspects, we introduce two competing theories for neural network dynamics: the mean field theory and the tangent kernel theory. These two theories characterize training dynamics of neural networks in different regimes that exhibit different behaviors. In the mean field framework, the training dynamics, in the large neuron limit, is captured by a particular non-linear partial differential equation. This characterization allows us to prove global convergence of the dynamics in certain scenarios. Comparatively, the tangent kernel theory characterizes the same dynamics in a different scaling limit and provides global convergence guarantees in more general scenarios. On the statistical aspects, we study the generalization properties of neural networks trained in the two regimes as described above. We first show that, in the high dimensional limit, neural tangent kernels are no better than polynomial regression, while neural networks trained in the mean field regime can potentially perform better. Next, we study more carefully the random features model, which is equivalent to a two-layers neural network in the kernel regime. We compute the precise asymptotics of its test error in the high dimensional limit and confirm that it exhibits an interesting double-descent curve that was observed in experiments},
  collaborator = {Stanford University},
  school = {Stanford University},
  file = {/home/wyq977/Zotero/storage/E2MABAX4/Mei et al. - 2020 - Computational and statistical theories for large-s.pdf}
}

@article{montanariOneLectureTwolayer2018,
  title = {One Lecture on Two-Layer Neural Networks},
  author = {Montanari, Andrea},
  year = {2018},
  month = aug,
  pages = {10},
  abstract = {Notes for a lecture at the Carg`ese Summer School `Statistical Physics and Machine Learning Back Together,' August 21, 2018.},
  langid = {english},
  file = {/home/wyq977/Zotero/storage/B84VZ8A5/Montanari - One lecture on two-layer neural networks.pdf}
}

@misc{neyshaburImplicitRegularizationDeep2017,
  title = {Implicit {{Regularization}} in {{Deep Learning}}},
  author = {Neyshabur, Behnam},
  year = {2017},
  month = sep,
  number = {arXiv:1709.01953},
  eprint = {1709.01953},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {In an attempt to better understand generalization in deep learning, we study several possible explanations. We show that implicit regularization induced by the optimization method is playing a key role in generalization and success of deep learning models. Motivated by this view, we study how different complexity measures can ensure generalization and explain how optimization algorithms can implicitly regularize complexity measures. We empirically investigate the ability of these measures to explain different observed phenomena in deep learning. We further study the invariances in neural networks, suggest complexity measures and optimization algorithms that have similar invariances to those in neural networks and evaluate them on a number of learning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/wyq977/Zotero/storage/9WB6TY4D/Neyshabur - 2017 - Implicit Regularization in Deep Learning.pdf;/home/wyq977/Zotero/storage/7286XC2I/1709.html}
}

@article{neyshaburNormBasedCapacityControl2015,
  title = {Norm-{{Based Capacity Control}} in {{Neural Networks}}},
  author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  year = {2015},
  month = apr,
  journal = {arXiv:1503.00036 [cs, stat]},
  eprint = {1503.00036},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We investigate the capacity, convexity and characterization of a general family of norm-constrained feed-forward networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/H47XLQHN/Neyshabur et al. - 2015 - Norm-Based Capacity Control in Neural Networks.pdf;/home/wyq977/Zotero/storage/ATQ32VYT/1503.html}
}

@article{oymakNearOptimalBoundsBinary2015,
  title = {Near-{{Optimal Bounds}} for {{Binary Embeddings}} of {{Arbitrary Sets}}},
  author = {Oymak, Samet and Recht, Ben},
  year = {2015},
  month = dec,
  journal = {arXiv:1512.04433 [cs, math]},
  eprint = {1512.04433},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {We study embedding a subset K of the unit sphere to the Hamming cube \{-1, +1\}m. We characterize the tradeoff between distortion and sample complexity m in terms of the Gaussian width {$\omega$}(K) of the set. For subspaces and several structured-sparse sets we show that Gaussian maps provide the optimal tradeoff m {$\sim$} {$\delta-$}2{$\omega$}2(K), in particular for {$\delta$} distortion one needs m {$\approx$} {$\delta-$}2d where d is the subspace dimension. For general sets, we provide sharp characterizations which reduces to m {$\approx$} {$\delta-$}4{$\omega$}2(K) after simplification. We provide improved results for local embedding of points that are in close proximity of each other which is related to locality sensitive hashing. We also discuss faster binary embedding where one takes advantage of an initial sketching procedure based on Fast Johnson-Lindenstauss Transform. Finally, we list several numerical observations and discuss open problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Mathematics - Functional Analysis},
  file = {/home/wyq977/Zotero/storage/GTN638CU/Oymak and Recht - 2015 - Near-Optimal Bounds for Binary Embeddings of Arbit.pdf}
}

@misc{parhiBanachSpaceRepresenter2021,
  title = {Banach {{Space Representer Theorems}} for {{Neural Networks}} and {{Ridge Splines}}},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2021},
  month = feb,
  number = {arXiv:2006.05626},
  eprint = {2006.05626},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.05626},
  abstract = {We develop a variational framework to understand the properties of the functions learned by neural networks fit to data. We propose and study a family of continuous-domain linear inverse problems with total variation-like regularization in the Radon domain subject to data fitting constraints. We derive a representer theorem showing that finite-width, single-hidden layer neural networks are solutions to these inverse problems. We draw on many techniques from variational spline theory and so we propose the notion of polynomial ridge splines, which correspond to single-hidden layer neural networks with truncated power functions as the activation function. The representer theorem is reminiscent of the classical reproducing kernel Hilbert space representer theorem, but we show that the neural network problem is posed over a non-Hilbertian Banach space. While the learning problems are posed in the continuous-domain, similar to kernel methods, the problems can be recast as finite-dimensional neural network training problems. These neural network training problems have regularizers which are related to the well-known weight decay and path-norm regularizers. Thus, our result gives insight into functional characteristics of trained neural networks and also into the design neural network regularizers. We also show that these regularizers promote neural network solutions with desirable generalization properties.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/TRTMDXUH/Parhi and Nowak - 2021 - Banach Space Representer Theorems for Neural Netwo.pdf;/home/wyq977/Zotero/storage/7C9YN4MA/2006.html}
}

@article{parhiRoleNeuralNetwork2020,
  title = {The {{Role}} of {{Neural Network Activation Functions}}},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2020},
  journal = {IEEE Signal Processing Letters},
  volume = {27},
  eprint = {1910.02333},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {1779--1783},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2020.3027517},
  abstract = {A wide variety of activation functions have been proposed for neural networks. The Rectified Linear Unit (ReLU) is especially popular today. There are many practical reasons that motivate the use of the ReLU. This paper provides new theoretical characterizations that support the use of the ReLU, its variants such as the leaky ReLU, as well as other activation functions in the case of univariate, single-hidden layer feedforward neural networks. Our results also explain the importance of commonly used strategies in the design and training of neural networks such as "weight decay" and "path-norm" regularization, and provide a new justification for the use of "skip connections" in network architectures. These new insights are obtained through the lens of spline theory. In particular, we show how neural network training problems are related to infinite-dimensional optimizations posed over Banach spaces of functions whose solutions are well-known to be fractional and polynomial splines, where the particular Banach space (which controls the order of the spline) depends on the choice of activation function.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/2QLXE6X7/Parhi and Nowak - 2020 - The Role of Neural Network Activation Functions.pdf;/home/wyq977/Zotero/storage/8JCZB47V/1910.html}
}

@article{parhiWhatKindsFunctions2022,
  title = {What {{Kinds}} of {{Functions}} Do {{Deep Neural Networks Learn}}? {{Insights}} from {{Variational Spline Theory}}},
  shorttitle = {What {{Kinds}} of {{Functions}} Do {{Deep Neural Networks Learn}}?},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2022},
  month = jun,
  journal = {SIAM Journal on Mathematics of Data Science},
  volume = {4},
  number = {2},
  eprint = {2105.03361},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {464--489},
  issn = {2577-0187},
  doi = {10.1137/21M1418642},
  abstract = {We develop a variational framework to understand the properties of functions learned by fitting deep neural networks with rectified linear unit activations to data. We propose a new function space, which is reminiscent of classical bounded variation-type spaces, that captures the compositional structure associated with deep neural networks. We derive a representer theorem showing that deep ReLU networks are solutions to regularized data fitting problems over functions from this space. The function space consists of compositions of functions from the Banach spaces of second-order bounded variation in the Radon domain. These are Banach spaces with sparsity-promoting norms, giving insight into the role of sparsity in deep neural networks. The neural network solutions have skip connections and rank bounded weight matrices, providing new theoretical support for these common architectural choices. The variational problem we study can be recast as a finite-dimensional neural network training problem with regularization schemes related to the notions of weight decay and path-norm regularization. Finally, our analysis builds on techniques from variational spline theory, providing new connections between deep neural networks and splines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/TIK3K2PG/Parhi and Nowak - 2022 - What Kinds of Functions do Deep Neural Networks Le.pdf;/home/wyq977/Zotero/storage/ZSDU6IU2/2105.html}
}

@inproceedings{rahimiUniformApproximationFunctions2008,
  title = {Uniform Approximation of Functions with Random Bases},
  booktitle = {2008 46th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}}},
  author = {Rahimi, Ali and Recht, Benjamin},
  year = {2008},
  month = sep,
  pages = {555--561},
  publisher = {{IEEE}},
  address = {{Monticello, IL, USA}},
  doi = {10.1109/ALLERTON.2008.4797607},
  abstract = {Random networks of nonlinear functions have a long history of empirical success in function fitting but few theoretical guarantees. In this paper, using techniques from probability on Banach Spaces, we analyze a specific architecture of random nonlinearities, provide L{$\infty$} and L2 error bounds for approximating functions in Reproducing Kernel Hilbert Spaces, and discuss scenarios when these expansions are dense in the continuous functions. We discuss connections between these random nonlinear networks and popular machine learning algorithms and show experimentally that these networks provide competitive performance at far lower computational cost on large-scale pattern recognition tasks.},
  isbn = {978-1-4244-2925-7},
  langid = {english},
  file = {/home/wyq977/Zotero/storage/WQALQ84F/Rahimi and Recht - 2008 - Uniform approximation of functions with random bas.pdf}
}

@article{schmidt-hieberNonparametricRegressionUsing2020,
  title = {Nonparametric Regression Using Deep Neural Networks with {{ReLU}} Activation Function},
  author = {{Schmidt-Hieber}, Johannes},
  year = {2020},
  month = sep,
  journal = {arXiv:1708.06633 [cs, math, stat]},
  eprint = {1708.06633},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  doi = {10.1214/19-AOS1875},
  abstract = {Consider the multivariate nonparametric regression model. It is shown that estimators based on sparsely connected deep neural networks with ReLU activation function and properly chosen network architecture achieve the minimax rates of convergence (up to \$\textbackslash log n\$-factors) under a general composition assumption on the regression function. The framework includes many well-studied structural constraints such as (generalized) additive models. While there is a lot of flexibility in the network architecture, the tuning parameter is the sparsity of the network. Specifically, we consider large networks with number of potential network parameters exceeding the sample size. The analysis gives some insights into why multilayer feedforward neural networks perform well in practice. Interestingly, for ReLU activation function the depth (number of layers) of the neural network architectures plays an important role and our theory suggests that for nonparametric regression, scaling the network depth with the sample size is natural. It is also shown that under the composition assumption wavelet estimators can only achieve suboptimal rates.},
  archiveprefix = {arXiv},
  keywords = {62G08,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/wyq977/Zotero/storage/ETB8XX9L/Schmidt-Hieber - 2020 - Nonparametric regression using deep neural network.pdf;/home/wyq977/Zotero/storage/GX95FRWR/1708.html}
}

@book{shalev-shwartzUnderstandingMachineLearning2014,
  title = {Understanding Machine Learning: From Theory to Algorithms},
  shorttitle = {Understanding Machine Learning},
  author = {{Shalev-Shwartz}, Shai and {Ben-David}, Shai},
  year = {2014},
  publisher = {{Cambridge University Press}},
  address = {{New York, NY, USA}},
  abstract = {"Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering"--},
  isbn = {978-1-107-05713-5},
  langid = {english},
  lccn = {Q325.5 .S475 2014},
  keywords = {Algorithms,COMPUTERS / Computer Vision \& Pattern Recognition,Machine learning},
  file = {/home/wyq977/Zotero/storage/TFGV6SZI/Shalev-Shwartz and Ben-David - 2014 - Understanding machine learning from theory to alg.pdf}
}

@article{starlingTargetedSmoothBayesian2020,
  title = {Targeted {{Smooth Bayesian Causal Forests}}: {{An}} Analysis of Heterogeneous Treatment Effects for Simultaneous versus Interval Medical Abortion Regimens over Gestation},
  shorttitle = {Targeted {{Smooth Bayesian Causal Forests}}},
  author = {Starling, Jennifer E. and Murray, Jared S. and Lohr, Patricia A. and Aiken, Abigail R. A. and Carvalho, Carlos M. and Scott, James G.},
  year = {2020},
  month = feb,
  journal = {arXiv:1905.09405 [stat]},
  eprint = {1905.09405},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We introduce Targeted Smooth Bayesian Causal Forests (tsBCF), a nonparametric Bayesian approach for estimating heterogeneous treatment effects which vary smoothly over a single covariate in the observational data setting. The tsBCF method induces smoothness by parameterizing terminal tree nodes with smooth functions, and allows for separate regularization of treatment effects versus prognostic effect of control covariates. Smoothing parameters for prognostic and treatment effects can be chosen to reflect prior knowledge or tuned in a datadependent way.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Applications},
  file = {/home/wyq977/Zotero/storage/7D9WEMZL/Starling et al. - 2020 - Targeted Smooth Bayesian Causal Forests An analys.pdf}
}

@article{wangDebiasedInferenceTreatment,
  title = {Debiased {{Inference}} on {{Treatment Effect}} in a {{High-Dimensional Model}}},
  author = {Wang, Jingshen and He, Xuming and Xu, Gongjun},
  pages = {14},
  abstract = {This article concerns the potential bias in statistical inference on treatment effects when a large number of covariates are present in a linear or partially linear model. While the estimation bias in an under-fitted model is well understood, we address a lesser-known bias that arises from an over-fitted model. The overfitting bias can be eliminated through data splitting at the cost of statistical efficiency, and we show that smoothing over random data splits can be pursued to mitigate the efficiency loss. We also discuss some of the existing methods for debiased inference and provide insights into their intrinsic bias-variance trade-off, which leads to an improvement in bias controls. Under appropriate conditions, we show that the proposed estimators for the treatment effects are asymptotically normal and their variances can be well estimated. We discuss the pros and cons of various methods both theoretically and empirically, and show that the proposed methods are valuable options in post-selection inference. Supplementary materials for this article are available online.},
  langid = {english},
  file = {/home/wyq977/Zotero/storage/QZGGM2TR/Wang et al. - Debiased Inference on Treatment Effect in a High-D.pdf}
}

@phdthesis{wuUnderstandingWhenKernel,
  title = {Understanding {{When Kernel Ridgeless Regression Works}}},
  author = {Wu, Mingqi},
  langid = {english},
  file = {/home/wyq977/Zotero/storage/9ISXLHQA/Wu - Understanding When Kernel Ridgeless Regression Wor.pdf}
}
