@misc{abbePolytimeUniversalityLimitations2020,
  title = {Poly-Time Universality and Limitations of Deep Learning},
  author = {Abbe, Emmanuel and Sandon, Colin},
  year = {2020},
  month = jan,
  number = {arXiv:2001.02992},
  eprint = {2001.02992},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.02992},
  urldate = {2022-09-06},
  abstract = {The goal of this paper is to characterize function distributions that deep learning can or cannot learn in poly-time. A universality result is proved for SGD-based deep learning and a non-universality result is proved for GD-based deep learning; this also gives a separation between SGD-based deep learning and statistical query algorithms: (1) \{{\textbackslash}it Deep learning with SGD is efficiently universal.\} Any function distribution that can be learned from samples in poly-time can also be learned by a poly-size neural net trained with SGD on a poly-time initialization with poly-steps, poly-rate and possibly poly-noise. Therefore deep learning provides a universal learning paradigm: it was known that the approximation and estimation errors could be controlled with poly-size neural nets, using ERM that is NP-hard; this new result shows that the optimization error can also be controlled with SGD in poly-time. The picture changes for GD with large enough batches: (2) \{{\textbackslash}it Result (1) does not hold for GD:\} Neural nets of poly-size trained with GD (full gradients or large enough batches) on any initialization with poly-steps, poly-range and at least poly-noise cannot learn any function distribution that has super-polynomial \{{\textbackslash}it cross-predictability,\} where the cross-predictability gives a measure of ``average'' function correlation -- relations and distinctions to the statistical dimension are discussed. In particular, GD with these constraints can learn efficiently monomials of degree \$k\$ if and only if \$k\$ is constant. Thus (1) and (2) point to an interesting contrast: SGD is universal even with some poly-noise while full GD or SQ algorithms are not (e.g., parities).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@book{alma990005456080205503,
  title = {Functional Analysis},
  author = {Rudin, Walter},
  year = {1973/1973},
  series = {{{McGraw-Hill}} Series in Higher Mathematics},
  publisher = {McGraw-Hill},
  address = {New York},
  isbn = {0-07-054225-2},
  langid = {english},
  lccn = {71039686},
  keywords = {Analyse fonctionnelle}
}

@book{alma991011905999705251,
  title = {Real Analysis},
  author = {Royden, H. L. and Fitzpatrick, Patric},
  year = {2018/2010},
  series = {Pearson Modern Classic},
  edition = {Fourth edition [2018 reissue].},
  publisher = {Pearson},
  address = {New York, NY},
  isbn = {978-0-13-468949-4},
  langid = {english},
  lccn = {2016055244},
  keywords = {Functions of real variables}
}

@book{alma991031140799705251,
  title = {Real Analysis : Modern Techniques and Their Applications},
  author = {Folland, G. B.},
  year = {1999/1999},
  series = {Pure and Applied Mathematics},
  edition = {Second edition.},
  publisher = {John Wiley \& Sons},
  address = {New York},
  isbn = {0-471-31716-0},
  langid = {english},
  lccn = {98037260},
  keywords = {Mathematical analysis}
}

@misc{andrealorkeCybenkoTheoremCapability,
  title = {Cybenko's {{Theorem}} and the Capability of a Neural Network as Function Approximator},
  author = {{Andrea L{\"o}rke} and {Fabian Schneider} and {Johannes Heck} and {Patrick Nitter}},
  urldate = {2022-06-23}
}

@article{bachBreakingCurseDimensionality2016,
  title = {Breaking the {{Curse}} of {{Dimensionality}} with {{Convex Neural Networks}}},
  author = {Bach, Francis},
  year = {2016},
  month = oct,
  journal = {arXiv:1412.8690 [cs, math, stat]},
  eprint = {1412.8690},
  primaryclass = {cs, math, stat},
  urldate = {2021-05-12},
  abstract = {We consider neural networks with a single hidden layer and non-decreasing homogeneous activa-tion functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of ob-servations. In addition, we provide a simple geometric interpretation to the non-convex problem of addition of a new unit, which is the core potentially hard computational element in the framework of learning from continuously many basis functions. We provide simple conditions for convex relaxations to achieve the same generalization error bounds, even when constant-factor approxi-mations cannot be found (e.g., because it is NP-hard such as for the zero-homogeneous activation function). We were not able to find strong enough convex relaxations and leave open the existence or non-existence of polynomial-time algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Mathematics - Statistics Theory}
}

@misc{baHighdimensionalAsymptoticsFeature2022,
  title = {High-Dimensional {{Asymptotics}} of {{Feature Learning}}: {{How One Gradient Step Improves}} the {{Representation}}},
  shorttitle = {High-Dimensional {{Asymptotics}} of {{Feature Learning}}},
  author = {Ba, Jimmy and Erdogdu, Murat A. and Suzuki, Taiji and Wang, Zhichao and Wu, Denny and Yang, Greg},
  year = {2022},
  month = may,
  number = {arXiv:2205.01445},
  eprint = {2205.01445},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2023-01-30},
  abstract = {We study the first gradient descent step on the first-layer parameters \${\textbackslash}boldsymbol\{W\}\$ in a two-layer neural network: \$f({\textbackslash}boldsymbol\{x\}) = {\textbackslash}frac\{1\}\{{\textbackslash}sqrt\{N\}\}{\textbackslash}boldsymbol\{a\}{\textasciicircum}{\textbackslash}top{\textbackslash}sigma({\textbackslash}boldsymbol\{W\}{\textasciicircum}{\textbackslash}top{\textbackslash}boldsymbol\{x\})\$, where \${\textbackslash}boldsymbol\{W\}{\textbackslash}in{\textbackslash}mathbb\{R\}{\textasciicircum}\{d{\textbackslash}times N\}, {\textbackslash}boldsymbol\{a\}{\textbackslash}in{\textbackslash}mathbb\{R\}{\textasciicircum}\{N\}\$ are randomly initialized, and the training objective is the empirical MSE loss: \${\textbackslash}frac\{1\}\{n\}{\textbackslash}sum\_\{i=1\}{\textasciicircum}n (f({\textbackslash}boldsymbol\{x\}\_i)-y\_i){\textasciicircum}2\$. In the proportional asymptotic limit where \$n,d,N{\textbackslash}to{\textbackslash}infty\$ at the same rate, and an idealized student-teacher setting, we show that the first gradient update contains a rank-1 "spike", which results in an alignment between the first-layer weights and the linear component of the teacher model \$f{\textasciicircum}*\$. To characterize the impact of this alignment, we compute the prediction risk of ridge regression on the conjugate kernel after one gradient step on \${\textbackslash}boldsymbol\{W\}\$ with learning rate \${\textbackslash}eta\$, when \$f{\textasciicircum}*\$ is a single-index model. We consider two scalings of the first step learning rate \${\textbackslash}eta\$. For small \${\textbackslash}eta\$, we establish a Gaussian equivalence property for the trained feature map, and prove that the learned kernel improves upon the initial random features model, but cannot defeat the best linear model on the input. Whereas for sufficiently large \${\textbackslash}eta\$, we prove that for certain \$f{\textasciicircum}*\$, the same ridge estimator on trained features can go beyond this "linear regime" and outperform a wide range of random features and rotationally invariant kernels. Our results demonstrate that even one gradient step can lead to a considerable advantage over random features, and highlight the role of learning rate scaling in the initial phase of training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@article{banerjeeStatistics612Lp,
  title = {Statistics 612: {{Lp}} Spaces, Metrics on Spaces of Probabilites, and Connections to Estimation},
  author = {Banerjee, Moulinath},
  pages = {11},
  langid = {english}
}

@article{barronApproximationEstimationBounds1994,
  title = {Approximation and {{Estimation Bounds}} for {{Artificial Neural Networks}}},
  author = {Barron, Andrew R.},
  year = {1994},
  month = jan,
  journal = {Machine Learning},
  volume = {14},
  number = {1},
  pages = {115--133},
  issn = {1573-0565},
  doi = {10.1007/BF00993164},
  urldate = {2021-04-19},
  abstract = {For a common class of artificial neural networks, the mean integrated squared error between the estimated network and a target functionf is shown to be bounded by\$\$O{\textbackslash}left( \{{\textbackslash}frac\{\{{\textbackslash}mathop c{\textbackslash}nolimits\_f{\textasciicircum}2 \}\}\{n\}\} {\textbackslash}right) + O{\textbackslash}left( \{{\textbackslash}frac\{\{nd\}\}\{N\}{\textbackslash}log  N\} {\textbackslash}right)\$\$wheren is the number of nodes,d is the input dimension of the function,N is the number of training observations, andCfis the first absolute moment of the Fourier magnitude distribution off. The two contributions to this total risk are the approximation error and the estimation error. Approximation error refers to the distance between the target function and the closest neural network function of a given architecture and estimation error refers to the distance between this ideal network function and an estimated network function. Withn {$\sim$} Cf(N/(d logN))1/2 nodes, the order of the bound on the mean integrated squared error is optimized to beO(Cf((d/N) logN)1/2). The bound demonstrates surprisingly favorable properties of network estimation compared to traditional series and nonparametric curve estimation techniques in the case thatd is moderately large. Similar bounds are obtained when the number of nodesn is not preselected as a function ofCf(which is generally not knowna priori), but rather the number of nodes is optimized from the observed data by the use of a complexity regularization or minimum description length criterion. The analysis involves Fourier techniques for the approximation error, metric entropy considerations for the estimation error, and a calculation of the index of resolvability of minimum complexity estimation of the family of networks.},
  langid = {english}
}

@article{barronApproximationEstimationHighDimensional2018,
  title = {Approximation and {{Estimation}} for {{High-Dimensional Deep Learning Networks}}},
  author = {Barron, Andrew R. and Klusowski, Jason M.},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.03090 [cs, stat]},
  eprint = {1809.03090},
  primaryclass = {cs, stat},
  urldate = {2021-04-21},
  abstract = {It has been experimentally observed in recent years that multi-layer artificial neural networks have a surprising ability to generalize, even when trained with far more parameters than observations. Is there a theoretical basis for this? The best available bounds on their metric entropy and associated complexity measures are essentially linear in the number of parameters, which is inadequate to explain this phenomenon. Here we examine the statistical risk (mean squared predictive error) of multi-layer networks with \${\textbackslash}ell{\textasciicircum}1\$-type controls on their parameters and with ramp activation functions (also called lower-rectified linear units). In this setting, the risk is shown to be upper bounded by \$[(L{\textasciicircum}3 {\textbackslash}log d)/n]{\textasciicircum}\{1/2\}\$, where \$d\$ is the input dimension to each layer, \$L\$ is the number of layers, and \$n\$ is the sample size. In this way, the input dimension can be much larger than the sample size and the estimator can still be accurate, provided the target function has such \${\textbackslash}ell{\textasciicircum}1\$ controls and that the sample size is at least moderately large compared to \$L{\textasciicircum}3{\textbackslash}log d\$. The heart of the analysis is the development of a sampling strategy that demonstrates the accuracy of a sparse covering of deep ramp networks. Lower bounds show that the identified risk is close to being optimal.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{barronApproximationLearningGreedy2008,
  title = {Approximation and Learning by Greedy Algorithms},
  author = {Barron, Andrew R. and Cohen, Albert and Dahmen, Wolfgang and DeVore, Ronald A.},
  year = {2008},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {36},
  number = {1},
  pages = {64--94},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053607000000631},
  urldate = {2023-03-20},
  abstract = {We consider the problem of approximating a given element f from a Hilbert space \${\textbackslash}mathcal\{H\}\$ by means of greedy algorithms and the application of such procedures to the regression problem in statistical learning theory. We improve on the existing theory of convergence rates for both the orthogonal greedy algorithm and the relaxed greedy algorithm, as well as for the forward stepwise projection algorithm. For all these algorithms, we prove convergence results for a variety of function classes and not simply those that are related to the convex hull of the dictionary. We then show how these bounds for convergence rates lead to a new theory for the performance of greedy algorithms in learning. In particular, we build upon the results in [IEEE Trans. Inform. Theory 42 (1996) 2118--2132] to construct learning algorithms based on greedy approximations which are universally consistent and provide provable convergence rates for large classes of functions. The use of greedy algorithms in the context of learning is very appealing since it greatly reduces the computational burden when compared with standard model selection using general dictionaries.},
  keywords = {41A46,41A63,46N30,62G07,convergence rates for greedy algorithms,interpolation spaces,neural networks,Nonparametric regression,Statistical learning}
}

@inproceedings{barronNeuralNetApproximation1992,
  title = {Neural {{Net Approximation}}},
  booktitle = {Proc. 7th {{Yale}} Workshop on Adaptive and Learning Systems},
  author = {Barron, Andrew R.},
  year = {1992},
  volume = {1},
  pages = {69--72}
}

@article{barronUniversalApproximationBounds1993,
  title = {Universal {{Approximation Bounds}} for {{Superpositions}} of a {{Sigmoidal Function}}},
  author = {Barron, A. R.},
  year = {1993},
  month = may,
  journal = {IEEE Transactions on Information Theory},
  volume = {39},
  number = {3},
  pages = {930--945},
  issn = {1557-9654},
  doi = {10.1109/18.256500},
  abstract = {Approximation properties of a class of artificial neural networks are established. It is shown that feedforward networks with one layer of sigmoidal nonlinearities achieve integrated squared error of order O(1/n), where n is the number of nodes. The approximated function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. The nonlinear parameters associated with the sigmoidal nodes, as well as the parameters of linear combination, are adjusted in the approximation. In contrast, it is shown that for series expansions with n terms, in which only the parameters of linear combination are adjusted, the integrated squared approximation error cannot be made smaller than order 1/n/sup 2/d/ uniformly for functions satisfying the same smoothness assumption, where d is the dimension of the input to the function. For the class of functions examined, the approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings.{$<>$}},
  keywords = {Approximation error,Artificial neural networks,Feedforward neural networks,Feeds,Fourier transforms,Information theory,Linear approximation,Neural networks,Statistical distributions,Statistics}
}

@article{bartlettLocalRademacherComplexities2005,
  title = {Local {{Rademacher}} Complexities},
  author = {Bartlett, Peter L. and Bousquet, Olivier and Mendelson, Shahar},
  year = {2005},
  month = aug,
  journal = {The Annals of Statistics},
  volume = {33},
  number = {4},
  eprint = {math/0508275},
  issn = {0090-5364},
  doi = {10.1214/009053605000000282},
  urldate = {2021-05-12},
  abstract = {We propose new bounds on the error of learning algorithms in terms of a data-dependent notion of complexity. The estimates we establish give optimal rates and are based on a local and empirical version of Rademacher averages, in the sense that the Rademacher averages are computed from the data, on a subset of functions with small empirical error. We present some applications to classification and prediction with convex function classes, and with kernel classes in particular.},
  archiveprefix = {arXiv},
  keywords = {62G08 68Q32 (Primary),Mathematics - Statistics Theory}
}

@article{beckMachineLearningApproximation2019,
  title = {Machine Learning Approximation Algorithms for High-Dimensional Fully Nonlinear Partial Differential Equations and Second-Order Backward Stochastic Differential Equations},
  author = {Beck, Christian and E, Weinan and Jentzen, Arnulf},
  year = {2019},
  month = aug,
  journal = {Journal of Nonlinear Science},
  volume = {29},
  number = {4},
  eprint = {1709.05963},
  primaryclass = {cs, math, stat},
  pages = {1563--1619},
  issn = {0938-8974, 1432-1467},
  doi = {10.1007/s00332-018-9525-3},
  urldate = {2023-02-17},
  abstract = {High-dimensional partial differential equations (PDE) appear in a number of models from the financial industry, such as in derivative pricing models, credit valuation adjustment (CVA) models, or portfolio optimization models. The PDEs in such applications are high-dimensional as the dimension corresponds to the number of financial assets in a portfolio. Moreover, such PDEs are often fully nonlinear due to the need to incorporate certain nonlinear phenomena in the model such as default risks, transaction costs, volatility uncertainty (Knightian uncertainty), or trading constraints in the model. Such high-dimensional fully nonlinear PDEs are exceedingly difficult to solve as the computational effort for standard approximation methods grows exponentially with the dimension. In this work we propose a new method for solving high-dimensional fully nonlinear second-order PDEs. Our method can in particular be used to sample from high-dimensional nonlinear expectations. The method is based on (i) a connection between fully nonlinear second-order PDEs and second-order backward stochastic differential equations (2BSDEs), (ii) a merged formulation of the PDE and the 2BSDE problem, (iii) a temporal forward discretization of the 2BSDE and a spatial approximation via deep neural nets, and (iv) a stochastic gradient descent-type optimization procedure. Numerical results obtained using \$\{{\textbackslash}rm T\{{\textbackslash}small ENSOR\}F\{{\textbackslash}small LOW\}\}\$ in \$\{{\textbackslash}rm P\{{\textbackslash}small YTHON\}\}\$ illustrate the efficiency and the accuracy of the method in the cases of a \$100\$-dimensional Black-Scholes-Barenblatt equation, a \$100\$-dimensional Hamilton-Jacobi-Bellman equation, and a nonlinear expectation of a \$ 100 \$-dimensional \$ G \$-Brownian motion.},
  archiveprefix = {arXiv},
  keywords = {65C99 65M99 60H30 65-05,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Numerical Analysis,Mathematics - Probability,Statistics - Machine Learning}
}

@article{beckSolvingKolmogorovPDE2021,
  title = {Solving the {{Kolmogorov PDE}} by Means of Deep Learning},
  author = {Beck, Christian and Becker, Sebastian and Grohs, Philipp and Jaafari, Nor and Jentzen, Arnulf},
  year = {2021},
  month = sep,
  journal = {Journal of Scientific Computing},
  volume = {88},
  number = {3},
  eprint = {1806.00421},
  primaryclass = {cs, math, stat},
  pages = {73},
  issn = {0885-7474, 1573-7691},
  doi = {10.1007/s10915-021-01590-0},
  urldate = {2023-02-17},
  abstract = {Stochastic differential equations (SDEs) and the Kolmogorov partial differential equations (PDEs) associated to them have been widely used in models from engineering, finance, and the natural sciences. In particular, SDEs and Kolmogorov PDEs, respectively, are highly employed in models for the approximative pricing of financial derivatives. Kolmogorov PDEs and SDEs, respectively, can typically not be solved explicitly and it has been and still is an active topic of research to design and analyze numerical methods which are able to approximately solve Kolmogorov PDEs and SDEs, respectively. Nearly all approximation methods for Kolmogorov PDEs in the literature suffer under the curse of dimensionality or only provide approximations of the solution of the PDE at a single fixed space-time point. In this paper we derive and propose a numerical approximation method which aims to overcome both of the above mentioned drawbacks and intends to deliver a numerical approximation of the Kolmogorov PDE on an entire region \$[a,b]{\textasciicircum}d\$ without suffering from the curse of dimensionality. Numerical results on examples including the heat equation, the Black-Scholes model, the stochastic Lorenz equation, and the Heston model suggest that the proposed approximation algorithm is quite effective in high dimensions in terms of both accuracy and speed.},
  archiveprefix = {arXiv},
  keywords = {65C99 65M99 60H30,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Probability,Statistics - Machine Learning}
}

@article{bellmanTheoryDynamicProgramming1952,
  title = {On the {{Theory}} of {{Dynamic Programming}}},
  author = {Bellman, Richard},
  year = {1952},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {38},
  number = {8},
  pages = {716--719},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.38.8.716},
  urldate = {2023-02-17}
}

@misc{berlindRademacherComplexity,
  title = {Rademacher {{Complexity}}},
  author = {Berlind, Chris},
  urldate = {2022-06-20}
}

@incollection{bernerModernMathematicsDeep2022,
  title = {The {{Modern Mathematics}} of {{Deep Learning}}},
  booktitle = {Mathematical Aspects of Deep Learning},
  author = {Berner, Julius and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
  editor = {Grohs, Philipp and Kutyniok, GittaEditors},
  year = {2022},
  pages = {1--111},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/9781009025096.002}
}

@incollection{bourgainDualityProblemEntropy1989,
  title = {On the Duality Problem for Entropy Numbers of Operators},
  booktitle = {Geometric {{Aspects}} of {{Functional Analysis}}},
  author = {Bourgain, J. and Pajor, A. and Szarek, S. J. and {Tomczak-Jaegermann}, N.},
  editor = {Lindenstrauss, Joram and Milman, Vitali D.},
  year = {1989},
  volume = {1376},
  pages = {50--63},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/BFb0090048},
  urldate = {2023-03-08},
  isbn = {978-3-540-51303-2},
  langid = {english}
}

@book{brezisFunctionalAnalysisSobolev2011,
  title = {Functional {{Analysis}}, {{Sobolev Spaces}} and {{Partial Differential Equations}}},
  author = {Brezis, Haim},
  year = {2011},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-0-387-70914-7},
  urldate = {2023-03-16},
  isbn = {978-0-387-70913-0 978-0-387-70914-7},
  langid = {english}
}

@misc{carageaNeuralNetworkApproximation2022,
  title = {Neural Network Approximation and Estimation of Classifiers with Classification Boundary in a {{Barron}} Class},
  author = {Caragea, Andrei and Petersen, Philipp and Voigtlaender, Felix},
  year = {2022},
  month = mar,
  number = {arXiv:2011.09363},
  eprint = {2011.09363},
  primaryclass = {math, stat},
  publisher = {arXiv},
  urldate = {2022-09-20},
  abstract = {We prove bounds for the approximation and estimation of certain binary classification functions using ReLU neural networks. Our estimation bounds provide a priori performance guarantees for empirical risk minimization using networks of a suitable size, depending on the number of training samples available. The obtained approximation and estimation rates are independent of the dimension of the input, showing that the curse of dimensionality can be overcome in this setting; in fact, the input dimension only enters in the form of a polynomial factor. Regarding the regularity of the target classification function, we assume the interfaces between the different classes to be locally of Barron-type. We complement our results by studying the relations between various Barron-type spaces that have been proposed in the literature. These spaces differ substantially more from each other than the current literature suggests.},
  archiveprefix = {arXiv},
  keywords = {68T07 41A25 41A46 42B35 46E15,Mathematics - Functional Analysis,Statistics - Machine Learning}
}

@inproceedings{carrollConstructionNeuralNets1989,
  title = {Construction of {{Neural Nets Using}} the {{Radon Transform}}},
  booktitle = {International 1989 {{Joint Conference}} on {{Neural Networks}}},
  author = {{Carroll} and {Dickinson}},
  year = {1989},
  pages = {607-611 vol.1},
  doi = {10.1109/IJCNN.1989.118639},
  abstract = {The authors present a method for constructing a feedforward neural net implementing an arbitrarily good approximation to any L/sub 2/ function over (-1, 1)/sup n/. The net uses n input nodes, a single hidden layer whose width is determined by the function to be implemented and the allowable mean square error, and a linear output neuron. Error bounds and an example are given for the method.{$<>$}},
  keywords = {Approximation methods,Neural networks}
}

@article{cheangBetterApproximationBalls2000,
  title = {A {{Better Approximation}} for {{Balls}}},
  author = {Cheang, Gerald H.L. and Barron, Andrew R.},
  year = {2000},
  month = jun,
  journal = {Journal of Approximation Theory},
  volume = {104},
  number = {2},
  pages = {183--203},
  issn = {00219045},
  doi = {10.1006/jath.1999.3441},
  urldate = {2023-02-27},
  langid = {english}
}

@misc{chenBrauerGroupRational2019,
  title = {The {{Brauer Group}} of {{Rational Numbers}}},
  author = {Chen, Haiyu},
  year = {2019},
  month = nov,
  number = {arXiv:1911.02368},
  eprint = {1911.02368},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.02368},
  urldate = {2023-03-13},
  abstract = {In this project, we will study the Brauer group that was first defined by R. Brauer. The elements of the Brauer group are the equivalence classes of finite dimensional central simple algebra. Therefore understanding the structure of the Brauer group of a field is equivalent to a complete classification of finite dimensional central division algebras over the field. One of the important achievements of algebra and number theory in the last century is the determination of Br(Q), the Brauer group of rational numbers. The aim of this dissertation is to review this project, i.e., determining Br(Q). There are three main steps. The first step is to determine Br(R), the Brauer group of real numbers. The second step is to identify Br(k\_{\textbackslash}nu), the Brauer group of the local fields. The third step is to construct two maps Br(Q) to Br(R) and Br(Q) to Br(Q\_p) and to use these two maps to understand Br(Q). This dissertation completed the first two steps of this enormous project. To the author's knowledge, in literature there is no document including all the details of determining Br(Q) and most of them are written from a advanced perspective that requires the knowledge of class field theory and cohomology. The goal of this document is to develop the result in a relatively elementary way. The project mainly follows the logic of the book [6], but significant amount of details are added and some proofs are originated by the author, for example, 1.2.6, 1.4.2(ii), 4.2.6, and maximality and uniqueness of 5.5.12.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Group Theory,Mathematics - History and Overview,Mathematics - Number Theory,Mathematics - Rings and Algebras}
}

@article{chengNeuralNetworksReview1994,
  title = {Neural {{Networks}}: {{A Review}} from a {{Statistical Perspective}}},
  shorttitle = {Neural {{Networks}}},
  author = {Cheng, Bing and Titterington, D. M.},
  year = {1994},
  journal = {Statistical Science},
  volume = {9},
  number = {1},
  eprint = {2246275},
  eprinttype = {jstor},
  pages = {2--30},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237},
  urldate = {2023-03-20},
  abstract = {This paper informs a statistical readership about Artificial Neural Networks (ANNs), points out some of the links with statistical methodology and encourages cross-disciplinary research in the directions most likely to bear fruit. The areas of statistical interest are briefly outlined, and a series of examples indicates the flavor of ANN models. We then treat various topics in more depth. In each case, we describe the neural network architectures and training rules and provide a statistical commentary. The topics treated in this way are perceptrons (from single-unit to multilayer versions), Hopfield-type recurrent networks (including probabilistic versions strongly related to statistical physics and Gibbs distributions) and associative memory networks trained by so-called unsuperviszd learning rules. Perceptrons are shown to have strong associations with discriminant analysis and regression, and unsupervized networks with cluster analysis. The paper concludes with some thoughts on the future of the interface between neural networks and statistics.}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by {{Superpositions}} of a {{Sigmoidal Function}}},
  author = {Cybenko, G.},
  year = {1989},
  month = dec,
  journal = {Mathematics of Control, Signals and Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  urldate = {2022-06-23},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  langid = {english},
  keywords = {Approximation,Completeness,Neural networks}
}

@misc{dehoopConvergenceRatesLearning2022,
  title = {Convergence {{Rates}} for {{Learning Linear Operators}} from {{Noisy Data}}},
  author = {{de Hoop}, Maarten V. and Kovachki, Nikola B. and Nelsen, Nicholas H. and Stuart, Andrew M.},
  year = {2022},
  month = nov,
  number = {arXiv:2108.12515},
  eprint = {2108.12515},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.12515},
  urldate = {2023-02-17},
  abstract = {This paper studies the learning of linear operators between infinite-dimensional Hilbert spaces. The training data comprises pairs of random input vectors in a Hilbert space and their noisy images under an unknown self-adjoint linear operator. Assuming that the operator is diagonalizable in a known basis, this work solves the equivalent inverse problem of estimating the operator's eigenvalues given the data. Adopting a Bayesian approach, the theoretical analysis establishes posterior contraction rates in the infinite data limit with Gaussian priors that are not directly linked to the forward map of the inverse problem. The main results also include learning-theoretic generalization error guarantees for a wide range of distribution shifts. These convergence rates quantify the effects of data smoothness and true eigenvalue decay or growth, for compact or unbounded operators, respectively, on sample complexity. Numerical evidence supports the theory in diagonal and non-diagonal settings.},
  archiveprefix = {arXiv},
  keywords = {62G20 62C10 68T05 47A62,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology}
}

@book{dellacherieProbabilitiesPotential1982,
  title = {Probabilities and Potential {{B}}},
  author = {Dellacherie, Claude and Meyer, Paul-Andr{\'e}},
  year = {1982},
  series = {North-{{Holland}} Mathematics Studies},
  number = {72},
  publisher = {{North-Holland Pub. Co. Sole distributors for the U.S.A. and Canada, Elsevier Science Pub. Co}},
  address = {Amsterdam New York},
  isbn = {978-0-444-86526-7},
  langid = {english},
  lccn = {519.2}
}

@article{devore_1998,
  title = {Nonlinear Approximation},
  author = {DeVore, Ronald A.},
  year = {1998},
  journal = {Acta Numerica},
  volume = {7},
  pages = {51--150},
  publisher = {Cambridge University Press},
  doi = {10.1017/S0962492900002816}
}

@article{devoreNeuralNetworkApproximation2021,
  title = {Neural Network Approximation},
  author = {DeVore, Ronald and Hanin, Boris and Petrova, Guergana},
  year = {2021},
  month = may,
  journal = {Acta Numerica},
  volume = {30},
  pages = {327--444},
  issn = {0962-4929, 1474-0508},
  doi = {10.1017/S0962492921000052},
  urldate = {2023-02-13},
  abstract = {Neural networks (NNs) are the method of choice for building learning algorithms. They are now being investigated for other numerical tasks such as solving high-dimensional partial differential equations. Their popularity stems from their empirical success on several challenging learning problems (computer chess/Go, autonomous navigation, face recognition). However, most scholars agree that a convincing theoretical explanation for this success is still lacking. Since these applications revolve around approximating an unknown function from data observations, part of the answer must involve the ability of NNs to produce accurate approximations.                            This article surveys the known approximation properties of the outputs of NNs with the aim of uncovering the properties that are not present in the more traditional methods of approximation used in numerical analysis, such as approximations using polynomials, wavelets, rational functions and splines. Comparisons are made with traditional approximation methods from the viewpoint of rate distortion,               i.e.               error versus the number of parameters used to create the approximant. Another major component in the analysis of numerical approximation is the computational time needed to construct the approximation, and this in turn is intimately connected with the stability of the approximation algorithm. So the stability of numerical approximation using NNs is a large part of the analysis put forward.                                         The survey, for the most part, is concerned with NNs using the popular ReLU activation function. In this case the outputs of the NNs are piecewise linear functions on rather complicated partitions of the domain of               f               into cells that are convex polytopes. When the architecture of the NN is fixed and the parameters are allowed to vary, the set of output functions of the NN is a parametrized nonlinear manifold. It is shown that this manifold has certain space-filling properties leading to an increased ability to approximate (better rate distortion) but at the expense of numerical stability. The space filling creates the challenge to the numerical method of finding best or good parameter choices when trying to approximate.},
  langid = {english}
}

@book{diestelSequencesSeriesBanach1984,
  title = {Sequences and {{Series}} in {{Banach Spaces}}},
  author = {Diestel, Joseph},
  editor = {Halmos, P. R. and Moore, C. C. and Gehring, F. W.},
  year = {1984},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {92},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-5200-9},
  urldate = {2023-03-26},
  isbn = {978-1-4612-9734-5 978-1-4612-5200-9},
  langid = {english}
}

@inproceedings{donohoHighDimensionalDataAnalysis2000,
  title = {High-{{Dimensional Data Analysis}}: {{The Curses}} and {{Blessings}} of {{Dimensionality}}},
  shorttitle = {High-Dimensional Data Analysis},
  booktitle = {{{AMS Conference}} on {{Math Challenges}} of the 21st {{Century}}},
  author = {Donoho, David L.},
  year = {2000},
  pages = {33},
  address = {Los Angeles, CA, USA},
  abstract = {The coming century is surely the century of data. A combination of blind faith and serious purpose makes our society invest massively in the collection and processing of data of all kinds, on scales unimaginable until recently. Hyperspectral Imagery, Internet Portals, Financial tick-by-tick data, and DNA Microarrays are just a few of the betterknown sources, feeding data in torrential streams into scientific and business databases worldwide. In traditional statistical data analysis, we think of observations of instances of particular phenomena (e.g. instance {$\leftrightarrow$} human being), these observations being a vector of values we measured on several variables (e.g. blood pressure, weight, height,...). In traditional statistical methodology, we assumed many observations and a few, wellchosen variables. The trend today is towards more observations but even more so, to radically larger numbers of variables -- voracious, automatic, systematic collection of hyper-informative detail about each observed instance. We are seeing examples where the observations gathered on individual instances are curves, or spectra, or images, or}
}

@article{durrettProbabilityTheoryExamples,
  title = {Probability: {{Theory}} and {{Examples}}},
  author = {Durrett, Rick},
  langid = {english}
}

@article{eBanachSpacesAssociated2020,
  title = {On the {{Banach}} Spaces Associated with Multi-Layer {{ReLU}} Networks: {{Function}} Representation, Approximation Theory and Gradient Descent Dynamics},
  shorttitle = {On the {{Banach}} Spaces Associated with Multi-Layer {{ReLU}} Networks},
  author = {E, Weinan and Wojtowytsch, Stephan},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.15623 [cs, math, stat]},
  eprint = {2007.15623},
  primaryclass = {cs, math, stat},
  urldate = {2021-05-11},
  abstract = {We develop Banach spaces for ReLU neural networks of finite depth \$L\$ and infinite width. The spaces contain all finite fully connected \$L\$-layer networks and their \$L{\textasciicircum}2\$-limiting objects under bounds on the natural path-norm. Under this norm, the unit ball in the space for \$L\$-layer networks has low Rademacher complexity and thus favorable generalization properties. Functions in these spaces can be approximated by multi-layer neural networks with dimension-independent convergence rates. The key to this work is a new way of representing functions in some form of expectations, motivated by multi-layer neural networks. This representation allows us to define a new class of continuous models for machine learning. We show that the gradient flow defined this way is the natural continuous analog of the gradient descent dynamics for the associated multi-layer neural networks. We show that the path-norm increases at most polynomially under this continuous gradient flow dynamics.},
  archiveprefix = {arXiv},
  keywords = {68T07 46E15 26B35 26B40,Computer Science - Machine Learning,Mathematics - Functional Analysis,Statistics - Machine Learning}
}

@article{eBarronSpaceFlowinduced2021,
  title = {The {{Barron Space}} and the {{Flow-induced Function Spaces}} for {{Neural Network Models}}},
  author = {E, Weinan and Ma, Chao and Wu, Lei},
  year = {2021},
  month = mar,
  journal = {arXiv:1906.08039 [cs, math, stat]},
  eprint = {1906.08039},
  primaryclass = {cs, math, stat},
  urldate = {2021-04-19},
  abstract = {One of the key issues in the analysis of machine learning models is to identify the appropriate function space and norm for the model. This is the set of functions endowed with a quantity which can control the approximation and estimation errors by a particular machine learning model. In this paper, we address this issue for two representative neural network models: the two-layer networks and the residual neural networks. We define the Barron space and show that it is the right space for two-layer neural network models in the sense that optimal direct and inverse approximation theorems hold for functions in the Barron space. For residual neural network models, we construct the so-called flow-induced function space, and prove direct and inverse approximation theorems for this space. In addition, we show that the Rademacher complexity for bounded sets under these norms has the optimal upper bounds.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Probability,Statistics - Machine Learning}
}

@article{eComparativeAnalysisOptimization2020,
  title = {A {{Comparative Analysis}} of the {{Optimization}} and {{Generalization Property}} of {{Two-layer Neural Network}} and {{Random Feature Models Under Gradient Descent Dynamics}}},
  author = {E, Weinan and Ma, Chao and Wu, Lei},
  year = {2020},
  month = jul,
  journal = {Science China Mathematics},
  volume = {63},
  number = {7},
  eprint = {1904.04326},
  primaryclass = {cs, math, stat},
  pages = {1235--1258},
  issn = {1674-7283, 1869-1862},
  doi = {10.1007/s11425-019-1628-5},
  urldate = {2022-09-21},
  abstract = {A fairly comprehensive analysis is presented for the gradient descent dynamics for training two-layer neural network models in the situation when the parameters in both layers are updated. General initialization schemes as well as general regimes for the network width and training data size are considered. In the over-parametrized regime, it is shown that gradient descent dynamics can achieve zero training loss exponentially fast regardless of the quality of the labels. In addition, it is proved that throughout the training process the functions represented by the neural network model are uniformly close to that of a kernel method. For general values of the network width and training data size, sharp estimates of the generalization error is established for target functions in the appropriate reproducing kernel Hilbert space.},
  archiveprefix = {arXiv},
  keywords = {41A99 49M99,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning}
}

@article{eDeepLearningbasedNumerical2017,
  title = {Deep Learning-Based Numerical Methods for High-Dimensional Parabolic Partial Differential Equations and Backward Stochastic Differential Equations},
  author = {E, Weinan and Han, Jiequn and Jentzen, Arnulf},
  year = {2017},
  month = dec,
  journal = {Communications in Mathematics and Statistics},
  volume = {5},
  number = {4},
  eprint = {1706.04702},
  primaryclass = {cs, math, stat},
  pages = {349--380},
  issn = {2194-6701, 2194-671X},
  doi = {10.1007/s40304-017-0117-6},
  urldate = {2023-02-17},
  abstract = {We propose a new algorithm for solving parabolic partial differential equations (PDEs) and backward stochastic differential equations (BSDEs) in high dimension, by making an analogy between the BSDE and reinforcement learning with the gradient of the solution playing the role of the policy function, and the loss function given by the error between the prescribed terminal condition and the solution of the BSDE. The policy function is then approximated by a neural network, as is done in deep reinforcement learning. Numerical results using TensorFlow illustrate the efficiency and accuracy of the proposed algorithms for several 100-dimensional nonlinear PDEs from physics and finance such as the Allen-Cahn equation, the Hamilton-Jacobi-Bellman equation, and a nonlinear pricing model for financial derivatives.},
  archiveprefix = {arXiv},
  keywords = {65M75 60H35 65C30,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Numerical Analysis,Mathematics - Probability,Statistics - Machine Learning}
}

@misc{eDeepRitzMethod2017,
  title = {The {{Deep Ritz}} Method: {{A}} Deep Learning-Based Numerical Algorithm for Solving Variational Problems},
  shorttitle = {The {{Deep Ritz}} Method},
  author = {E, Weinan and Yu, Bing},
  year = {2017},
  month = sep,
  number = {arXiv:1710.00211},
  eprint = {1710.00211},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.00211},
  urldate = {2023-02-17},
  abstract = {We propose a deep learning based method, the Deep Ritz Method, for numerically solving variational problems, particularly the ones that arise from partial differential equations. The Deep Ritz method is naturally nonlinear, naturally adaptive and has the potential to work in rather high dimensions. The framework is quite simple and fits well with the stochastic gradient descent method used in deep learning. We illustrate the method on several problems including some eigenvalue problems.},
  archiveprefix = {arXiv},
  keywords = {35Q68,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{eMathematicalUnderstandingNeural2020,
  title = {Towards a {{Mathematical Understanding}} of {{Neural Network-Based Machine Learning}}: What We Know and What We Don't},
  shorttitle = {Towards a {{Mathematical Understanding}} of {{Neural Network-Based Machine Learning}}},
  author = {E, Weinan and Ma, Chao and Wojtowytsch, Stephan and Wu, Lei},
  year = {2020},
  month = dec,
  journal = {arXiv:2009.10713 [cs, math, stat]},
  eprint = {2009.10713},
  primaryclass = {cs, math, stat},
  urldate = {2021-04-21},
  abstract = {The purpose of this article is to review the achievements made in the last few years towards the understanding of the reasons behind the success and subtleties of neural network-based machine learning. In the tradition of good old applied mathematics, we will not only give attention to rigorous mathematical results, but also the insight we have gained from careful numerical experiments as well as the analysis of simplified models. Along the way, we also list the open problems which we believe to be the most important topics for further study. This is not a complete overview over this quickly moving field, but we hope to provide a perspective which may be helpful especially to new researchers in the area.},
  archiveprefix = {arXiv},
  keywords = {68T07 (primary) 26B40 41A30 35Q68,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning}
}

@article{eObservationsPartialDifferential2020,
  title = {Some Observations on Partial Differential Equations in {{Barron}} and Multi-Layer Spaces},
  author = {E, Weinan and Wojtowytsch, Stephan},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.01484 [cs, math]},
  eprint = {2012.01484},
  primaryclass = {cs, math},
  urldate = {2021-04-21},
  abstract = {We use explicit representation formulas to show that solutions to certain partial differential equations lie in Barron spaces or multilayer spaces if the PDE data lie in such function spaces. Consequently, these solutions can be represented efficiently using artificial neural networks, even in high dimension. Conversely, we present examples in which the solution fails to lie in the function space associated to a neural network under consideration.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T07 35C15 65M80,Computer Science - Machine Learning,Mathematics - Analysis of PDEs}
}

@article{ePrioriEstimatesPopulation2019,
  title = {A {{Priori Estimates}} of the {{Population Risk}} for {{Residual Networks}}},
  author = {E, Weinan and Ma, Chao and Wang, Qingcan},
  year = {2019},
  month = may,
  journal = {arXiv:1903.02154 [cs, stat]},
  eprint = {1903.02154},
  primaryclass = {cs, stat},
  urldate = {2021-04-29},
  abstract = {Optimal a priori estimates are derived for the population risk, also known as the generalization error, of a regularized residual network model. An important part of the regularized model is the usage of a new path norm, called the weighted path norm, as the regularization term. The weighted path norm treats the skip connections and the nonlinearities differently so that paths with more nonlinearities are regularized by larger weights. The error estimates are a priori in the sense that the estimates depend only on the target function, not on the parameters obtained in the training process. The estimates are optimal, in a high dimensional setting, in the sense that both the bound for the approximation and estimation errors are comparable to the Monte Carlo error rates. A crucial step in the proof is to establish an optimal bound for the Rademacher complexity of the residual networks. Comparisons are made with existing norm-based generalization error bounds.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{ePrioriEstimatesPopulation2019a,
  title = {A {{Priori Estimates}} of the {{Population Risk}} for {{Two-layer Neural Networks}}},
  author = {E, Weinan and Ma, Chao and Wu, Lei},
  year = {2019},
  journal = {Communications in Mathematical Sciences},
  volume = {17},
  number = {5},
  eprint = {1810.06397},
  pages = {1407--1425},
  issn = {15396746, 19450796},
  doi = {10.4310/CMS.2019.v17.n5.a11},
  urldate = {2021-04-19},
  abstract = {New estimates for the population risk are established for two-layer neural networks. These estimates are nearly optimal in the sense that the error rates scale in the same way as the Monte Carlo error rates. They are equally effective in the over-parametrized regime when the network size is much larger than the size of the dataset. These new estimates are a priori in nature in the sense that the bounds depend only on some norms of the underlying functions to be fitted, not the parameters in the model, in contrast with most existing results which are a posteriori in nature. Using these a priori estimates, we provide a perspective for understanding why two-layer neural networks perform better than the related kernel methods.},
  archiveprefix = {arXiv},
  keywords = {41A46 41A63 62J02 65D05,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@article{eRepresentationFormulasPointwise2020,
  title = {Representation Formulas and Pointwise Properties for {{Barron}} Functions},
  author = {E, Weinan and Wojtowytsch, Stephan},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.05982 [cs, math, stat]},
  eprint = {2006.05982},
  primaryclass = {cs, math, stat},
  urldate = {2021-04-19},
  abstract = {We study the natural function space for infinitely wide two-layer neural networks and establish different representation formulae. In two cases, we describe the space explicitly up to isomorphism. Using a convenient representation, we study the pointwise properties of two-layer networks and show that functions whose singular set is fractal or curved (for example distance functions from smooth submanifolds) cannot be represented by infinitely wide two-layer networks with finite path-norm.},
  archiveprefix = {arXiv},
  keywords = {68T07 46E15 26B35 26B40,Computer Science - Machine Learning,Mathematics - Analysis of PDEs,Mathematics - Functional Analysis,Statistics - Machine Learning}
}

@misc{EuroClixStartVandaag,
  title = {{{EuroClix}} {\textbar} {{Start}} Vandaag Nog Met Geld Verdienen},
  urldate = {2024-12-11},
  abstract = {Bij EuroClix kunnen leden cashback ontvangen op online aankopen en geld verdienen met het invullen van vragenlijsten.},
  howpublished = {https://www.euroclix.nl/index},
  langid = {english}
}

@misc{fangLargedimensionalCentralLimit2021,
  title = {Large-Dimensional {{Central Limit Theorem}} with {{Fourth-moment Error Bounds}} on {{Convex Sets}} and {{Balls}}},
  author = {Fang, Xiao and Koike, Yuta},
  year = {2021},
  month = mar,
  number = {arXiv:2009.00339},
  eprint = {2009.00339},
  primaryclass = {math, stat},
  publisher = {arXiv},
  urldate = {2024-09-01},
  abstract = {We prove the large-dimensional Gaussian approximation of a sum of n independent random vectors in Rd together with fourth-moment error bounds on convex sets and Euclidean balls. We show that compared with classical third-moment bounds, our bounds have near-optimal dependence on n and can achieve improved dependence on the dimension d. For centered balls, we obtain an additional error bound that has a suboptimal dependence on n, but recovers the known result of the validity of the Gaussian approximation if and only if d = o(n). We discuss an application to the bootstrap. We prove our main results using Stein's method.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {60F05 62E17,Mathematics - Probability,Mathematics - Statistics Theory}
}

@article{funahashiApproximateRealizationContinuous1989,
  title = {On the {{Approximate Realization}} of {{Continuous Mappings}} by {{Neural Networks}}},
  author = {Funahashi, Ken-Ichi},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {3},
  pages = {183--192},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90003-8},
  urldate = {2023-03-23},
  abstract = {In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the Kolmogorov-Arnold-Sprecher theorem and this proof also gives non-trivial realizations.},
  langid = {english},
  keywords = {Back propagation,Continuous mapping,Hidden layer,Neural network,Output function,Realization,Sigmoid function,Unit}
}

@article{gaoEntropyAbsoluteConvex2004,
  title = {Entropy of {{Absolute Convex Hulls}} in {{Hilbert Spaces}}},
  author = {Gao, Fuchang},
  year = {2004},
  month = jul,
  journal = {Bulletin of the London Mathematical Society},
  volume = {36},
  number = {4},
  pages = {460--468},
  publisher = {Cambridge University Press},
  issn = {1469-2120, 0024-6093},
  doi = {10.1112/S0024609304003121},
  urldate = {2022-12-13},
  abstract = {The metric entropy of absolute convex hulls of sets in Hilbert spaces is studied for the general case when the metric entropy of the sets is arbitrary. Under some regularity assumptions, the results are sharp.},
  langid = {english},
  keywords = {41A46 (primary),60G15 (secondary)}
}

@misc{grunwaldNeymanPearsonEvaluesEnable2024,
  title = {Beyond {{Neyman-Pearson}}: E-Values Enable Hypothesis Testing with a Data-Driven Alpha},
  shorttitle = {Beyond {{Neyman-Pearson}}},
  author = {Gr{\"u}nwald, Peter},
  year = {2024},
  month = apr,
  number = {arXiv:2205.00901},
  eprint = {2205.00901},
  primaryclass = {stat},
  publisher = {arXiv},
  urldate = {2024-12-16},
  abstract = {A standard practice in statistical hypothesis testing is to mention the p-value alongside the accept/reject decision. We show the advantages of mentioning an e-value instead. With p-values, it is not clear how to use an extreme observation (e.g. p {$\ll$} {$\alpha$}) for getting better frequentist decisions. With e-values it is straightforward, since they provide Type-I risk control in a generalized Neyman-Pearson setting with the decision task (a general loss function) determined post-hoc, after observation of the data --- thereby providing a handle on `roving {$\alpha$}'s'. When Type-II risks are taken into consideration, the only admissible decision rules in the post-hoc setting turn out to be e-value-based. Similarly, if the loss incurred when specifying a faulty confidence interval is not fixed in advance, standard confidence intervals and distributions may fail whereas e-confidence sets and e-posteriors still provide valid risk guarantees. Sufficiently powerful e-values have by now been developed for a range of classical testing problems. We discuss the main challenges for wider development and deployment.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Methodology}
}

@misc{grunwaldOptimalEValuesExponential2024,
  title = {Optimal {{E-Values}} for {{Exponential Families}}: The {{Simple Case}}},
  shorttitle = {Optimal {{E-Values}} for {{Exponential Families}}},
  author = {Gr{\"u}nwald, Peter and Lardy, Tyron and Hao, Yunda and {Bar-Lev}, Shaul K. and {de Jong}, Martijn},
  year = {2024},
  month = apr,
  number = {arXiv:2404.19465},
  eprint = {2404.19465},
  primaryclass = {math, stat},
  publisher = {arXiv},
  urldate = {2024-12-13},
  abstract = {We provide a general condition under which e-variables in the form of a simple-vs.-simple likelihood ratio exist when the null hypothesis is a composite, multivariate exponential family. Such `simple' e-variables are easy to compute and expected-log-optimal with respect to any stopping time. Simple e-variables were previously only known to exist in quite specific settings, but we offer a unifying theorem on their existence for testing exponential families. We start with a simple alternative Q and a regular exponential family null. Together these induce a second exponential family Q containing Q, with the same sufficient statistic as the null. Our theorem shows that simple e-variables exist whenever the covariance matrices of Q and the null are in a certain relation. Examples in which this relation holds include some k-sample tests, Gaussian location- and scale tests, and tests for more general classes of natural exponential families.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology}
}

@misc{grunwaldSafeTesting2023,
  title = {Safe {{Testing}}},
  author = {Gr{\"u}nwald, Peter and {de Heide}, Rianne and Koolen, Wouter},
  year = {2023},
  month = mar,
  number = {arXiv:1906.07801},
  eprint = {1906.07801},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.07801},
  urldate = {2024-09-04},
  abstract = {We develop the theory of hypothesis testing based on the e-value, a notion of evidence that, unlike the p-value, allows for effortlessly combining results from several studies in the common scenario where the decision to perform a new study may depend on previous outcomes. Tests based on e-values are safe, i.e. they preserve Type-I error guarantees, under such optional continuation. We define growth-rate optimality (GRO) as an analogue of power in an optional continuation context, and we show how to construct GRO e-variables for general testing problems with composite null and alternative, emphasizing models with nuisance parameters. GRO e-values take the form of Bayes factors with special priors. We illustrate the theory using several classic examples including a one-sample safe t-test and the 2 x 2 contingency table. Sharing Fisherian, Neymanian and Jeffreys-Bayesian interpretations, e-values may provide a methodology acceptable to adherents of all three schools.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Methodology}
}

@misc{grunwaldSafeTesting2023a,
  title = {Safe {{Testing}}},
  author = {Gr{\"u}nwald, Peter and {de Heide}, Rianne and Koolen, Wouter},
  year = {2023},
  month = mar,
  number = {arXiv:1906.07801},
  eprint = {1906.07801},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-12-12},
  abstract = {We develop the theory of hypothesis testing based on the e-value, a notion of evidence that, unlike the p-value, allows for effortlessly combining results from several studies in the common scenario where the decision to perform a new study may depend on previous outcomes. Tests based on e-values are safe, i.e. they preserve Type-I error guarantees, under such optional continuation. We define growth-rate optimality (GRO) as an analogue of power in an optional continuation context, and we show how to construct GRO e-variables for general testing problems with composite null and alternative, emphasizing models with nuisance parameters. GRO e-values take the form of Bayes factors with special priors. We illustrate the theory using several classic examples including a one-sample safe t-test and the 2 {\texttimes} 2 contingency table. Sharing Fisherian, Neymanian and Jeffreys-Bayesian interpretations, e-values may provide a methodology acceptable to adherents of all three schools.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Methodology}
}

@article{halabiCombinatorialPenaltiesWhich2018,
  title = {Combinatorial {{Penalties}}: {{Which}} Structures Are Preserved by Convex Relaxations?},
  shorttitle = {Combinatorial {{Penalties}}},
  author = {Halabi, Marwa El and Bach, Francis and Cevher, Volkan},
  year = {2018},
  month = mar,
  journal = {arXiv:1710.06273 [cs, stat]},
  eprint = {1710.06273},
  primaryclass = {cs, stat},
  urldate = {2021-04-19},
  abstract = {We consider the homogeneous and the nonhomogeneous convex relaxations for combinatorial penalty functions defined on support sets. Our study identifies key differences in the tightness of the resulting relaxations through the notion of the lower combinatorial envelope of a setfunction along with new necessary conditions for support identification. We then propose a general adaptive estimator for convex monotone regularizers, and derive new sufficient conditions for support recovery in the asymptotic setting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{hanClassDimensionalityfreeMetrics2021,
  title = {A {{Class}} of {{Dimensionality-free Metrics}} for the {{Convergence}} of {{Empirical Measures}}},
  author = {Han, Jiequn and Hu, Ruimeng and Long, Jihao},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.12036 [cs, math, stat]},
  eprint = {2104.12036},
  primaryclass = {cs, math, stat},
  urldate = {2021-05-13},
  abstract = {This paper concerns the convergence of empirical measures in high dimensions. We propose a new class of metrics and show that under such metrics, the convergence is free of the curse of dimensionality (CoD). Such a feature is critical for high-dimensional analysis and stands in contrast to classical metrics (\{{\textbackslash}it e.g.\}, the Wasserstein distance). The proposed metrics originate from the maximum mean discrepancy, which we generalize by proposing specific criteria for selecting test function spaces to guarantee the property of being free of CoD. Therefore, we call this class of metrics the generalized maximum mean discrepancy (GMMD). Examples of the selected test function spaces include the reproducing kernel Hilbert space, Barron space, and flow-induced function spaces. Three applications of the proposed metrics are presented: 1. The convergence of empirical measure in the case of random variables; 2. The convergence of \$n\$-particle system to the solution to McKean-Vlasov stochastic differential equation; 3. The construction of an \${\textbackslash}varepsilon\$-Nash equilibrium for a homogeneous \$n\$-player game by its mean-field limit. As a byproduct, we prove that, given a distribution close to the target distribution measured by GMMD and a certain representation of the target distribution, we can generate a distribution close to the target one in terms of the Wasserstein distance and relative entropy. Overall, we show that the proposed class of metrics is a powerful tool to analyze the convergence of empirical measures in high dimensions without CoD.},
  archiveprefix = {arXiv},
  keywords = {60B10 60E15 60K35 91A16 60H10,Computer Science - Machine Learning,Mathematics - Probability,Statistics - Machine Learning}
}

@misc{haoEvaluesKSampleTests2024,
  title = {E-Values for k-{{Sample Tests With Exponential Families}}},
  author = {Hao, Yunda and Gr{\"u}nwald, Peter and Lardy, Tyron and Long, Long and Adams, Reuben},
  year = {2024},
  month = jan,
  number = {arXiv:2303.00471},
  eprint = {2303.00471},
  primaryclass = {math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.00471},
  urldate = {2024-09-04},
  abstract = {We develop and compare e-variables for testing whether \$k\$ samples of data are drawn from the same distribution, the alternative being that they come from different elements of an exponential family. We consider the GRO (growth-rate optimal) e-variables for (1) a `small' null inside the same exponential family, and (2) a `large' nonparametric null, as well as (3) an e-variable arrived at by conditioning on the sum of the sufficient statistics. (2) and (3) are efficiently computable, and extend ideas from Turner et al. [2021] and Wald [1947] respectively from Bernoulli to general exponential families. We provide theoretical and simulation-based comparisons of these e-variables in terms of their logarithmic growth rate, and find that for small effects all four e-variables behave surprisingly similarly; for the Gaussian location and Poisson families, e-variables (1) and (3) coincide; for Bernoulli, (1) and (2) coincide; but in general, whether (2) or (3) grows faster under the alternative is family-dependent. We furthermore discuss algorithms for numerically approximating (1).},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology}
}

@misc{heissHowImplicitRegularization2021,
  title = {How {{Implicit Regularization}} of {{ReLU Neural Networks Characterizes}} the {{Learned Function}} -- {{Part I}}: The 1-{{D Case}} of {{Two Layers}} with {{Random First Layer}}},
  shorttitle = {How {{Implicit Regularization}} of {{ReLU Neural Networks Characterizes}} the {{Learned Function}} -- {{Part I}}},
  author = {Heiss, Jakob and Teichmann, Josef and Wutte, Hanna},
  year = {2021},
  month = jul,
  number = {arXiv:1911.02903},
  eprint = {1911.02903},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2023-01-30},
  abstract = {Today, various forms of neural networks are trained to perform approximation tasks in many fields. However, the estimates obtained are not fully understood on function space. Empirical results suggest that typical training algorithms favor regularized solutions. These observations motivate us to analyze properties of the neural networks found by gradient descent initialized close to zero, that is frequently employed to perform the training task. As a starting point, we consider one dimensional (shallow) ReLU neural networks in which weights are chosen randomly and only the terminal layer is trained. First, we rigorously show that for such networks ridge regularized regression corresponds in function space to regularizing the estimate's second derivative for fairly general loss functionals. For least squares regression, we show that the trained network converges to the smooth spline interpolation of the training data as the number of hidden nodes tends to infinity. Moreover, we derive a correspondence between the early stopped gradient descent and the smoothing spline regression. Our analysis might give valuable insight on the properties of the solutions obtained using gradient descent methods in general settings.},
  archiveprefix = {arXiv},
  keywords = {41Axx 93Exx 68T05 68Q32,Computer Science - Machine Learning,G.3,I.2.6,Mathematics - Numerical Analysis,Statistics - Machine Learning}
}

@article{hendriksenOptionalStoppingBayes2021,
  title = {Optional {{Stopping}} with {{Bayes Factors}}: {{A Categorization}} and {{Extension}} of {{Folklore Results}}, with an {{Application}} to {{Invariant Situations}}},
  shorttitle = {Optional {{Stopping}} with {{Bayes Factors}}},
  author = {Hendriksen, Allard and De Heide, Rianne and Gr{\"u}nwald, Peter},
  year = {2021},
  month = sep,
  journal = {Bayesian Analysis},
  volume = {16},
  number = {3},
  issn = {1936-0975},
  doi = {10.1214/20-BA1234},
  urldate = {2024-08-13},
  abstract = {It is often claimed that Bayesian methods, in particular Bayes factor methods for hypothesis testing, can deal with optional stopping. We first give an overview, using elementary probability theory, of three different mathematical meanings that various authors give to this claim: (1) stopping rule independence, (2) posterior calibration and (3) (semi-) frequentist robustness to optional stopping. We then prove theorems to the effect that these claims do indeed hold in a general measure-theoretic setting. For claims of type (2) and (3), such results are new. By allowing for non-integrable measures based on improper priors, we obtain particularly strong results for the practically important case of models with nuisance parameters satisfying a group invariance (such as location or scale). We also discuss the practical relevance of (1)--(3), and conclude that whether Bayes factor methods actually perform well under optional stopping crucially depends on details of models, priors and the goal of the analysis.},
  langid = {english}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer {{Feedforward Networks}} Are {{Universal Approximators}}},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {08936080},
  doi = {10.1016/0893-6080(89)90020-8},
  urldate = {2022-10-11},
  langid = {english},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation}
}

@misc{hunterSpaces2011,
  author = {Hunter, John K.},
  year = {2011},
  journal = {\$L{\textasciicircum}p\$ spaces},
  urldate = {2023-03-30},
  howpublished = {https://www.math.ucdavis.edu/{\textasciitilde}hunter/measure\_theory/measure\_notes\_ch7.pdf},
  title = {{$L^p$} Spaces}
}

@inproceedings{irieCapabilitiesThreelayeredPerceptrons1988,
  title = {Capabilities of {{Three-layered Perceptrons}}},
  booktitle = {{{IEEE}} 1988 {{International Conference}} on {{Neural Networks}}},
  author = {{Irie} and {Miyake}},
  year = {1988},
  month = jul,
  pages = {641-648 vol.1},
  doi = {10.1109/ICNN.1988.23901},
  abstract = {A theorem is proved to the effect that three-layered perceptrons with an infinite number of computing units can represent arbitrary mapping if the desired mapping and the input-output characteristics of the computing units satisfy some constraints. The proof is constructive, and each coefficient is explicitly presented. The theorem theoretically guarantees a kind of universality for three-layered perceptrons. Although two-layered perceptrons (simple perceptrons) cannot represent arbitrary functions, three-layers prove necessary and sufficient. The relationship between the model used in the proof and the distributed storage and processing of information is also discussed.{$<>$}},
  keywords = {Neural networks}
}

@misc{javanmardAnalysisTwoLayerNeural2019,
  title = {Analysis of a {{Two-Layer Neural Network}} via {{Displacement Convexity}}},
  author = {Javanmard, Adel and Mondelli, Marco and Montanari, Andrea},
  year = {2019},
  month = aug,
  number = {arXiv:1901.01375},
  eprint = {1901.01375},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2022-06-23},
  abstract = {Fitting a function by using linear combinations of a large number \$N\$ of `simple' components is one of the most fruitful ideas in statistical learning. This idea lies at the core of a variety of methods, from two-layer neural networks to kernel regression, to boosting. In general, the resulting risk minimization problem is non-convex and is solved by gradient descent or its variants. Unfortunately, little is known about global convergence properties of these approaches. Here we consider the problem of learning a concave function \$f\$ on a compact convex domain \${\textbackslash}Omega{\textbackslash}subseteq \{{\textbackslash}mathbb R\}{\textasciicircum}d\$, using linear combinations of `bump-like' components (neurons). The parameters to be fitted are the centers of \$N\$ bumps, and the resulting empirical risk minimization problem is highly non-convex. We prove that, in the limit in which the number of neurons diverges, the evolution of gradient descent converges to a Wasserstein gradient flow in the space of probability distributions over \${\textbackslash}Omega\$. Further, when the bump width \${\textbackslash}delta\$ tends to \$0\$, this gradient flow has a limit which is a viscous porous medium equation. Remarkably, the cost function optimized by this gradient flow exhibits a special property known as displacement convexity, which implies exponential convergence rates for \$N{\textbackslash}to{\textbackslash}infty\$, \${\textbackslash}delta{\textbackslash}to 0\$. Surprisingly, this asymptotic theory appears to capture well the behavior for moderate values of \${\textbackslash}delta, N\$. Explaining this phenomenon, and understanding the dependence on \${\textbackslash}delta,N\$ in a quantitative manner remains an outstanding challenge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory}
}

@misc{jinLogicalFallacyDetection2022,
  title = {Logical {{Fallacy Detection}}},
  author = {Jin, Zhijing and Lalwani, Abhinav and Vaidhya, Tejas and Shen, Xiaoyu and Ding, Yiwen and Lyu, Zhiheng and Sachan, Mrinmaya and Mihalcea, Rada and Sch{\"o}lkopf, Bernhard},
  year = {2022},
  month = may,
  number = {arXiv:2202.13758},
  eprint = {2202.13758},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-09-28},
  abstract = {Reasoning is central to human intelligence. However, fallacious arguments are common, and some exacerbate problems such as spreading misinformation about climate change. In this paper, we propose the task of logical fallacy detection, and provide a new dataset (Logic) of logical fallacies generally found in text, together with an additional challenge set for detecting logical fallacies in climate change claims (LogicClimate). Detecting logical fallacies is a hard problem as the model must understand the underlying logical structure of the argument. We find that existing pretrained large language models perform poorly on this task. In contrast, we show that a simple structure-aware classifier outperforms the best language model by 5.46\% on Logic and 4.51\% on LogicClimate. We encourage future work to explore this task as (a) it can serve as a new reasoning challenge for language models, and (b) it can have potential applications in tackling the spread of misinformation. Our dataset and code are available at https://github.com/causalNLP/logical-fallacy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Logic in Computer Science,Computer Science - Machine Learning}
}

@article{jonesSimpleLemmaGreedy1992,
  title = {A {{Simple Lemma}} on {{Greedy Approximation}} in {{Hilbert Space}} and {{Convergence Rates}} for {{Projection Pursuit Regression}} and {{Neural Network Training}}},
  author = {Jones, Lee K.},
  year = {1992},
  journal = {The Annals of Statistics},
  volume = {20},
  number = {1},
  eprint = {2242184},
  eprinttype = {jstor},
  pages = {608--613},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364},
  urldate = {2023-03-02},
  abstract = {A general convergence criterion for certain iterative sequences in Hilbert space is presented. For an important subclass of these sequences, estimates of the rate of convergence are given. Under very mild assumptions these results establish an \$O(1/ {\textbackslash}sqrt n)\$ nonsampling convergence rate for projection pursuit regression and neural network training; where n represents the number of ridge functions, neurons or coefficients in a greedy basis expansion.}
}

@article{juppApproximationDataSplines1978,
  title = {Approximation to {{Data}} by {{Splines}} with {{Free Knots}}},
  author = {Jupp, David L. B.},
  year = {1978},
  month = apr,
  journal = {SIAM Journal on Numerical Analysis},
  volume = {15},
  number = {2},
  pages = {328--343},
  issn = {0036-1429, 1095-7170},
  doi = {10.1137/0715022},
  urldate = {2023-02-27},
  abstract = {Approximations to data by splines improve greatly if the knots are free variables. Using the B-spline representation for splines, and separating the linear and nonlinear aspects, the approximation problem reduces to nonlinear least squares in the variable knots.},
  langid = {english}
}

@misc{klusowskiApproximationCombinationsReLU2018,
  author = {Klusowski, Jason M. and Barron, Andrew R.},
  year = {2018},
  month = may,
  number = {arXiv:1607.07819},
  eprint = {1607.07819},
  primaryclass = {math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1607.07819},
  urldate = {2023-03-30},
  abstract = {We establish \$ L{\textasciicircum}\{{\textbackslash}infty\} \$ and \$ L{\textasciicircum}2 \$ error bounds for functions of many variables that are approximated by linear combinations of ReLU (rectified linear unit) and squared ReLU ridge functions with \$ {\textbackslash}ell{\textasciicircum}1 \$ and \$ {\textbackslash}ell{\textasciicircum}0 \$ controls on their inner and outer parameters. With the squared ReLU ridge function, we show that the \$ L{\textasciicircum}2 \$ approximation error is inversely proportional to the inner layer \$ {\textbackslash}ell{\textasciicircum}0 \$ sparsity and it need only be sublinear in the outer layer \$ {\textbackslash}ell{\textasciicircum}0 \$ sparsity. Our constructions are obtained using a variant of the Jones-Barron probabilistic method, which can be interpreted as either stratified sampling with proportionate allocation or two-stage cluster sampling. We also provide companion error lower bounds that reveal near optimality of our constructions. Despite the sparsity assumptions, we showcase the richness and flexibility of these ridge combinations by defining a large family of functions, in terms of certain spectral conditions, that are particularly well approximated by them.},
  archiveprefix = {arXiv},
  keywords = {62M45 41A15,Mathematics - Statistics Theory,Statistics - Machine Learning},
  title = {Approximation by {{Combinations}} of {{ReLU}} and {{Squared ReLU Ridge Functions}} with {$ \ell^1 $} and {$ \ell^0 $} {{Controls}}}
}

@article{klusowskiApproximationCombinationsReLU2018a,
  author = {Klusowski, Jason M. and Barron, Andrew R.},
  year = {2018},
  month = dec,
  journal = {IEEE Transactions on Information Theory},
  volume = {64},
  number = {12},
  pages = {7649--7656},
  issn = {1557-9654},
  doi = {10.1109/TIT.2018.2874447},
  abstract = {We establish L{$\infty$} and L2 error bounds for functions of many variables that are approximated by linear combinations of rectified linear unit (ReLU) and squared ReLU ridge functions with {$\ell$}1 and {$\ell$}0 controls on their inner and outer parameters. With the squared ReLU ridge function, we show that the L2 approximation error is inversely proportional to the inner layer {$\ell$}0 sparsity and it need only be sublinear in the outer layer {$\ell$}0 sparsity. Our constructions are obtained using a variant of the Maurey-Jones-Barron probabilistic method, which can be interpreted as either stratified sampling with proportionate allocation or two-stage cluster sampling. We also provide companion error lower bounds that reveal near optimality of our constructions. Despite the sparsity assumptions, we showcase the richness and flexibility of these ridge combinations by defining a large family of functions, in terms of certain spectral conditions, that are particularly well approximated by them.},
  keywords = {approximation error,Approximation error,Frequency modulation,Probabilistic logic,rectified linear unit,Resource management,Ridge combinations,Sociology,sparse models,spline,Statistics,stratified sampling,Upper bound},
  title = {Approximation by {{Combinations}} of {{ReLU}} and {{Squared ReLU Ridge Functions With}} {$\ell^1$} and {$\ell^0$} {{Controls}}}
}

@misc{klusowskiMinimaxLowerBounds2017,
  title = {Minimax {{Lower Bounds}} for {{Ridge Combinations Including Neural Nets}}},
  author = {Klusowski, Jason M. and Barron, Andrew R.},
  year = {2017},
  month = feb,
  number = {arXiv:1702.02828},
  eprint = {1702.02828},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1702.02828},
  urldate = {2022-11-22},
  abstract = {Estimation of functions of \$ d \$ variables is considered using ridge combinations of the form \$ {\textbackslash}textstyle{\textbackslash}sum\_\{k=1\}{\textasciicircum}m c\_\{1,k\} {\textbackslash}phi({\textbackslash}textstyle{\textbackslash}sum\_\{j=1\}{\textasciicircum}d c\_\{0,j,k\}x\_j-b\_k) \$ where the activation function \$ {\textbackslash}phi \$ is a function with bounded value and derivative. These include single-hidden layer neural networks, polynomials, and sinusoidal models. From a sample of size \$ n \$ of possibly noisy values at random sites \$ X {\textbackslash}in B = [-1,1]{\textasciicircum}d \$, the minimax mean square error is examined for functions in the closure of the \$ {\textbackslash}ell\_1 \$ hull of ridge functions with activation \$ {\textbackslash}phi \$. It is shown to be of order \$ d/n \$ to a fractional power (when \$ d \$ is of smaller order than \$ n \$), and to be of order \$ ({\textbackslash}log d)/n \$ to a fractional power (when \$ d \$ is of larger order than \$ n \$). Dependence on constraints \$ v\_0 \$ and \$ v\_1 \$ on the \$ {\textbackslash}ell\_1 \$ norms of inner parameter \$ c\_0 \$ and outer parameter \$ c\_1 \$, respectively, is also examined. Also, lower and upper bounds on the fractional power are given. The heart of the analysis is development of information-theoretic packing numbers for these classes of functions.},
  archiveprefix = {arXiv},
  keywords = {62J02 62G08 68T05,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{klusowskiRiskBoundsHighdimensional2018a,
  title = {Risk {{Bounds}} for {{High-dimensional Ridge Function Combinations Including Neural Networks}}},
  author = {Klusowski, Jason M. and Barron, Andrew R.},
  year = {2018},
  month = oct,
  journal = {arXiv:1607.01434 [math, stat]},
  eprint = {1607.01434},
  primaryclass = {math, stat},
  urldate = {2021-04-19},
  abstract = {Let \$ f{\textasciicircum}\{{\textbackslash}star\} \$ be a function on \$ {\textbackslash}mathbb\{R\}{\textasciicircum}d \$ with an assumption of a spectral norm \$ v\_\{f{\textasciicircum}\{{\textbackslash}star\}\} \$. For various noise settings, we show that \$ {\textbackslash}mathbb\{E\}{\textbackslash}{\textbar}{\textbackslash}hat\{f\} - f{\textasciicircum}\{{\textbackslash}star\} {\textbackslash}{\textbar}{\textasciicircum}2 {\textbackslash}leq {\textbackslash}left(v{\textasciicircum}4\_\{f{\textasciicircum}\{{\textbackslash}star\}\}{\textbackslash}frac\{{\textbackslash}log d\}\{n\}{\textbackslash}right){\textasciicircum}\{1/3\} \$, where \$ n \$ is the sample size and \$ {\textbackslash}hat\{f\} \$ is either a penalized least squares estimator or a greedily obtained version of such using linear combinations of sinusoidal, sigmoidal, ramp, ramp-squared or other smooth ridge functions. The candidate fits may be chosen from a continuum of functions, thus avoiding the rigidity of discretizations of the parameter space. On the other hand, if the candidate fits are chosen from a discretization, we show that \$ {\textbackslash}mathbb\{E\}{\textbackslash}{\textbar}{\textbackslash}hat\{f\} - f{\textasciicircum}\{{\textbackslash}star\} {\textbackslash}{\textbar}{\textasciicircum}2 {\textbackslash}leq {\textbackslash}left(v{\textasciicircum}3\_\{f{\textasciicircum}\{{\textbackslash}star\}\}{\textbackslash}frac\{{\textbackslash}log d\}\{n\}{\textbackslash}right){\textasciicircum}\{2/5\} \$. This work bridges non-linear and non-parametric function estimation and includes single-hidden layer nets. Unlike past theory for such settings, our bound shows that the risk is small even when the input dimension \$ d \$ of an infinite-dimensional parameterized dictionary is much larger than the available sample size. When the dimension is larger than the cube root of the sample size, this quantity is seen to improve the more familiar risk bound of \$ v\_\{f{\textasciicircum}\{{\textbackslash}star\}\}{\textbackslash}left({\textbackslash}frac\{d{\textbackslash}log (n/d)\}\{n\}{\textbackslash}right){\textasciicircum}\{1/2\} \$, also investigated here.},
  archiveprefix = {arXiv},
  keywords = {62J02 62G08 68T05,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@misc{kuboImplicitRegularizationOverparameterized2019,
  title = {Implicit {{Regularization}} in {{Over-parameterized Neural Networks}}},
  author = {Kubo, Masayoshi and Banno, Ryotaro and Manabe, Hidetaka and Minoji, Masataka},
  year = {2019},
  month = mar,
  number = {arXiv:1903.01997},
  eprint = {1903.01997},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1903.01997},
  urldate = {2023-02-20},
  abstract = {Over-parameterized neural networks generalize well in practice without any explicit regularization. Although it has not been proven yet, empirical evidence suggests that implicit regularization plays a crucial role in deep learning and prevents the network from overfitting. In this work, we introduce the gradient gap deviation and the gradient deflection as statistical measures corresponding to the network curvature and the Hessian matrix to analyze variations of network derivatives with respect to input parameters, and investigate how implicit regularization works in ReLU neural networks from both theoretical and empirical perspectives. Our result reveals that the network output between each pair of input samples is properly controlled by random initialization and stochastic gradient descent to keep interpolating between samples almost straight, which results in low complexity of over-parameterized neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{kundurMagnitudePhase,
  title = {Magnitude and {{Phase}}},
  author = {Kundur, Professor Deepa},
  pages = {5},
  langid = {english}
}

@article{kurkovaBoundsRatesVariablebasis2001,
  title = {Bounds on Rates of Variable-Basis and Neural-Network Approximation},
  author = {Kurkova, V. and Sanguineti, M.},
  year = {2001},
  month = sep,
  journal = {IEEE Transactions on Information Theory},
  volume = {47},
  number = {6},
  pages = {2659--2665},
  issn = {00189448},
  doi = {10.1109/18.945285},
  urldate = {2023-03-02},
  abstract = {Tightness of bounds on rates of approximation by feedforward neural networks is investigated in a more general context of nonlinear approximation by variable-basis functions. Tight bounds on the worst case error in approximation by linear combinations of elements of an orthonormal variable basis are derived.},
  langid = {english}
}

@article{kurkovaComparisonWorstCase2002,
  title = {Comparison of Worst Case Errors in Linear and Neural Network Approximation},
  author = {Kurkova, V. and Sanguineti, M.},
  year = {2002},
  month = jan,
  journal = {IEEE Transactions on Information Theory},
  volume = {48},
  number = {1},
  pages = {264--275},
  issn = {1557-9654},
  doi = {10.1109/18.971754},
  abstract = {Sets of multivariable functions are described for which worst case errors in linear approximation are larger than those in approximation by neural networks. A theoretical framework for such a description is developed in the context of nonlinear approximation by fixed versus variable basis functions. Comparisons of approximation rates are formulated in terms of certain norms tailored to sets of basis functions. The results are applied to perceptron networks.},
  keywords = {Approximation methods}
}

@misc{lanthalerErrorEstimatesDeepOnets2022,
  title = {Error Estimates for {{DeepOnets}}: {{A}} Deep Learning Framework in Infinite Dimensions},
  shorttitle = {Error Estimates for {{DeepOnets}}},
  author = {Lanthaler, Samuel and Mishra, Siddhartha and Karniadakis, George Em},
  year = {2022},
  month = jan,
  number = {arXiv:2102.09618},
  eprint = {2102.09618},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.09618},
  urldate = {2023-02-17},
  abstract = {DeepONets have recently been proposed as a framework for learning nonlinear operators mapping between infinite dimensional Banach spaces. We analyze DeepONets and prove estimates on the resulting approximation and generalization errors. In particular, we extend the universal approximation property of DeepONets to include measurable mappings in non-compact spaces. By a decomposition of the error into encoding, approximation and reconstruction errors, we prove both lower and upper bounds on the total error, relating it to the spectral decay properties of the covariance operators, associated with the underlying measures. We derive almost optimal error bounds with very general affine reconstructors and with random sensor locations as well as bounds on the generalization error, using covering number arguments. We illustrate our general framework with four prototypical examples of nonlinear operators, namely those arising in a nonlinear forced ODE, an elliptic PDE with variable coefficients and nonlinear parabolic and hyperbolic PDEs. While the approximation of arbitrary Lipschitz operators by DeepONets to accuracy \${\textbackslash}epsilon\$ is argued to suffer from a "curse of dimensionality" (requiring a neural networks of exponential size in \$1/{\textbackslash}epsilon\$), in contrast, for all the above concrete examples of interest, we rigorously prove that DeepONets can break this curse of dimensionality (achieving accuracy \${\textbackslash}epsilon\$ with neural networks of size that can grow algebraically in \$1/{\textbackslash}epsilon\$). Thus, we demonstrate the efficient approximation of a potentially large class of operators with this machine learning framework.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Numerical Analysis}
}

@article{lardyReverseInformationProjections2024,
  title = {Reverse {{Information Projections}} and {{Optimal E-Statistics}}},
  author = {Lardy, Tyron and Gr{\"u}nwald, Peter and Harremo{\"e}s, Peter},
  year = {2024},
  month = nov,
  journal = {IEEE Transactions on Information Theory},
  volume = {70},
  number = {11},
  pages = {7616--7631},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/TIT.2024.3444458},
  urldate = {2024-12-12},
  abstract = {Information projections have found important applications in probability theory, statistics, and related areas. In the field of hypothesis testing in particular, the reverse information projection (RIPr) has recently been shown to lead to growth-rate optimal (GRO) e-statistics for testing simple alternatives against composite null hypotheses. However, the RIPr as well as the GRO criterion are undefined whenever the infimum information divergence between the null and alternative is infinite. We show that in such scenarios, under some assumptions, there still exists a measure in the null that is closest to the alternative in a specific sense. Whenever the information divergence is finite, this measure coincides with the usual RIPr. It therefore gives a natural extension of the RIPr to certain cases where the latter was previously not defined. This extended notion of the RIPr is shown to lead to optimal e-statistics in a sense that is a novel, but natural, extension of the GRO criterion. We also give conditions under which the (extension of the) RIPr is a strict sub-probability measure, as well as conditions under which an approximation of the RIPr leads to approximate e-statistics. For this case we provide tight relations between the corresponding approximation rates.},
  langid = {english}
}

@misc{leeAbilityNeuralNets2021,
  title = {On the Ability of Neural Nets to Express Distributions},
  author = {Lee, Holden and Ge, Rong and Ma, Tengyu and Risteski, Andrej and Arora, Sanjeev},
  year = {2021},
  month = apr,
  number = {arXiv:1702.07028},
  eprint = {1702.07028},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-10-11},
  abstract = {Deep neural nets have caused a revolution in many classification tasks. A related ongoing revolution -- also theoretically not understood -- concerns their ability to serve as generative models for complicated types of data such as images and texts. These models are trained using ideas like variational autoencoders and Generative Adversarial Networks. We take a first cut at explaining the expressivity of multilayer nets by giving a sufficient criterion for a function to be approximable by a neural network with \$n\$ hidden layers. A key ingredient is Barron's Theorem {\textbackslash}cite\{Barron1993\}, which gives a Fourier criterion for approximability of a function by a neural network with 1 hidden layer. We show that a composition of \$n\$ functions which satisfy certain Fourier conditions ("Barron functions") can be approximated by a \$n+1\$-layer neural network. For probability distributions, this translates into a criterion for a probability distribution to be approximable in Wasserstein distance -- a natural metric on probability distributions -- by a neural network applied to a fixed base distribution (e.g., multivariate gaussian). Building up recent lower bound work, we also give an example function that shows that composition of Barron functions is more expressive than Barron functions alone.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{leshnoMultilayerFeedforwardNetworks1993,
  title = {Multilayer {{Feedforward Networks}} with {{Non-Polynomial Activation Function Can Approximate Any Function}}},
  author = {Leshno, Moshe and Lin, Vladimir Ya. and Pinkus, Allan and Schocken, Shimon},
  year = {1993},
  month = jan,
  journal = {Neural Networks},
  volume = {6},
  number = {6},
  pages = {861--867},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(05)80131-5},
  urldate = {2023-03-20},
  abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold.},
  langid = {english},
  keywords = {() approximation,Activation functions,Multilayer feedforward networks,Role of threshold,Universal approximation capabilities}
}

@article{liangJustInterpolateKernel2019,
  title = {Just {{Interpolate}}: {{Kernel}} "{{Ridgeless}}" {{Regression Can Generalize}}},
  shorttitle = {Just {{Interpolate}}},
  author = {Liang, Tengyuan and Rakhlin, Alexander},
  year = {2019},
  month = feb,
  journal = {arXiv:1808.00387 [cs, math, stat]},
  eprint = {1808.00387},
  primaryclass = {cs, math, stat},
  doi = {10.1214/19-AOS1849},
  urldate = {2021-04-19},
  abstract = {In the absence of explicit regularization, Kernel ``Ridgeless'' Regression with nonlinear kernels has the potential to fit the training data perfectly. It has been observed empirically, however, that such interpolated solutions can still generalize well on test data. We isolate a phenomenon of implicit regularization for minimum-norm interpolated solutions which is due to a combination of high dimensionality of the input data, curvature of the kernel function, and favorable geometric properties of the data such as an eigenvalue decay of the empirical covariance and kernel matrices. In addition to deriving a data-dependent upper bound on the out-of-sample error, we present experimental evidence suggesting that the phenomenon occurs in the MNIST dataset.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@misc{liComplexityMeasuresNeural2020,
  title = {Complexity {{Measures}} for {{Neural Networks}} with {{General Activation Functions Using Path-based Norms}}},
  author = {Li, Zhong and Ma, Chao and Wu, Lei},
  year = {2020},
  month = sep,
  number = {arXiv:2009.06132},
  eprint = {2009.06132},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.06132},
  urldate = {2022-12-01},
  abstract = {A simple approach is proposed to obtain complexity controls for neural networks with general activation functions. The approach is motivated by approximating the general activation functions with one-dimensional ReLU networks, which reduces the problem to the complexity controls of ReLU networks. Specifically, we consider two-layer networks and deep residual networks, for which path-based norms are derived to control complexities. We also provide preliminary analyses of the function spaces induced by these norms and a priori estimates of the corresponding regularized estimators.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{liebElementaryProofTheorem2006,
  title = {Elementary {{Proof}} of a {{Theorem}} of {{Jean Ville}}},
  author = {Lieb, Elliott H. and Osherson, Daniel and Weinstein, Scott},
  year = {2006},
  month = jul,
  number = {arXiv:cs/0607054},
  eprint = {cs/0607054},
  publisher = {arXiv},
  doi = {10.48550/arXiv.cs/0607054},
  urldate = {2024-08-17},
  abstract = {Considerable thought has been devoted to an adequate definition of the class of infinite, random binary sequences (the sort of sequence that almost certainly arises from flipping a fair coin indefinitely). The first mathematical exploration of this problem was due to R. Von Mises, and based on his concept of a "selection function." A decisive objection to Von Mises' idea was formulated in a theorem offered by Jean Ville in 1939. It shows that some sequences admitted by Von Mises as "random" in fact manifest a certain kind of systematicity. Ville's proof is challenging, and an alternative approach has appeared only in condensed form. We attempt to provide an expanded version of the latter, alternative argument.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity}
}

@misc{liLearningOverparameterizedNeural2019,
  title = {Learning {{Overparameterized Neural Networks}} via {{Stochastic Gradient Descent}} on {{Structured Data}}},
  author = {Li, Yuanzhi and Liang, Yingyu},
  year = {2019},
  month = aug,
  number = {arXiv:1808.01204},
  eprint = {1808.01204},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1808.01204},
  urldate = {2023-02-20},
  abstract = {Neural networks have many successful applications, while much less theoretical understanding has been gained. Towards bridging this gap, we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting, when the data comes from mixtures of well-separated distributions, we prove that SGD learns a network with a small generalization error, albeit the network has enough capacity to fit arbitrary labels. Furthermore, the analysis provides interesting insights into several aspects of learning neural networks and can be verified based on empirical studies on synthetic data and on the MNIST dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{liuDeepNonparametricEstimation2022,
  title = {Deep {{Nonparametric Estimation}} of {{Operators}} between {{Infinite Dimensional Spaces}}},
  author = {Liu, Hao and Yang, Haizhao and Chen, Minshuo and Zhao, Tuo and Liao, Wenjing},
  year = {2022},
  month = jan,
  number = {arXiv:2201.00217},
  eprint = {2201.00217},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.00217},
  urldate = {2023-02-17},
  abstract = {Learning operators between infinitely dimensional spaces is an important learning task arising in wide applications in machine learning, imaging science, mathematical modeling and simulations, etc. This paper studies the nonparametric estimation of Lipschitz operators using deep neural networks. Non-asymptotic upper bounds are derived for the generalization error of the empirical risk minimizer over a properly chosen network class. Under the assumption that the target operator exhibits a low dimensional structure, our error bounds decay as the training sample size increases, with an attractive fast rate depending on the intrinsic dimension in our estimation. Our assumptions cover most scenarios in real applications and our results give rise to fast rates by exploiting low dimensional structures of data in operator estimation. We also investigate the influence of network structures (e.g., network width, depth, and sparsity) on the generalization error of the neural network estimator and propose a general suggestion on the choice of network structures to maximize the learning efficiency quantitatively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{liUnprecedentedGenomicDiversity2015,
  title = {Unprecedented Genomic Diversity of {{RNA}} Viruses in Arthropods Reveals the Ancestry of Negative-Sense {{RNA}} Viruses},
  author = {Li, Ci-Xiu and Shi, Mang and Tian, Jun-Hua and Lin, Xian-Dan and Kang, Yan-Jun and Chen, Liang-Jun and Qin, Xin-Cheng and Xu, Jianguo and Holmes, Edward C. and Zhang, Yong-Zhen},
  year = {2015},
  month = jan,
  journal = {eLife},
  volume = {4},
  issn = {2050-084X},
  doi = {10.7554/eLife.05378},
  abstract = {Although arthropods are important viral vectors, the biodiversity of arthropod viruses, as well as the role that arthropods have played in viral origins and evolution, is unclear. Through RNA sequencing of 70 arthropod species we discovered 112 novel viruses that appear to be ancestral to much of the documented genetic diversity of negative-sense RNA viruses, a number of which are also present as endogenous genomic copies. With this greatly enriched diversity we revealed that arthropods contain viruses that fall basal to major virus groups, including the vertebrate-specific arenaviruses, filoviruses, hantaviruses, influenza viruses, lyssaviruses, and paramyxoviruses. We similarly documented a remarkable diversity of genome structures in arthropod viruses, including a putative circular form, that sheds new light on the evolution of genome organization. Hence, arthropods are a major reservoir of viral genetic diversity and have likely been central to viral evolution.},
  langid = {english},
  pmcid = {PMC4384744},
  pmid = {25633976},
  keywords = {Animals,arthropods,Arthropods,Biodiversity,evolution,Evolution Molecular,Genome,infectious disease,microbiology,negative-sense,phylogeny,Phylogeny,RNA virus,RNA Viruses,segmentation,viruses}
}

@misc{maennelGradientDescentQuantizes2018,
  title = {Gradient {{Descent Quantizes ReLU Network Features}}},
  author = {Maennel, Hartmut and Bousquet, Olivier and Gelly, Sylvain},
  year = {2018},
  month = mar,
  number = {arXiv:1803.08367},
  eprint = {1803.08367},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.08367},
  urldate = {2023-02-20},
  abstract = {Deep neural networks are often trained in the over-parametrized regime (i.e. with far more parameters than training examples), and understanding why the training converges to solutions that generalize remains an open problem. Several studies have highlighted the fact that the training procedure, i.e. mini-batch Stochastic Gradient Descent (SGD) leads to solutions that have specific properties in the loss landscape. However, even with plain Gradient Descent (GD) the solutions found in the over-parametrized regime are pretty good and this phenomenon is poorly understood. We propose an analysis of this behavior for feedforward networks with a ReLU activation function under the assumption of small initialization and learning rate and uncover a quantization effect: The weight vectors tend to concentrate at a small number of directions determined by the input data. As a consequence, we show that for given input data there are only finitely many, "simple" functions that can be obtained, independent of the network size. This puts these functions in analogy to linear interpolations (for given input data there are finitely many triangulations, which each determine a function by linear interpolation). We ask whether this analogy extends to the generalization properties - while the usual distribution-independent generalization property does not hold, it could be that for e.g. smooth functions with bounded second derivative an approximation property holds which could "explain" generalization of networks (of unbounded size) to unseen inputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{maFunctionSpaceTheory2020,
  title = {A {{Function Space Theory}} and {{Generalization Error Estimates}} for {{Neural Network Models}}},
  author = {Ma, Chao},
  year = {2020},
  langid = {english}
}

@article{makovozRandomApproximantsNeural1996,
  title = {Random {{Approximants}} and {{Neural Networks}}},
  author = {Makovoz, Y.},
  year = {1996},
  month = apr,
  journal = {Journal of Approximation Theory},
  volume = {85},
  number = {1},
  pages = {98--109},
  issn = {00219045},
  doi = {10.1006/jath.1996.0031},
  urldate = {2023-02-28},
  langid = {english}
}

@article{makovozUniformApproximationNeural1998,
  title = {Uniform {{Approximation}} by {{Neural Networks}}},
  author = {Makovoz, Y.},
  year = {1998},
  month = nov,
  journal = {Journal of Approximation Theory},
  volume = {95},
  number = {2},
  pages = {215--228},
  issn = {0021-9045},
  doi = {10.1006/jath.1997.3217},
  urldate = {2023-03-06},
  abstract = {LetD{$\subset$}Rdbe a compact set and let{$\Phi$}be a uniformly bounded set ofD{$\rightarrow$}Rfunctions. For a given real-valued functionfdefined onDand a given natural numbern, we are looking for a good uniform approximation tofof the form {$\sum$}ni=1a1{$\varphi$}i, with{$\varphi$}i{$\in\Phi$},ai{$\in$}R. Two main cases are considered: (1) whenDis a finite set and (2) when the set{$\Phi$}is formed by the functions{$\varphi$}v,b(x):=s(v{$\cdot$}x+b), wherev{$\in$}Rd,b{$\in$}R, andsis a fixedR{$\rightarrow$}Rfunction.},
  langid = {english}
}

@article{matousekDiscrepancyApproximationsBounded1993,
  title = {Discrepancy and Approximations for Bounded {{VC-dimension}}},
  author = {Matou{\v s}ek, Ji{\v r}{\'i} and Welzl, Emo and Wernisch, Lorenz},
  year = {1993},
  month = dec,
  journal = {Combinatorica},
  volume = {13},
  number = {4},
  pages = {455--466},
  issn = {1439-6912},
  doi = {10.1007/BF01303517},
  urldate = {2023-03-17},
  abstract = {Let (X, {$R$}) be a set system on ann-point setX. For a two-coloring onX, itsdiscrepancy is defined as the maximum number by which the occurrences of the two colors differ in any set in {$R$}. We show that if for anym-point subset\$\$Y {\textbackslash}subseteq X\$\$the number of distinct subsets induced by {$R$} onY is bounded byO(md) for a fixed integerd, then there is a coloring with discrepancy bounded byO(n1/2-1/2d(logn)1+1/2d). Also if any subcollection ofm sets of {$R$} partitions the points into at mostO(md) classes, then there is a coloring with discrepancy at mostO(n1/2-1/2dlogn). These bounds imply improved upper bounds on the size of {$\varepsilon$}-approximations for (X, {$R$}). All the bounds are tight up to polylogarithmic factors in the worst case. Our results allow to generalize several results of Beck bounding the discrepancy in certain geometric settings to the case when the discrepancy is taken relative to an arbitrary measure.},
  langid = {english},
  keywords = {05 A 05,05 C 65,52 C 10,52 C 99}
}

@article{matousekTightUpperBounds1995,
  title = {Tight Upper Bounds for the Discrepancy of Half-Spaces},
  author = {Matou{\v s}ek, J.},
  year = {1995},
  month = jun,
  journal = {Discrete \& Computational Geometry},
  volume = {13},
  number = {3},
  pages = {593--601},
  issn = {1432-0444},
  doi = {10.1007/BF02574066},
  urldate = {2023-03-17},
  abstract = {We show that the discrepancy of anyn-point setP in the Euclideand-space with respect to half-spaces is bounded byCdn1/2-1/2d, that is, a mapping {$\chi$}:P{$\rightarrow$}\{-1,1\} exists such that, for any half-space {$\gamma$}, {$\gamma$}, {\textbar}{$\Sigma$}p{$\in$}P{$\bigcap\gamma\chi$}(p){\textbar}{$\leq$}Cdn1/2-1/2d. In fact, the result holds for arbitrary set systems as long as theprimal shatter function isO(md). This matches known lower bounds, improving previous upper bounds by a\% MathType!MTEF!2!1!+-\% feaafiart1ev1aaatCvAUfKttLearuqr1ngBPrgarmWu51MyVXgatC\% vAUfeBSjuyZL2yd9gzLbvyNv2CaeHbd9wDYLwzYbItLDharyavP1wz\% ZbItLDhis9wBH5garqqtubsr4rNCHbGeaGqiVu0Je9sqqrpepC0xbb\% L8F4rqqrFfpeea0xe9Lq-Jc9vqaqpepm0xbba9pwe9Q8fs0-yqaqpe\% pae9pg0FirpepeKkFr0xfr-xfr-xb9adbaqaaeGaciGaaiaabeqaam\% aaeaqbaaGcbaWaaOaaaeaacyGGSbaBcqGGVbWBcqGGNbWziqGacaWF\% Ubaaleqaaaaa!4001!\$\${\textbackslash}sqrt \{{\textbackslash}log n\} \$\$factor.},
  langid = {english},
  keywords = {34th IEEE Symposium,Computational Geometry,Discrete Comput Geom,Partial Coloring,Random Coloring}
}

@article{maUniformApproximationRates2022,
  title = {Uniform Approximation Rates and Metric Entropy of Shallow Neural Networks},
  author = {Ma, Limin and Siegel, Jonathan W. and Xu, Jinchao},
  year = {2022},
  month = sep,
  journal = {Research in the Mathematical Sciences},
  volume = {9},
  number = {3},
  pages = {46},
  issn = {2522-0144, 2197-9847},
  doi = {10.1007/s40687-022-00346-y},
  urldate = {2023-03-06},
  abstract = {We study the approximation properties of the variation spaces corresponding to shallow neural networks with respect to the uniform norm. Specifically, we consider the spectral Barron space, which consists of the convex hull of decaying Fourier modes, and the convex hull of indicator functions of half-spaces, which corresponds to shallow neural networks with sigmoidal activation function. Up to logarithmic factors, we determine the metric entropy and nonlinear dictionary approximation rates for these spaces with respect to the uniform norm. Combined with previous results with respect to the L2-norm, this also gives the metric entropy up to logarithmic factors with respect to any Lp-norm with 1 {$\leq$} p {$\leq$} {$\infty$}. In addition, we study the approximation rates for high-order spectral Barron spaces using shallow neural networks with ReLUk activation function. Specifically, we show that for a sufficiently high-order spectral Barron space, ReLUk networks are able to achieve an approximation rate of n-(k+1) with respect to the uniform norm.},
  langid = {english}
}

@phdthesis{meiComputationalStatisticalTheories2020,
  title = {Computational and Statistical Theories for Large-Scale Neural Networks},
  author = {Mei, Song and Montanari, Andrea and Johnstone, Iain and Ying, Lexing},
  year = {2020},
  address = {Stanford, California},
  abstract = {Deep learning methods operate in regimes that defy the traditional computational and statistical mindsets. Despite the non-convexity of empirical risks and the huge complexity of neural network architectures, stochastic gradient algorithms can often find an approximate global minimizer of the training loss and achieve small generalization error on test data. In recent years, an important research direction is to theoretically explain these observed optimization efficiency and generalization efficacy of neural network systems. This thesis tries to tackle these challenges in the model of two-layers neural networks, by analyzing its computational and statistical properties in various scaling limits. On the computational aspects, we introduce two competing theories for neural network dynamics: the mean field theory and the tangent kernel theory. These two theories characterize training dynamics of neural networks in different regimes that exhibit different behaviors. In the mean field framework, the training dynamics, in the large neuron limit, is captured by a particular non-linear partial differential equation. This characterization allows us to prove global convergence of the dynamics in certain scenarios. Comparatively, the tangent kernel theory characterizes the same dynamics in a different scaling limit and provides global convergence guarantees in more general scenarios. On the statistical aspects, we study the generalization properties of neural networks trained in the two regimes as described above. We first show that, in the high dimensional limit, neural tangent kernels are no better than polynomial regression, while neural networks trained in the mean field regime can potentially perform better. Next, we study more carefully the random features model, which is equivalent to a two-layers neural network in the kernel regime. We compute the precise asymptotics of its test error in the high dimensional limit and confirm that it exhibits an interesting double-descent curve that was observed in experiments},
  collaborator = {Stanford University},
  school = {Stanford University}
}

@article{montanariOneLectureTwolayer2018,
  title = {One Lecture on Two-Layer Neural Networks},
  author = {Montanari, Andrea},
  year = {2018},
  month = aug,
  pages = {10},
  abstract = {Notes for a lecture at the Carg`ese Summer School `Statistical Physics and Machine Learning Back Together,' August 21, 2018.},
  langid = {english}
}

@misc{mousavi-hosseiniNeuralNetworksEfficiently2022,
  title = {Neural {{Networks Efficiently Learn Low-Dimensional Representations}} with {{SGD}}},
  author = {{Mousavi-Hosseini}, Alireza and Park, Sejun and Girotti, Manuela and Mitliagkas, Ioannis and Erdogdu, Murat A.},
  year = {2022},
  month = sep,
  number = {arXiv:2209.14863},
  eprint = {2209.14863},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-02-08},
  abstract = {We study the problem of training a two-layer neural network (NN) of arbitrary width using stochastic gradient descent (SGD) where the input \${\textbackslash}boldsymbol\{x\}{\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}d\$ is Gaussian and the target \$y {\textbackslash}in {\textbackslash}mathbb\{R\}\$ follows a multiple-index model, i.e., \$y=g({\textbackslash}langle{\textbackslash}boldsymbol\{u\_1\},{\textbackslash}boldsymbol\{x\}{\textbackslash}rangle,...,{\textbackslash}langle{\textbackslash}boldsymbol\{u\_k\},{\textbackslash}boldsymbol\{x\}{\textbackslash}rangle)\$ with a noisy link function \$g\$. We prove that the first-layer weights of the NN converge to the \$k\$-dimensional principal subspace spanned by the vectors \${\textbackslash}boldsymbol\{u\_1\},...,{\textbackslash}boldsymbol\{u\_k\}\$ of the true model, when online SGD with weight decay is used for training. This phenomenon has several important consequences when \$k {\textbackslash}ll d\$. First, by employing uniform convergence on this smaller subspace, we establish a generalization error bound of \${\textbackslash}mathcal\{O\}({\textbackslash}sqrt\{\{kd\}/\{T\}\})\$ after \$T\$ iterations of SGD, which is independent of the width of the NN. We further demonstrate that, SGD-trained ReLU NNs can learn a single-index target of the form \$y=f({\textbackslash}langle{\textbackslash}boldsymbol\{u\},{\textbackslash}boldsymbol\{x\}{\textbackslash}rangle) + {\textbackslash}epsilon\$ by recovering the principal direction, with a sample complexity linear in \$d\$ (up to log factors), where \$f\$ is a monotonic function with at most polynomial growth, and \${\textbackslash}epsilon\$ is the noise. This is in contrast to the known \$d{\textasciicircum}\{{\textbackslash}Omega(p)\}\$ sample requirement to learn any degree \$p\$ polynomial in the kernel regime, and it shows that NNs trained with SGD can outperform the neural tangent kernel at initialization. Finally, we also provide compressibility guarantees for NNs using the approximate low-rank structure produced by SGD.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{neyshaburImplicitRegularizationDeep2017,
  title = {Implicit {{Regularization}} in {{Deep Learning}}},
  author = {Neyshabur, Behnam},
  year = {2017},
  month = sep,
  number = {arXiv:1709.01953},
  eprint = {1709.01953},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-11-04},
  abstract = {In an attempt to better understand generalization in deep learning, we study several possible explanations. We show that implicit regularization induced by the optimization method is playing a key role in generalization and success of deep learning models. Motivated by this view, we study how different complexity measures can ensure generalization and explain how optimization algorithms can implicitly regularize complexity measures. We empirically investigate the ability of these measures to explain different observed phenomena in deep learning. We further study the invariances in neural networks, suggest complexity measures and optimization algorithms that have similar invariances to those in neural networks and evaluate them on a number of learning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{neyshaburNormBasedCapacityControl2015,
  title = {Norm-{{Based Capacity Control}} in {{Neural Networks}}},
  author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  year = {2015},
  month = apr,
  journal = {arXiv:1503.00036 [cs, stat]},
  eprint = {1503.00036},
  primaryclass = {cs, stat},
  urldate = {2021-04-22},
  abstract = {We investigate the capacity, convexity and characterization of a general family of norm-constrained feed-forward networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@misc{neyshaburSearchRealInductive2015,
  title = {In {{Search}} of the {{Real Inductive Bias}}: {{On}} the {{Role}} of {{Implicit Regularization}} in {{Deep Learning}}},
  shorttitle = {In {{Search}} of the {{Real Inductive Bias}}},
  author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  year = {2015},
  month = apr,
  number = {arXiv:1412.6614},
  eprint = {1412.6614},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6614},
  urldate = {2023-02-20},
  abstract = {We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multilayer feed-forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{oymakNearOptimalBoundsBinary2015,
  title = {Near-{{Optimal Bounds}} for {{Binary Embeddings}} of {{Arbitrary Sets}}},
  author = {Oymak, Samet and Recht, Ben},
  year = {2015},
  month = dec,
  journal = {arXiv:1512.04433 [cs, math]},
  eprint = {1512.04433},
  primaryclass = {cs, math},
  urldate = {2021-04-19},
  abstract = {We study embedding a subset K of the unit sphere to the Hamming cube \{-1, +1\}m. We characterize the tradeoff between distortion and sample complexity m in terms of the Gaussian width {$\omega$}(K) of the set. For subspaces and several structured-sparse sets we show that Gaussian maps provide the optimal tradeoff m {$\sim$} {$\delta-$}2{$\omega$}2(K), in particular for {$\delta$} distortion one needs m {$\approx$} {$\delta-$}2d where d is the subspace dimension. For general sets, we provide sharp characterizations which reduces to m {$\approx$} {$\delta-$}4{$\omega$}2(K) after simplification. We provide improved results for local embedding of points that are in close proximity of each other which is related to locality sensitive hashing. We also discuss faster binary embedding where one takes advantage of an initial sketching procedure based on Fast Johnson-Lindenstauss Transform. Finally, we list several numerical observations and discuss open problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Mathematics - Functional Analysis}
}

@article{paluzo-hidalgoTwohiddenlayerFeedforwardNetworks2020,
  title = {Two-Hidden-Layer Feed-Forward Networks Are Universal Approximators: {{A}} Constructive Approach},
  shorttitle = {Two-Hidden-Layer Feed-Forward Networks Are Universal Approximators},
  author = {{Paluzo-Hidalgo}, Eduardo and {Gonzalez-Diaz}, Rocio and {Guti{\'e}rrez-Naranjo}, Miguel A.},
  year = {2020},
  month = nov,
  journal = {Neural Networks},
  volume = {131},
  pages = {29--36},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2020.07.021},
  urldate = {2023-03-03},
  abstract = {It is well-known that artificial neural networks are universal approximators. The classical existence result proves that, given a continuous function on a compact set embedded in an n-dimensional space, there exists a one-hidden-layer feed-forward network that approximates the function. In this paper, a constructive approach to this problem is given for the case of a continuous function on triangulated spaces. Once a triangulation of the space is given, a two-hidden-layer feed-forward network with a concrete set of weights is computed. The level of the approximation depends on the refinement of the triangulation.},
  langid = {english},
  keywords = {Multi-layer feed-forward network,Simplicial Approximation Theorem,Triangulations,Universal Approximation Theorem}
}

@misc{parhiBanachSpaceRepresenter2021,
  title = {Banach {{Space Representer Theorems}} for {{Neural Networks}} and {{Ridge Splines}}},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2021},
  month = feb,
  number = {arXiv:2006.05626},
  eprint = {2006.05626},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.05626},
  urldate = {2022-09-20},
  abstract = {We develop a variational framework to understand the properties of the functions learned by neural networks fit to data. We propose and study a family of continuous-domain linear inverse problems with total variation-like regularization in the Radon domain subject to data fitting constraints. We derive a representer theorem showing that finite-width, single-hidden layer neural networks are solutions to these inverse problems. We draw on many techniques from variational spline theory and so we propose the notion of polynomial ridge splines, which correspond to single-hidden layer neural networks with truncated power functions as the activation function. The representer theorem is reminiscent of the classical reproducing kernel Hilbert space representer theorem, but we show that the neural network problem is posed over a non-Hilbertian Banach space. While the learning problems are posed in the continuous-domain, similar to kernel methods, the problems can be recast as finite-dimensional neural network training problems. These neural network training problems have regularizers which are related to the well-known weight decay and path-norm regularizers. Thus, our result gives insight into functional characteristics of trained neural networks and also into the design neural network regularizers. We also show that these regularizers promote neural network solutions with desirable generalization properties.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{parhiRoleNeuralNetwork2020,
  title = {The {{Role}} of {{Neural Network Activation Functions}}},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2020},
  journal = {IEEE Signal Processing Letters},
  volume = {27},
  eprint = {1910.02333},
  primaryclass = {cs, stat},
  pages = {1779--1783},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2020.3027517},
  urldate = {2022-10-27},
  abstract = {A wide variety of activation functions have been proposed for neural networks. The Rectified Linear Unit (ReLU) is especially popular today. There are many practical reasons that motivate the use of the ReLU. This paper provides new theoretical characterizations that support the use of the ReLU, its variants such as the leaky ReLU, as well as other activation functions in the case of univariate, single-hidden layer feedforward neural networks. Our results also explain the importance of commonly used strategies in the design and training of neural networks such as "weight decay" and "path-norm" regularization, and provide a new justification for the use of "skip connections" in network architectures. These new insights are obtained through the lens of spline theory. In particular, we show how neural network training problems are related to infinite-dimensional optimizations posed over Banach spaces of functions whose solutions are well-known to be fractional and polynomial splines, where the particular Banach space (which controls the order of the spline) depends on the choice of activation function.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{parhiWhatKindsFunctions2022,
  title = {What {{Kinds}} of {{Functions}} Do {{Deep Neural Networks Learn}}? {{Insights}} from {{Variational Spline Theory}}},
  shorttitle = {What {{Kinds}} of {{Functions}} Do {{Deep Neural Networks Learn}}?},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2022},
  month = jun,
  journal = {SIAM Journal on Mathematics of Data Science},
  volume = {4},
  number = {2},
  eprint = {2105.03361},
  primaryclass = {cs, stat},
  pages = {464--489},
  issn = {2577-0187},
  doi = {10.1137/21M1418642},
  urldate = {2022-09-20},
  abstract = {We develop a variational framework to understand the properties of functions learned by fitting deep neural networks with rectified linear unit activations to data. We propose a new function space, which is reminiscent of classical bounded variation-type spaces, that captures the compositional structure associated with deep neural networks. We derive a representer theorem showing that deep ReLU networks are solutions to regularized data fitting problems over functions from this space. The function space consists of compositions of functions from the Banach spaces of second-order bounded variation in the Radon domain. These are Banach spaces with sparsity-promoting norms, giving insight into the role of sparsity in deep neural networks. The neural network solutions have skip connections and rank bounded weight matrices, providing new theoretical support for these common architectural choices. The variational problem we study can be recast as a finite-dimensional neural network training problem with regularization schemes related to the notions of weight decay and path-norm regularization. Finally, our analysis builds on techniques from variational spline theory, providing new connections between deep neural networks and splines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{perez-ortizEStatisticsGroupInvariance2023,
  title = {E-{{Statistics}}, {{Group Invariance}} and {{Anytime Valid Testing}}},
  author = {{P{\'e}rez-Ortiz}, Muriel Felipe and Lardy, Tyron and {de Heide}, Rianne and Gr{\"u}nwald, Peter},
  year = {2023},
  month = oct,
  number = {arXiv:2208.07610},
  eprint = {2208.07610},
  primaryclass = {math, stat},
  publisher = {arXiv},
  urldate = {2024-08-13},
  abstract = {We study worst-case-growth-rate-optimal (GROW) e-statistics for hypothesis testing between two group models. It is known that under a mild condition on the action of the underlying group G on the data, there exists a maximally invariant statistic. We show that among all e-statistics, invariant or not, the likelihood ratio of the maximally invariant statistic is GROW, both in the absolute and in the relative sense, and that an anytime-valid test can be based on it. The GROW e-statistic is equal to a Bayes factor with a right Haar prior on G. Our treatment avoids nonuniqueness issues that sometimes arise for such priors in Bayesian contexts. A crucial assumption on the group G is its amenability, a well-known group-theoretical condition, which holds, for instance, in scale-location families. Our results also apply to finite-dimensional linear regression.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology}
}

@article{petersenClassicalDifferentialGeometry,
  title = {Classical {{Differential Geometry}}},
  author = {Petersen, Peter},
  langid = {english}
}

@article{pinkusApproximationTheoryMLP1999,
  title = {Approximation Theory of the {{MLP}} Model in Neural Networks},
  author = {Pinkus, Allan},
  year = {1999},
  month = jan,
  journal = {Acta Numerica},
  volume = {8},
  pages = {143--195},
  publisher = {Cambridge University Press},
  issn = {1474-0508, 0962-4929},
  doi = {10.1017/S0962492900002919},
  urldate = {2023-02-24},
  abstract = {In this survey we discuss various approximation-theoretic problems that arise in the multilayer feedforward perceptron (MLP) model in neural networks. The MLP model is one of the more popular and practical of the many neural network models. Mathematically it is also one of the simpler models. Nonetheless the mathematics of this model is not well understood, and many of these problems are approximation-theoretic in character. Most of the research we will discuss is of very recent vintage. We will report on what has been done and on various unanswered questions. We will not be presenting practical (algorithmic) methods. We will, however, be exploring the capabilities and limitations of this model.},
  langid = {english}
}

@article{pisierRemarquesResultatNon1980,
  title = {Remarques Sur Un R{\'e}sultat Non Publi{\'e} de {{B}}. {{Maurey}}},
  author = {Pisier, G.},
  year = {1980/1981},
  journal = {S{\'e}minaire d'Analyse fonctionnelle (dit "Maurey-Schwartz")},
  pages = {1--12},
  issn = {2497-4005},
  urldate = {2023-03-01},
  langid = {english}
}

@misc{poggioWhyWhenCan2017,
  title = {Why and {{When Can Deep}} -- but {{Not Shallow}} -- {{Networks Avoid}} the {{Curse}} of {{Dimensionality}}: A {{Review}}},
  shorttitle = {Why and {{When Can Deep}} -- but {{Not Shallow}} -- {{Networks Avoid}} the {{Curse}} of {{Dimensionality}}},
  author = {Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
  year = {2017},
  month = feb,
  number = {arXiv:1611.00740},
  eprint = {1611.00740},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-02-16},
  abstract = {The paper characterizes classes of functions for which deep learning can be exponentially better than shallow learning. Deep convolutional networks are a special case of these conditions, though weight sharing is not the main reason for their exponential advantage.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{rahimiUniformApproximationFunctions2008,
  title = {Uniform Approximation of Functions with Random Bases},
  booktitle = {2008 46th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}}},
  author = {Rahimi, Ali and Recht, Benjamin},
  year = {2008},
  month = sep,
  pages = {555--561},
  publisher = {IEEE},
  address = {Monticello, IL, USA},
  doi = {10.1109/ALLERTON.2008.4797607},
  urldate = {2021-04-29},
  abstract = {Random networks of nonlinear functions have a long history of empirical success in function fitting but few theoretical guarantees. In this paper, using techniques from probability on Banach Spaces, we analyze a specific architecture of random nonlinearities, provide L{$\infty$} and L2 error bounds for approximating functions in Reproducing Kernel Hilbert Spaces, and discuss scenarios when these expansions are dense in the continuous functions. We discuss connections between these random nonlinear networks and popular machine learning algorithms and show experimentally that these networks provide competitive performance at far lower computational cost on large-scale pattern recognition tasks.},
  isbn = {978-1-4244-2925-7},
  langid = {english}
}

@misc{ramdasGametheoreticStatisticsSafe2023,
  title = {Game-Theoretic Statistics and Safe Anytime-Valid Inference},
  author = {Ramdas, Aaditya and Gr{\"u}nwald, Peter and Vovk, Vladimir and Shafer, Glenn},
  year = {2023},
  month = jun,
  number = {arXiv:2210.01948},
  eprint = {2210.01948},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-08-13},
  abstract = {Safe anytime-valid inference (SAVI) provides measures of statistical evidence and certainty---e-processes for testing and confidence sequences for estimation---that remain valid at all stopping times, accommodating continuous monitoring and analysis of accumulating data and optional stopping or continuation for any reason. These measures crucially rely on test martingales, which are nonnegative martingales starting at one. Since a test martingale is the wealth process of a player in a betting game, SAVI centrally employs game-theoretic intuition, language and mathematics. We summarize the SAVI goals and philosophy, and report recent advances in testing composite hypotheses and estimating functionals in nonparametric settings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Information Theory,Mathematics - Statistics Theory,Statistics - Methodology}
}

@book{rudinFunctionalAnalysis1991,
  title = {Functional Analysis},
  author = {Rudin, Walter},
  year = {1991},
  series = {International Series in Pure and Applied Mathematics},
  edition = {2nd ed},
  publisher = {McGraw-Hill},
  address = {New York},
  isbn = {978-0-07-054236-5},
  langid = {english},
  lccn = {QA320 .R83 1991},
  keywords = {Functional analysis}
}

@book{rudinRealComplexAnalysis1987,
  title = {Real and Complex Analysis},
  author = {Rudin, Walter},
  year = {1987},
  edition = {3rd ed},
  publisher = {McGraw-Hill},
  address = {New York},
  isbn = {978-0-07-054234-1},
  langid = {english},
  lccn = {QA300 .R82 1987},
  keywords = {Mathematical analysis}
}

@article{schmidt-hieberNonparametricRegressionUsing2020,
  title = {Nonparametric Regression Using Deep Neural Networks with {{ReLU}} Activation Function},
  author = {{Schmidt-Hieber}, Johannes},
  year = {2020},
  month = sep,
  journal = {arXiv:1708.06633 [cs, math, stat]},
  eprint = {1708.06633},
  primaryclass = {cs, math, stat},
  doi = {10.1214/19-AOS1875},
  urldate = {2021-04-21},
  abstract = {Consider the multivariate nonparametric regression model. It is shown that estimators based on sparsely connected deep neural networks with ReLU activation function and properly chosen network architecture achieve the minimax rates of convergence (up to \${\textbackslash}log n\$-factors) under a general composition assumption on the regression function. The framework includes many well-studied structural constraints such as (generalized) additive models. While there is a lot of flexibility in the network architecture, the tuning parameter is the sparsity of the network. Specifically, we consider large networks with number of potential network parameters exceeding the sample size. The analysis gives some insights into why multilayer feedforward neural networks perform well in practice. Interestingly, for ReLU activation function the depth (number of layers) of the neural network architectures plays an important role and our theory suggests that for nonparametric regression, scaling the network depth with the sample size is natural. It is also shown that under the composition assumption wavelet estimators can only achieve suboptimal rates.},
  archiveprefix = {arXiv},
  keywords = {62G08,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@book{shaferGametheoreticFoundationsProbability2019,
  title = {Game-Theoretic Foundations for Probability and Finance},
  author = {Shafer, Glenn and Vovk, Vladimir},
  year = {2019},
  series = {Wiley Series in Probability and Statistics},
  publisher = {Wiley},
  address = {Hoboken, NJ},
  abstract = {Game-theoretic probability and finance come of age Glenn Shafer and Vladimir Vovk Probability and Finance, published in 2001, showed that perfect-information games can be used to define mathematical probability. Based on fifteen years of further research, Game-Theoretic Foundations for Probability and Finance presents a mature view of the foundational role game theory can play. Its account of probability theory opens the way to new methods of prediction and testing and makes many statistical methods more transparent and widely usable. Its contributions to finance theory include purely game-theoretic accounts of Ito stochastic calculus, the capital asset pricing model, the equity premium, and portfolio theory. Game-Theoretic Foundations for Probability and Finance is a book of research. It is also a teaching resource. Each chapter is supplemented with carefully designed exercises and notes relating the new theory to its historical context. Praise from early readers ver since Kolmogorov's Grundbegriffe, the standard mathematical treatment of probability theory has been measure-theoretic. In this ground-breaking work, Shafer and Vovk give a game-theoretic foundation instead. While being just as rigorous, the game-theoretic approach allows for vast and useful generalizations of classical measure-theoretic results, while also giving rise to new, radical ideas for prediction, statistics and mathematical finance without stochastic assumptions. The authors set out their theory in great detail, resulting in what is definitely one of the most important books on the foundations of probability to have appeared in the last few decades. Peter Gr130 0Wiley series in probability and statisticsnwald, CWI and University of Leiden hafer and Vovk have thoroughly re-written their 2001 book on the game-theoretic foundations for probability and for finance. They have included an account of the tremendous growth that has occurred since, in the game-theoretic and pathwise approaches to stochastic analysis and in their applications to continuous-time finance. This new book will undoubtedly spur a better understanding of the foundations of these very important fields, and we should all be grateful to its authors. Ioannis Karatzas, Columbia University},
  isbn = {978-1-118-54793-9 978-1-118-54803-5 978-1-118-54802-8 978-0-470-90305-6},
  langid = {english},
  lccn = {519.3}
}

@article{shaferTestMartingalesBayes2011,
  title = {Test {{Martingales}}, {{Bayes Factors}} and p-{{Values}}},
  author = {Shafer, Glenn and Shen, Alexander and Vereshchagin, Nikolai and Vovk, Vladimir},
  year = {2011},
  month = feb,
  journal = {Statistical Science},
  volume = {26},
  number = {1},
  issn = {0883-4237},
  doi = {10.1214/10-STS347},
  urldate = {2024-08-13},
  abstract = {A nonnegative martingale with initial value equal to one measures evidence against a probabilistic hypothesis. The inverse of its value at some stopping time can be interpreted as a Bayes factor. If we exaggerate the evidence by considering the largest value attained so far by such a martingale, the exaggeration will be limited, and there are systematic ways to eliminate it. The inverse of the exaggerated value at some stopping time can be interpreted as a p-value. We give a simple characterization of all increasing functions that eliminate the exaggeration.},
  langid = {english}
}

@book{shalev-shwartzUnderstandingMachineLearning2014,
  title = {Understanding Machine Learning: From Theory to Algorithms},
  shorttitle = {Understanding Machine Learning},
  author = {{Shalev-Shwartz}, Shai and {Ben-David}, Shai},
  year = {2014},
  publisher = {Cambridge University Press},
  address = {New York, NY, USA},
  abstract = {"Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering"--},
  isbn = {978-1-107-05713-5},
  langid = {english},
  lccn = {Q325.5 .S475 2014},
  keywords = {Algorithms,COMPUTERS / Computer Vision & Pattern Recognition,Machine learning}
}

@article{shenWhatMathematicalStatistics,
  title = {What the {{Mathematical Statistics Cannot Say}}  (and What It Can)},
  author = {Shen, Alexander},
  langid = {english}
}

@misc{siegelApproximationRatesNeural2021,
  title = {Approximation {{Rates}} for {{Neural Networks}} with {{General Activation Functions}}},
  author = {Siegel, Jonathan W. and Xu, Jinchao},
  year = {2021},
  month = jan,
  number = {arXiv:1904.02311},
  eprint = {1904.02311},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.02311},
  urldate = {2023-02-28},
  abstract = {We prove some new results concerning the approximation rate of neural networks with general activation functions. Our first result concerns the rate of approximation of a two layer neural network with a polynomially-decaying non-sigmoidal activation function. We extend the dimension independent approximation rates previously obtained to this new class of activation functions. Our second result gives a weaker, but still dimension independent, approximation rate for a larger class of activation functions, removing the polynomial decay assumption. This result applies to any bounded, integrable activation function. Finally, we show that a stratified sampling approach can be used to improve the approximation rate for polynomially decaying activation functions under mild additional assumptions.},
  archiveprefix = {arXiv},
  keywords = {41A25 41A30,Computer Science - Machine Learning,Mathematics - Classical Analysis and ODEs}
}

@misc{siegelCharacterizationVariationSpaces2022,
  title = {Characterization of the {{Variation Spaces Corresponding}} to {{Shallow Neural Networks}}},
  author = {Siegel, Jonathan W. and Xu, Jinchao},
  year = {2022},
  month = apr,
  number = {arXiv:2106.15002},
  eprint = {2106.15002},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.15002},
  urldate = {2023-03-08},
  abstract = {We study the variation space corresponding to a dictionary of functions in \$L{\textasciicircum}2({\textbackslash}Omega)\$ for a bounded domain \${\textbackslash}Omega{\textbackslash}subset {\textbackslash}mathbb\{R\}{\textasciicircum}d\$. Specifically, we compare the variation space, which is defined in terms of a convex hull with related notions based on integral representations. This allows us to show that three important notions relating to the approximation theory of shallow neural networks, the Barron space, the spectral Barron space, and the Radon BV space, are actually variation spaces with respect to certain natural dictionaries.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{siegelHighOrderApproximationRates2021,
  title = {High-{{Order Approximation Rates}} for {{Shallow Neural Networks}} with {{Cosine}} and {{ReLU}}{\textasciicircum}k {{Activation Functions}}},
  author = {Siegel, Jonathan W. and Xu, Jinchao},
  year = {2021},
  month = dec,
  number = {arXiv:2012.07205},
  eprint = {2012.07205},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.07205},
  urldate = {2023-02-24},
  abstract = {We study the approximation properties of shallow neural networks with an activation function which is a power of the rectified linear unit. Specifically, we consider the dependence of the approximation rate on the dimension and the smoothness in the spectral Barron space of the underlying function \$f\$ to be approximated. We show that as the smoothness index \$s\$ of \$f\$ increases, shallow neural networks with ReLU\${\textasciicircum}k\$ activation function obtain an improved approximation rate up to a best possible rate of \$O(n{\textasciicircum}\{-(k+1)\}{\textbackslash}log(n))\$ in \$L{\textasciicircum}2\$, independent of the dimension \$d\$. The significance of this result is that the activation function ReLU\${\textasciicircum}k\$ is fixed independent of the dimension, while for classical methods the degree of polynomial approximation or the smoothness of the wavelets used would have to increase in order to take advantage of the dimension dependent smoothness of \$f\$. In addition, we derive improved approximation rates for shallow neural networks with cosine activation function on the spectral Barron space. Finally, we prove lower bounds showing that the approximation rates attained are optimal under the given assumptions.},
  archiveprefix = {arXiv},
  keywords = {41A25,Mathematics - Numerical Analysis}
}

@misc{siegelSharpBoundsApproximation2022,
  author = {Siegel, Jonathan W. and Xu, Jinchao},
  year = {2022},
  month = jul,
  number = {arXiv:2101.12365},
  eprint = {2101.12365},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.12365},
  urldate = {2023-02-28},
  abstract = {In this article, we study approximation properties of the variation spaces corresponding to shallow neural networks with a variety of activation functions. We introduce two main tools for estimating the metric entropy, approximation rates, and \$n\$-widths of these spaces. First, we introduce the notion of a smoothly parameterized dictionary and give upper bounds on the non-linear approximation rates, metric entropy and \$n\$-widths of their absolute convex hull. The upper bounds depend upon the order of smoothness of the parameterization. This result is applied to dictionaries of ridge functions corresponding to shallow neural networks, and they improve upon existing results in many cases. Next, we provide a method for lower bounding the metric entropy and \$n\$-widths of variation spaces which contain certain classes of ridge functions. This result gives sharp lower bounds on the \$L{\textasciicircum}2\$-approximation rates, metric entropy, and \$n\$-widths for variation spaces corresponding to neural networks with a range of important activation functions, including ReLU\${\textasciicircum}k\$ activation functions and sigmoidal activation functions with bounded variation.},
  archiveprefix = {arXiv},
  keywords = {62M45 41A46,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  title = {Sharp {{Bounds}} on the {{Approximation Rates}}, {{Metric Entropy}}, and {$n$}-Widths of {{Shallow Neural Networks}}}
}

@misc{soudryImplicitBiasGradient2022,
  title = {The {{Implicit Bias}} of {{Gradient Descent}} on {{Separable Data}}},
  author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  year = {2022},
  month = jul,
  number = {arXiv:1710.10345},
  eprint = {1710.10345},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.10345},
  urldate = {2023-02-20},
  abstract = {We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization n more complex models and with other optimization methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{starlingTargetedSmoothBayesian2020,
  title = {Targeted {{Smooth Bayesian Causal Forests}}: {{An}} Analysis of Heterogeneous Treatment Effects for Simultaneous versus Interval Medical Abortion Regimens over Gestation},
  shorttitle = {Targeted {{Smooth Bayesian Causal Forests}}},
  author = {Starling, Jennifer E. and Murray, Jared S. and Lohr, Patricia A. and Aiken, Abigail R. A. and Carvalho, Carlos M. and Scott, James G.},
  year = {2020},
  month = feb,
  journal = {arXiv:1905.09405 [stat]},
  eprint = {1905.09405},
  primaryclass = {stat},
  urldate = {2021-04-19},
  abstract = {We introduce Targeted Smooth Bayesian Causal Forests (tsBCF), a nonparametric Bayesian approach for estimating heterogeneous treatment effects which vary smoothly over a single covariate in the observational data setting. The tsBCF method induces smoothness by parameterizing terminal tree nodes with smooth functions, and allows for separate regularization of treatment effects versus prognostic effect of control covariates. Smoothing parameters for prognostic and treatment effects can be chosen to reflect prior knowledge or tuned in a datadependent way.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Applications}
}

@misc{todorovaPaperReviewMultilayer2016,
  title = {Paper {{Review}} "{{Multilayer Feedforward Networks}} Are {{Universal Approximators}}"},
  author = {Todorova, Sonia},
  year = {2016}
}

@misc{turnerGenericEVariablesExact2022,
  title = {Generic {{E-Variables}} for {{Exact Sequential}} k-{{Sample Tests}} That Allow for {{Optional Stopping}}},
  author = {Turner, Rosanne and Ly, Alexander and Gr{\"u}nwald, Peter},
  year = {2022},
  month = jun,
  number = {arXiv:2106.02693},
  eprint = {2106.02693},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.02693},
  urldate = {2024-09-04},
  abstract = {We develop E-variables for testing whether two or more data streams come from the same source or not, and more generally, whether the difference between the sources is larger than some minimal effect size. These E-variables lead to exact, nonasymptotic tests that remain safe, i.e. keep their type-I error guarantees, under flexible sampling scenarios such as optional stopping and continuation. In special cases our E-variables also have an optimal 'growth' property under the alternative. While the construction is generic, we illustrate it through the special case of k x 2 contingency tables, where we also allow for the incorporation of different restrictions on a composite alternative. Comparison to p-value analysis in simulations and a real-world example show that E-variables, through their flexibility, often allow for early stopping of data collection, thereby retaining similar power as classical methods, while also retaining the option of extending or combining data afterwards.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Methodology}
}

@article{vandegeerEMPIRICALPROCESSTHEORY,
  title = {{{EMPIRICAL PROCESS THEORY AND APPLICATIONS}}},
  author = {{van de Geer}, Sara},
  langid = {english}
}

@book{vandervaartWeakConvergenceEmpirical1996,
  title = {Weak {{Convergence}} and {{Empirical Processes}}},
  author = {{van der Vaart}, Aad W. and Wellner, Jon A.},
  year = {1996},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4757-2545-2},
  urldate = {2023-03-01},
  isbn = {978-1-4757-2547-6 978-1-4757-2545-2},
  langid = {english}
}

@article{villeEtudeCritiqueNotion1939,
  title = {{{\'E}tude critique de la notion de collectif}},
  author = {Ville, Jean},
  year = {1939},
  langid = {french}
}

@article{vovkConfidenceDiscoveriesEvalues2023,
  title = {Confidence and Discoveries with E-Values},
  author = {Vovk, Vladimir and Wang, Ruodu},
  year = {2023},
  month = may,
  journal = {Statistical Science},
  volume = {38},
  number = {2},
  eprint = {1912.13292},
  primaryclass = {math, stat},
  issn = {0883-4237},
  doi = {10.1214/22-STS874},
  urldate = {2024-08-13},
  abstract = {We discuss systematically two versions of confidence regions: those based on p-values and those based on e-values, a recent alternative to pvalues. Both versions can be applied to multiple hypothesis testing, and in this paper we are interested in procedures that control the number of false discoveries under arbitrary dependence between the base p- or e-values. We introduce a procedure that is based on e-values and show that it is efficient both computationally and statistically using simulated and realworld datasets. Comparison with the corresponding standard procedure based on p-values is not straightforward, but there are indications that the new one performs significantly better in some situations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {62F03 62F25 62G10 62G15 (Primary) 62A01 (Secondary),Mathematics - Statistics Theory}
}

@article{vovkEvaluesCalibrationCombination2021,
  title = {E-Values: {{Calibration}}, Combination, and Applications},
  shorttitle = {E-Values},
  author = {Vovk, Vladimir and Wang, Ruodu},
  year = {2021},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {49},
  number = {3},
  eprint = {1912.06116},
  primaryclass = {math, stat},
  issn = {0090-5364},
  doi = {10.1214/20-AOS2020},
  urldate = {2024-08-13},
  abstract = {Multiple testing of a single hypothesis and testing multiple hypotheses are usually done in terms of p-values. In this paper we replace p-values with their natural competitor, e-values, which are closely related to betting, Bayes factors, and likelihood ratios. We demonstrate that e-values are often mathematically more tractable; in particular, in multiple testing of a single hypothesis, e-values can be merged simply by averaging them. This allows us to develop efficient procedures using e-values for testing multiple hypotheses.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {62G10 62F03 (Primary) 62C07 62C15 (Secondary),Mathematics - Statistics Theory}
}

@incollection{wahbaRepresenterTheorem2019,
  title = {Representer {{Theorem}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Wahba, Grace and Wang, Yuedong},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2019},
  month = nov,
  edition = {1},
  pages = {1--11},
  publisher = {Wiley},
  doi = {10.1002/9781118445112.stat08200},
  urldate = {2023-02-16},
  abstract = {The representer theorem plays an outsized role in a large class of learning problems. It provides a means to reduce infinite dimensional optimization problems to tractable finite dimensional ones. This article reviews the representer theorem for various learning problems under the reproducing kernel Hilbert spaces framework. We present solutions to the penalized least squares and penalized likelihood for nonparametric regression, and support vector machines for classification as a solution to the penalized hinge loss. We discuss extensions of the representer theorem for regression with functional data.},
  isbn = {978-1-118-44511-2},
  langid = {english}
}

@article{wangDebiasedInferenceTreatment,
  title = {Debiased {{Inference}} on {{Treatment Effect}} in a {{High-Dimensional Model}}},
  author = {Wang, Jingshen and He, Xuming and Xu, Gongjun},
  pages = {14},
  abstract = {This article concerns the potential bias in statistical inference on treatment effects when a large number of covariates are present in a linear or partially linear model. While the estimation bias in an under-fitted model is well understood, we address a lesser-known bias that arises from an over-fitted model. The overfitting bias can be eliminated through data splitting at the cost of statistical efficiency, and we show that smoothing over random data splits can be pursued to mitigate the efficiency loss. We also discuss some of the existing methods for debiased inference and provide insights into their intrinsic bias-variance trade-off, which leads to an improvement in bias controls. Under appropriate conditions, we show that the proposed estimators for the treatment effects are asymptotically normal and their variances can be well estimated. We discuss the pros and cons of various methods both theoretically and empirically, and show that the proposed methods are valuable options in post-selection inference. Supplementary materials for this article are available online.},
  langid = {english}
}

@misc{wangFalseDiscoveryRate2021,
  title = {False Discovery Rate Control with E-Values},
  author = {Wang, Ruodu and Ramdas, Aaditya},
  year = {2021},
  month = dec,
  number = {arXiv:2009.02824},
  eprint = {2009.02824},
  primaryclass = {math, stat},
  publisher = {arXiv},
  urldate = {2024-08-13},
  abstract = {E-values have gained attention as potential alternatives to p-values as measures of uncertainty, significance and evidence. In brief, e-values are realized by random variables with expectation at most one under the null; examples include betting scores, (point null) Bayes factors, likelihood ratios and stopped supermartingales. We design a natural analog of the Benjamini-Hochberg (BH) procedure for false discovery rate (FDR) control that utilizes e-values, called the e-BH procedure, and compare it with the standard procedure for p-values. One of our central results is that, unlike the usual BH procedure, the e-BH procedure controls the FDR at the desired level---with no correction---for any dependence structure between the e-values. We illustrate that the new procedure is convenient in various settings of complicated dependence, structured and post-selection hypotheses, and multi-armed bandit problems. Moreover, the BH procedure is a special case of the e-BH procedure through calibration between p-values and e-values. Overall, the e-BH procedure is a novel, powerful and general tool for multiple testing under dependence, that is complementary to the BH procedure, each being an appropriate choice in different applications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory}
}

@misc{wangTestingValuesPvalues2022,
  title = {Testing with P*-Values: {{Between}} p-Values, Mid p-Values, and e-Values},
  shorttitle = {Testing with P*-Values},
  author = {Wang, Ruodu},
  year = {2022},
  month = feb,
  number = {arXiv:2010.14010},
  eprint = {2010.14010},
  primaryclass = {math, stat},
  publisher = {arXiv},
  urldate = {2024-08-15},
  abstract = {We introduce the notion of p*-values (p*-variables), which generalizes p-values (p-variables) in several senses. The new notion has four natural interpretations: operational, probabilistic, Bayesian, and frequentist. A main example of a p*-value is a mid p-value, which arises in the presence of discrete test statistics. A unified stochastic representation for p-values, mid p-values, and p*-values is obtained to illustrate the relationship between the three objects. We study several ways of merging arbitrarily dependent or independent p*-values into one p-value or p*-value. Admissible calibrators of p*-values to and from p-values and e-values are obtained with nice mathematical forms, revealing the role of p*-values as a bridge between p-values and e-values. The notion of p*-values becomes useful in many situations even if one is only interested in p-values, mid p-values, or e-values. In particular, deterministic tests based on p*-values can be applied to improve some classic methods for p-values and e-values.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory}
}

@article{wangTinyReviewEvalues,
  title = {A Tiny Review on E-Values and e-Processes},
  author = {Wang, Ruodu},
  langid = {english}
}

@article{williamsonEntropyNumbersLinear,
  title = {Entropy {{Numbers}} of {{Linear Function Classes}}},
  author = {Williamson, Robert C and Smola, Alex J and Scholkopf, Bernhard},
  abstract = {This paper collects together a miscellany of results originally motivated by the analysis of the generalization performance of the ``maximum-margin'' algorithm due to Vapnik and others. The key feature of the paper is its operator-theoretic viewpoint. New bounds on covering numbers for classes related to Maximum Margin classes are derived directly without making use of a combinatorial dimension such as the VC-dimension. Specific contents of the paper include: {\textasciimacron} a new and self-contained proof of Maurey's theorem and some generalizations with small explicit values of constants; {\textasciimacron} bounds on the covering numbers of maximum margin classes suitable for the analysis of their generalization performance; {\textasciimacron} the extension of such classes to those induced {$\frac{1}{2}$} by balls in quasi-Banach spaces (such as {\^O}norms with {$\frac{1}{4}$} {\^O} ).},
  langid = {english}
}

@phdthesis{wuUnderstandingWhenKernel,
  title = {Understanding {{When Kernel Ridgeless Regression Works}}},
  author = {Wu, Mingqi},
  langid = {english}
}

@article{xuFiniteNeuronMethod2020,
  title = {The {{Finite Neuron Method}} and {{Convergence Analysis}}},
  author = {Xu, Jinchao},
  year = {2020},
  month = jun,
  journal = {Communications in Computational Physics},
  volume = {28},
  number = {5},
  eprint = {2010.01458},
  primaryclass = {cs, math},
  pages = {1707--1745},
  issn = {1815-2406, 1991-7120},
  doi = {10.4208/cicp.OA-2020-0191},
  urldate = {2023-02-28},
  abstract = {We study a family of \$H{\textasciicircum}m\$-conforming piecewise polynomials based on artificial neural network, named as the finite neuron method (FNM), for numerical solution of \$2m\$-th order partial differential equations in \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$ for any \$m,d {\textbackslash}geq 1\$ and then provide convergence analysis for this method. Given a general domain \${\textbackslash}Omega{\textbackslash}subset{\textbackslash}mathbb R{\textasciicircum}d\$ and a partition \${\textbackslash}mathcal T\_h\$ of \${\textbackslash}Omega\$, it is still an open problem in general how to construct conforming finite element subspace of \$H{\textasciicircum}m({\textbackslash}Omega)\$ that have adequate approximation properties. By using techniques from artificial neural networks, we construct a family of \$H{\textasciicircum}m\$-conforming set of functions consisting of piecewise polynomials of degree \$k\$ for any \$k{\textbackslash}ge m\$ and we further obtain the error estimate when they are applied to solve elliptic boundary value problem of any order in any dimension. For example, the following error estimates between the exact solution \$u\$ and finite neuron approximation \$u\_N\$ are obtained. \$\$ {\textbackslash}{\textbar}u-u\_N{\textbackslash}{\textbar}\_\{H{\textasciicircum}m({\textbackslash}Omega)\}={\textbackslash}mathcal O(N{\textasciicircum}\{-\{1{\textbackslash}over 2\}-\{1{\textbackslash}over d\}\}). \$\$ Discussions will also be given on the difference and relationship between the finite neuron method and finite element methods (FEM). For example, for finite neuron method, the underlying finite element grids are not given a priori and the discrete solution can only be obtained by solving a non-linear and non-convex optimization problem. Despite of many desirable theoretical properties of the finite neuron method analyzed in the paper, its practical value is a subject of further investigation since the aforementioned underlying non-linear and non-convex optimization problem can be expensive and challenging to solve. For completeness and also convenience to readers, some basic known results and their proofs are also included in this manuscript.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Numerical Analysis}
}

@inproceedings{xuUnifiedFrameworkBandit2021,
  title = {A Unified Framework for Bandit Multiple Testing},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Xu, Ziyu and Wang, Ruodu and Ramdas, Aaditya},
  year = {2021},
  volume = {34},
  pages = {16833--16845},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-09-04},
  abstract = {In bandit multiple hypothesis testing, each arm corresponds to a different null hypothesis that we wish to test, and the goal is to design adaptive algorithms that correctly identify large set of interesting arms (true discoveries), while only mistakenly identifying a few uninteresting ones (false discoveries). One common metric in non-bandit multiple testing is the false discovery rate (FDR). We propose a unified, modular framework for bandit FDR control that emphasizes the decoupling of exploration and summarization of evidence. We utilize the powerful martingale-based concept of "e-processes" to ensure FDR control for arbitrary composite nulls, exploration rules and stopping times in generic problem settings. In particular, valid FDR control holds even if the reward distributions of the arms could be dependent, multiple arms may be queried simultaneously, and multiple (cooperating or competing) agents may be querying arms, covering combinatorial semi-bandit type settings as well. Prior work has considered in great detail the setting where each arm's reward distribution is independent and sub-Gaussian, and a single arm is queried at each step. Our framework recovers matching sample complexity guarantees in this special case, and performs comparably or better in practice. For other settings, sample complexities will depend on the finer details of the problem (composite nulls being tested, exploration algorithm, data dependence structure, stopping rule) and we do not explore these; our contribution is to show that the FDR guarantee is clean and entirely agnostic to these details.}
}

@article{yukichSupnormApproximationBounds1995,
  title = {Sup-Norm Approximation Bounds for Networks through Probabilistic Methods},
  author = {Yukich, J.E. and Stinchcombe, M.B. and White, H.},
  year = {1995},
  month = jul,
  journal = {IEEE Transactions on Information Theory},
  volume = {41},
  number = {4},
  pages = {1021--1027},
  issn = {1557-9654},
  doi = {10.1109/18.391247},
  abstract = {We consider the problem of approximating a smooth target function and its derivatives by networks involving superpositions and translations of a fixed activation function. The approximation is with respect to the sup-norm and the rate is shown to be of order O(n/sup -1/2/); that is, the rate is independent of the dimension d. The results apply to neural and wavelet networks and extend the work of Barren(see Proc. 7th Yale Workshop on Adaptive and Learning Systems, May, 1992, and ibid., vol.39, p.930, 1993). The approach involves probabilistic methods based on central limit theorems for empirical processes indexed by classes of functions.{$<>$}},
  keywords = {Artificial neural networks,Chaos,Fourier transforms,Mathematics,Neural networks,Robots}
}
