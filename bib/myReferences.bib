@misc{abbePolytimeUniversalityLimitations2020,
  title = {Poly-Time Universality and Limitations of Deep Learning},
  author = {Abbe, Emmanuel and Sandon, Colin},
  year = {2020},
  month = jan,
  number = {arXiv:2001.02992},
  eprint = {arXiv:2001.02992},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.02992},
  urldate = {2022-09-06},
  abstract = {The goal of this paper is to characterize function distributions that deep learning can or cannot learn in poly-time. A universality result is proved for SGD-based deep learning and a non-universality result is proved for GD-based deep learning; this also gives a separation between SGD-based deep learning and statistical query algorithms: (1) \{\textbackslash it Deep learning with SGD is efficiently universal.\} Any function distribution that can be learned from samples in poly-time can also be learned by a poly-size neural net trained with SGD on a poly-time initialization with poly-steps, poly-rate and possibly poly-noise. Therefore deep learning provides a universal learning paradigm: it was known that the approximation and estimation errors could be controlled with poly-size neural nets, using ERM that is NP-hard; this new result shows that the optimization error can also be controlled with SGD in poly-time. The picture changes for GD with large enough batches: (2) \{\textbackslash it Result (1) does not hold for GD:\} Neural nets of poly-size trained with GD (full gradients or large enough batches) on any initialization with poly-steps, poly-range and at least poly-noise cannot learn any function distribution that has super-polynomial \{\textbackslash it cross-predictability,\} where the cross-predictability gives a measure of ``average'' function correlation -- relations and distinctions to the statistical dimension are discussed. In particular, GD with these constraints can learn efficiently monomials of degree \$k\$ if and only if \$k\$ is constant. Thus (1) and (2) point to an interesting contrast: SGD is universal even with some poly-noise while full GD or SQ algorithms are not (e.g., parities).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/TFP9X4RR/Abbe and Sandon - 2020 - Poly-time universality and limitations of deep lea.pdf;/Users/wyq977/Zotero/storage/3IJP73A2/2001.html}
}

@book{alma990005456080205503,
  title = {Functional Analysis},
  author = {Rudin, Walter},
  year = {1973},
  series = {{{McGraw-Hill}} Series in Higher Mathematics},
  publisher = {{McGraw-Hill}},
  address = {{New York}},
  isbn = {0-07-054225-2},
  langid = {english},
  lccn = {71039686},
  keywords = {Analyse fonctionnelle},
  file = {/Users/wyq977/Zotero/storage/VWDLGRLZ/Rudin - 1973 - Functional analysis.pdf}
}

@misc{andrealorkeCybenkoTheoremCapability,
  title = {Cybenko's {{Theorem}} and the Capability of a Neural Network as Function Approximator},
  author = {{Andrea L\"orke} and {Fabian Schneider} and {Johannes Heck} and {Patrick Nitter}},
  urldate = {2022-06-23},
  file = {/Users/wyq977/Zotero/storage/KRD4JLDJ/Andrea Lörke et al. - Cybenko’s Theorem and the capability of a neural n.pdf}
}

@article{bachBreakingCurseDimensionality2016,
  title = {Breaking the {{Curse}} of {{Dimensionality}} with {{Convex Neural Networks}}},
  author = {Bach, Francis},
  year = {2016},
  month = oct,
  journal = {arXiv:1412.8690 [cs, math, stat]},
  eprint = {1412.8690},
  primaryclass = {cs, math, stat},
  urldate = {2021-05-12},
  abstract = {We consider neural networks with a single hidden layer and non-decreasing homogeneous activa-tion functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of ob-servations. In addition, we provide a simple geometric interpretation to the non-convex problem of addition of a new unit, which is the core potentially hard computational element in the framework of learning from continuously many basis functions. We provide simple conditions for convex relaxations to achieve the same generalization error bounds, even when constant-factor approxi-mations cannot be found (e.g., because it is NP-hard such as for the zero-homogeneous activation function). We were not able to find strong enough convex relaxations and leave open the existence or non-existence of polynomial-time algorithms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Mathematics - Statistics Theory},
  file = {/Users/wyq977/Zotero/storage/UG8ZUQH7/Bach - 2016 - Breaking the Curse of Dimensionality with Convex N.pdf;/Users/wyq977/Zotero/storage/JYLYQE6U/1412.html}
}

@misc{baHighdimensionalAsymptoticsFeature2022,
  title = {High-Dimensional {{Asymptotics}} of {{Feature Learning}}: {{How One Gradient Step Improves}} the {{Representation}}},
  shorttitle = {High-Dimensional {{Asymptotics}} of {{Feature Learning}}},
  author = {Ba, Jimmy and Erdogdu, Murat A. and Suzuki, Taiji and Wang, Zhichao and Wu, Denny and Yang, Greg},
  year = {2022},
  month = may,
  number = {arXiv:2205.01445},
  eprint = {arXiv:2205.01445},
  publisher = {{arXiv}},
  urldate = {2023-01-30},
  abstract = {We study the first gradient descent step on the first-layer parameters \$\textbackslash boldsymbol\{W\}\$ in a two-layer neural network: \$f(\textbackslash boldsymbol\{x\}) = \textbackslash frac\{1\}\{\textbackslash sqrt\{N\}\}\textbackslash boldsymbol\{a\}\^\textbackslash top\textbackslash sigma(\textbackslash boldsymbol\{W\}\^\textbackslash top\textbackslash boldsymbol\{x\})\$, where \$\textbackslash boldsymbol\{W\}\textbackslash in\textbackslash mathbb\{R\}\^\{d\textbackslash times N\}, \textbackslash boldsymbol\{a\}\textbackslash in\textbackslash mathbb\{R\}\^\{N\}\$ are randomly initialized, and the training objective is the empirical MSE loss: \$\textbackslash frac\{1\}\{n\}\textbackslash sum\_\{i=1\}\^n (f(\textbackslash boldsymbol\{x\}\_i)-y\_i)\^2\$. In the proportional asymptotic limit where \$n,d,N\textbackslash to\textbackslash infty\$ at the same rate, and an idealized student-teacher setting, we show that the first gradient update contains a rank-1 "spike", which results in an alignment between the first-layer weights and the linear component of the teacher model \$f\^*\$. To characterize the impact of this alignment, we compute the prediction risk of ridge regression on the conjugate kernel after one gradient step on \$\textbackslash boldsymbol\{W\}\$ with learning rate \$\textbackslash eta\$, when \$f\^*\$ is a single-index model. We consider two scalings of the first step learning rate \$\textbackslash eta\$. For small \$\textbackslash eta\$, we establish a Gaussian equivalence property for the trained feature map, and prove that the learned kernel improves upon the initial random features model, but cannot defeat the best linear model on the input. Whereas for sufficiently large \$\textbackslash eta\$, we prove that for certain \$f\^*\$, the same ridge estimator on trained features can go beyond this "linear regime" and outperform a wide range of random features and rotationally invariant kernels. Our results demonstrate that even one gradient step can lead to a considerable advantage over random features, and highlight the role of learning rate scaling in the initial phase of training.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/7LXSGTX9/Ba et al. - 2022 - High-dimensional Asymptotics of Feature Learning .pdf;/Users/wyq977/Zotero/storage/JBZQF37D/2205.html}
}

@article{banerjeeStatistics612Lp,
  title = {Statistics 612: {{Lp}} Spaces, Metrics on Spaces of Probabilites, and Connections to Estimation},
  author = {Banerjee, Moulinath},
  pages = {11},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/LGPVNTG8/Banerjee - Statistics 612 Lp spaces, metrics on spaces of pr.pdf}
}

@article{barronApproximationEstimationBounds1994,
  title = {Approximation and Estimation Bounds for Artificial Neural Networks},
  author = {Barron, Andrew R.},
  year = {1994},
  month = jan,
  journal = {Machine Learning},
  volume = {14},
  number = {1},
  pages = {115--133},
  issn = {1573-0565},
  doi = {10.1007/BF00993164},
  urldate = {2021-04-19},
  abstract = {For a common class of artificial neural networks, the mean integrated squared error between the estimated network and a target functionf is shown to be bounded by\$\$O\textbackslash left( \{\textbackslash frac\{\{\textbackslash mathop c\textbackslash nolimits\_f\^2 \}\}\{n\}\} \textbackslash right) + O\textbackslash left( \{\textbackslash frac\{\{nd\}\}\{N\}\textbackslash log  N\} \textbackslash right)\$\$wheren is the number of nodes,d is the input dimension of the function,N is the number of training observations, andCfis the first absolute moment of the Fourier magnitude distribution off. The two contributions to this total risk are the approximation error and the estimation error. Approximation error refers to the distance between the target function and the closest neural network function of a given architecture and estimation error refers to the distance between this ideal network function and an estimated network function. Withn {$\sim$} Cf(N/(d logN))1/2 nodes, the order of the bound on the mean integrated squared error is optimized to beO(Cf((d/N) logN)1/2). The bound demonstrates surprisingly favorable properties of network estimation compared to traditional series and nonparametric curve estimation techniques in the case thatd is moderately large. Similar bounds are obtained when the number of nodesn is not preselected as a function ofCf(which is generally not knowna priori), but rather the number of nodes is optimized from the observed data by the use of a complexity regularization or minimum description length criterion. The analysis involves Fourier techniques for the approximation error, metric entropy considerations for the estimation error, and a calculation of the index of resolvability of minimum complexity estimation of the family of networks.},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/CS2MZZXX/Barron - 1994 - Approximation and estimation bounds for artificial.pdf}
}

@article{barronApproximationEstimationHighDimensional2018,
  title = {Approximation and {{Estimation}} for {{High-Dimensional Deep Learning Networks}}},
  author = {Barron, Andrew R. and Klusowski, Jason M.},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.03090 [cs, stat]},
  eprint = {1809.03090},
  primaryclass = {cs, stat},
  urldate = {2021-04-21},
  abstract = {It has been experimentally observed in recent years that multi-layer artificial neural networks have a surprising ability to generalize, even when trained with far more parameters than observations. Is there a theoretical basis for this? The best available bounds on their metric entropy and associated complexity measures are essentially linear in the number of parameters, which is inadequate to explain this phenomenon. Here we examine the statistical risk (mean squared predictive error) of multi-layer networks with \$\textbackslash ell\^1\$-type controls on their parameters and with ramp activation functions (also called lower-rectified linear units). In this setting, the risk is shown to be upper bounded by \$[(L\^3 \textbackslash log d)/n]\^\{1/2\}\$, where \$d\$ is the input dimension to each layer, \$L\$ is the number of layers, and \$n\$ is the sample size. In this way, the input dimension can be much larger than the sample size and the estimator can still be accurate, provided the target function has such \$\textbackslash ell\^1\$ controls and that the sample size is at least moderately large compared to \$L\^3\textbackslash log d\$. The heart of the analysis is the development of a sampling strategy that demonstrates the accuracy of a sparse covering of deep ramp networks. Lower bounds show that the identified risk is close to being optimal.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/LQAQTE5L/Barron and Klusowski - 2018 - Approximation and Estimation for High-Dimensional .pdf;/Users/wyq977/Zotero/storage/2DXPA558/1809.html}
}

@article{barronApproximationLearningGreedy2008,
  title = {Approximation and Learning by Greedy Algorithms},
  author = {Barron, Andrew R. and Cohen, Albert and Dahmen, Wolfgang and DeVore, Ronald A.},
  year = {2008},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {36},
  number = {1},
  pages = {64--94},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053607000000631},
  urldate = {2023-03-20},
  abstract = {We consider the problem of approximating a given element f from a Hilbert space \$\textbackslash mathcal\{H\}\$ by means of greedy algorithms and the application of such procedures to the regression problem in statistical learning theory. We improve on the existing theory of convergence rates for both the orthogonal greedy algorithm and the relaxed greedy algorithm, as well as for the forward stepwise projection algorithm. For all these algorithms, we prove convergence results for a variety of function classes and not simply those that are related to the convex hull of the dictionary. We then show how these bounds for convergence rates lead to a new theory for the performance of greedy algorithms in learning. In particular, we build upon the results in [IEEE Trans. Inform. Theory 42 (1996) 2118\textendash 2132] to construct learning algorithms based on greedy approximations which are universally consistent and provide provable convergence rates for large classes of functions. The use of greedy algorithms in the context of learning is very appealing since it greatly reduces the computational burden when compared with standard model selection using general dictionaries.},
  keywords = {41A46,41A63,46N30,62G07,convergence rates for greedy algorithms,interpolation spaces,neural networks,Nonparametric regression,Statistical learning},
  file = {/Users/wyq977/Zotero/storage/XSSHNJEH/Barron et al. - 2008 - Approximation and learning by greedy algorithms.pdf}
}

@inproceedings{barronNeuralNetApproximation1992,
  title = {Neural Net Approximation},
  booktitle = {Proc. 7th {{Yale}} Workshop on Adaptive and Learning Systems},
  author = {Barron, Andrew R.},
  year = {1992},
  volume = {1},
  pages = {69--72},
  file = {/Users/wyq977/Zotero/storage/FS6I4UR9/neural net approximation.pdf}
}

@article{barronUniversalApproximationBounds1993,
  title = {Universal Approximation Bounds for Superpositions of a Sigmoidal Function},
  author = {Barron, A. R.},
  year = {1993},
  month = may,
  journal = {IEEE Transactions on Information Theory},
  volume = {39},
  number = {3},
  pages = {930--945},
  issn = {1557-9654},
  doi = {10.1109/18.256500},
  abstract = {Approximation properties of a class of artificial neural networks are established. It is shown that feedforward networks with one layer of sigmoidal nonlinearities achieve integrated squared error of order O(1/n), where n is the number of nodes. The approximated function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. The nonlinear parameters associated with the sigmoidal nodes, as well as the parameters of linear combination, are adjusted in the approximation. In contrast, it is shown that for series expansions with n terms, in which only the parameters of linear combination are adjusted, the integrated squared approximation error cannot be made smaller than order 1/n/sup 2/d/ uniformly for functions satisfying the same smoothness assumption, where d is the dimension of the input to the function. For the class of functions examined, the approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings.{$<>$}},
  keywords = {Approximation error,Artificial neural networks,Feedforward neural networks,Feeds,Fourier transforms,Information theory,Linear approximation,Neural networks,Statistical distributions,Statistics},
  file = {/Users/wyq977/Zotero/storage/3E86QZQI/Barron - 1993 - Universal approximation bounds for superpositions .pdf;/Users/wyq977/Zotero/storage/8TLMAXBS/256500.html}
}

@article{bartlettLocalRademacherComplexities2005,
  title = {Local {{Rademacher}} Complexities},
  author = {Bartlett, Peter L. and Bousquet, Olivier and Mendelson, Shahar},
  year = {2005},
  month = aug,
  journal = {The Annals of Statistics},
  volume = {33},
  number = {4},
  eprint = {math/0508275},
  issn = {0090-5364},
  doi = {10.1214/009053605000000282},
  urldate = {2021-05-12},
  abstract = {We propose new bounds on the error of learning algorithms in terms of a data-dependent notion of complexity. The estimates we establish give optimal rates and are based on a local and empirical version of Rademacher averages, in the sense that the Rademacher averages are computed from the data, on a subset of functions with small empirical error. We present some applications to classification and prediction with convex function classes, and with kernel classes in particular.},
  archiveprefix = {arxiv},
  keywords = {62G08; 68Q32 (Primary),Mathematics - Statistics Theory},
  file = {/Users/wyq977/Zotero/storage/EZ98ZI5Q/Bartlett et al. - 2005 - Local Rademacher complexities.pdf;/Users/wyq977/Zotero/storage/BUSDCDE9/0508275.html}
}

@article{beckMachineLearningApproximation2019,
  title = {Machine Learning Approximation Algorithms for High-Dimensional Fully Nonlinear Partial Differential Equations and Second-Order Backward Stochastic Differential Equations},
  author = {Beck, Christian and E, Weinan and Jentzen, Arnulf},
  year = {2019},
  month = aug,
  journal = {Journal of Nonlinear Science},
  volume = {29},
  number = {4},
  eprint = {1709.05963},
  primaryclass = {cs, math, stat},
  pages = {1563--1619},
  issn = {0938-8974, 1432-1467},
  doi = {10.1007/s00332-018-9525-3},
  urldate = {2023-02-17},
  abstract = {High-dimensional partial differential equations (PDE) appear in a number of models from the financial industry, such as in derivative pricing models, credit valuation adjustment (CVA) models, or portfolio optimization models. The PDEs in such applications are high-dimensional as the dimension corresponds to the number of financial assets in a portfolio. Moreover, such PDEs are often fully nonlinear due to the need to incorporate certain nonlinear phenomena in the model such as default risks, transaction costs, volatility uncertainty (Knightian uncertainty), or trading constraints in the model. Such high-dimensional fully nonlinear PDEs are exceedingly difficult to solve as the computational effort for standard approximation methods grows exponentially with the dimension. In this work we propose a new method for solving high-dimensional fully nonlinear second-order PDEs. Our method can in particular be used to sample from high-dimensional nonlinear expectations. The method is based on (i) a connection between fully nonlinear second-order PDEs and second-order backward stochastic differential equations (2BSDEs), (ii) a merged formulation of the PDE and the 2BSDE problem, (iii) a temporal forward discretization of the 2BSDE and a spatial approximation via deep neural nets, and (iv) a stochastic gradient descent-type optimization procedure. Numerical results obtained using \$\{\textbackslash rm T\{\textbackslash small ENSOR\}F\{\textbackslash small LOW\}\}\$ in \$\{\textbackslash rm P\{\textbackslash small YTHON\}\}\$ illustrate the efficiency and the accuracy of the method in the cases of a \$100\$-dimensional Black-Scholes-Barenblatt equation, a \$100\$-dimensional Hamilton-Jacobi-Bellman equation, and a nonlinear expectation of a \$ 100 \$-dimensional \$ G \$-Brownian motion.},
  archiveprefix = {arxiv},
  keywords = {65C99; 65M99; 60H30; 65-05,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Numerical Analysis,Mathematics - Probability,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/WU6A94MY/Beck et al. - 2019 - Machine learning approximation algorithms for high.pdf;/Users/wyq977/Zotero/storage/ZACTA8VF/1709.html}
}

@article{beckSolvingKolmogorovPDE2021,
  title = {Solving the {{Kolmogorov PDE}} by Means of Deep Learning},
  author = {Beck, Christian and Becker, Sebastian and Grohs, Philipp and Jaafari, Nor and Jentzen, Arnulf},
  year = {2021},
  month = sep,
  journal = {Journal of Scientific Computing},
  volume = {88},
  number = {3},
  eprint = {1806.00421},
  primaryclass = {cs, math, stat},
  pages = {73},
  issn = {0885-7474, 1573-7691},
  doi = {10.1007/s10915-021-01590-0},
  urldate = {2023-02-17},
  abstract = {Stochastic differential equations (SDEs) and the Kolmogorov partial differential equations (PDEs) associated to them have been widely used in models from engineering, finance, and the natural sciences. In particular, SDEs and Kolmogorov PDEs, respectively, are highly employed in models for the approximative pricing of financial derivatives. Kolmogorov PDEs and SDEs, respectively, can typically not be solved explicitly and it has been and still is an active topic of research to design and analyze numerical methods which are able to approximately solve Kolmogorov PDEs and SDEs, respectively. Nearly all approximation methods for Kolmogorov PDEs in the literature suffer under the curse of dimensionality or only provide approximations of the solution of the PDE at a single fixed space-time point. In this paper we derive and propose a numerical approximation method which aims to overcome both of the above mentioned drawbacks and intends to deliver a numerical approximation of the Kolmogorov PDE on an entire region \$[a,b]\^d\$ without suffering from the curse of dimensionality. Numerical results on examples including the heat equation, the Black-Scholes model, the stochastic Lorenz equation, and the Heston model suggest that the proposed approximation algorithm is quite effective in high dimensions in terms of both accuracy and speed.},
  archiveprefix = {arxiv},
  keywords = {65C99; 65M99; 60H30,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Probability,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/KT9LFFTQ/Beck et al. - 2021 - Solving the Kolmogorov PDE by means of deep learni.pdf;/Users/wyq977/Zotero/storage/WFQNWNLC/1806.html}
}

@article{bellmanTheoryDynamicProgramming1952,
  title = {On the {{Theory}} of {{Dynamic Programming}}},
  author = {Bellman, Richard},
  year = {1952},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {38},
  number = {8},
  pages = {716--719},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.38.8.716},
  urldate = {2023-02-17},
  file = {/Users/wyq977/Zotero/storage/CKT8EZZN/Bellman - 1952 - On the Theory of Dynamic Programming.pdf}
}

@misc{berlindRademacherComplexity,
  title = {Rademacher {{Complexity}}},
  author = {Berlind, Chris},
  urldate = {2022-06-20},
  file = {/Users/wyq977/Zotero/storage/V5J72EJI/Berlind - Rademacher Complexity.pdf}
}

@misc{bernerModernMathematicsDeep2021,
  title = {The Modern Mathematics of Deep Learning},
  author = {Berner, Julius and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
  year = {2021},
  month = may,
  number = {arXiv:2105.04026},
  eprint = {arXiv:2105.04026},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.04026},
  urldate = {2022-09-20},
  abstract = {We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/FT3STUMW/Berner et al. - 2021 - The Modern Mathematics of Deep Learning.pdf}
}

@incollection{bourgainDualityProblemEntropy1989,
  title = {On the Duality Problem for Entropy Numbers of Operators},
  booktitle = {Geometric {{Aspects}} of {{Functional Analysis}}},
  author = {Bourgain, J. and Pajor, A. and Szarek, S. J. and {Tomczak-Jaegermann}, N.},
  editor = {Lindenstrauss, Joram and Milman, Vitali D.},
  year = {1989},
  volume = {1376},
  pages = {50--63},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/BFb0090048},
  urldate = {2023-03-08},
  isbn = {978-3-540-51303-2},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/W6TDLSR4/Bourgain et al. - 1989 - On the duality problem for entropy numbers of oper.pdf}
}

@book{brezisFunctionalAnalysisSobolev2011,
  title = {Functional {{Analysis}}, {{Sobolev Spaces}} and {{Partial Differential Equations}}},
  author = {Brezis, Haim},
  year = {2011},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-70914-7},
  urldate = {2023-03-16},
  isbn = {978-0-387-70913-0 978-0-387-70914-7},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/XMFRJW8Y/Brezis - 2011 - Functional Analysis, Sobolev Spaces and Partial Di.pdf}
}

@misc{carageaNeuralNetworkApproximation2022,
  title = {Neural Network Approximation and Estimation of Classifiers with Classification Boundary in a {{Barron}} Class},
  author = {Caragea, Andrei and Petersen, Philipp and Voigtlaender, Felix},
  year = {2022},
  month = mar,
  number = {arXiv:2011.09363},
  eprint = {arXiv:2011.09363},
  publisher = {{arXiv}},
  urldate = {2022-09-20},
  abstract = {We prove bounds for the approximation and estimation of certain binary classification functions using ReLU neural networks. Our estimation bounds provide a priori performance guarantees for empirical risk minimization using networks of a suitable size, depending on the number of training samples available. The obtained approximation and estimation rates are independent of the dimension of the input, showing that the curse of dimensionality can be overcome in this setting; in fact, the input dimension only enters in the form of a polynomial factor. Regarding the regularity of the target classification function, we assume the interfaces between the different classes to be locally of Barron-type. We complement our results by studying the relations between various Barron-type spaces that have been proposed in the literature. These spaces differ substantially more from each other than the current literature suggests.},
  archiveprefix = {arxiv},
  keywords = {68T07; 41A25; 41A46; 42B35; 46E15,Mathematics - Functional Analysis,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/2UZ24WB4/Caragea et al. - 2022 - Neural network approximation and estimation of cla.pdf}
}

@inproceedings{carrollConstructionNeuralNets1989,
  title = {Construction of Neural Nets Using the Radon Transform},
  booktitle = {International 1989 {{Joint Conference}} on {{Neural Networks}}},
  author = {{Carroll} and {Dickinson}},
  year = {1989},
  pages = {607-611 vol.1},
  doi = {10.1109/IJCNN.1989.118639},
  abstract = {The authors present a method for constructing a feedforward neural net implementing an arbitrarily good approximation to any L/sub 2/ function over (-1, 1)/sup n/. The net uses n input nodes, a single hidden layer whose width is determined by the function to be implemented and the allowable mean square error, and a linear output neuron. Error bounds and an example are given for the method.{$<>$}},
  keywords = {Approximation methods,Neural networks},
  file = {/Users/wyq977/Zotero/storage/AKITFRAD/118639.html}
}

@article{cheangBetterApproximationBalls2000,
  title = {A {{Better Approximation}} for {{Balls}}},
  author = {Cheang, Gerald H.L. and Barron, Andrew R.},
  year = {2000},
  month = jun,
  journal = {Journal of Approximation Theory},
  volume = {104},
  number = {2},
  pages = {183--203},
  issn = {00219045},
  doi = {10.1006/jath.1999.3441},
  urldate = {2023-02-27},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/PMHJLXE3/Cheang and Barron - 2000 - A Better Approximation for Balls.pdf}
}

@misc{chenBrauerGroupRational2019,
  title = {The {{Brauer Group}} of {{Rational Numbers}}},
  author = {Chen, Haiyu},
  year = {2019},
  month = nov,
  number = {arXiv:1911.02368},
  eprint = {arXiv:1911.02368},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.02368},
  urldate = {2023-03-13},
  abstract = {In this project, we will study the Brauer group that was first defined by R. Brauer. The elements of the Brauer group are the equivalence classes of finite dimensional central simple algebra. Therefore understanding the structure of the Brauer group of a field is equivalent to a complete classification of finite dimensional central division algebras over the field. One of the important achievements of algebra and number theory in the last century is the determination of Br(Q), the Brauer group of rational numbers. The aim of this dissertation is to review this project, i.e., determining Br(Q). There are three main steps. The first step is to determine Br(R), the Brauer group of real numbers. The second step is to identify Br(k\_\textbackslash nu), the Brauer group of the local fields. The third step is to construct two maps Br(Q) to Br(R) and Br(Q) to Br(Q\_p) and to use these two maps to understand Br(Q). This dissertation completed the first two steps of this enormous project. To the author's knowledge, in literature there is no document including all the details of determining Br(Q) and most of them are written from a advanced perspective that requires the knowledge of class field theory and cohomology. The goal of this document is to develop the result in a relatively elementary way. The project mainly follows the logic of the book [6], but significant amount of details are added and some proofs are originated by the author, for example, 1.2.6, 1.4.2(ii), 4.2.6, and maximality and uniqueness of 5.5.12.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Group Theory,Mathematics - History and Overview,Mathematics - Number Theory,Mathematics - Rings and Algebras},
  file = {/Users/wyq977/Zotero/storage/XWPZBZZE/Chen - 2019 - The Brauer Group of Rational Numbers.pdf;/Users/wyq977/Zotero/storage/9UFMR757/1911.html}
}

@article{chengNeuralNetworksReview1994,
  title = {Neural {{Networks}}: {{A Review}} from a {{Statistical Perspective}}},
  shorttitle = {Neural {{Networks}}},
  author = {Cheng, Bing and Titterington, D. M.},
  year = {1994},
  journal = {Statistical Science},
  volume = {9},
  number = {1},
  eprint = {2246275},
  eprinttype = {jstor},
  pages = {2--30},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237},
  urldate = {2023-03-20},
  abstract = {This paper informs a statistical readership about Artificial Neural Networks (ANNs), points out some of the links with statistical methodology and encourages cross-disciplinary research in the directions most likely to bear fruit. The areas of statistical interest are briefly outlined, and a series of examples indicates the flavor of ANN models. We then treat various topics in more depth. In each case, we describe the neural network architectures and training rules and provide a statistical commentary. The topics treated in this way are perceptrons (from single-unit to multilayer versions), Hopfield-type recurrent networks (including probabilistic versions strongly related to statistical physics and Gibbs distributions) and associative memory networks trained by so-called unsuperviszd learning rules. Perceptrons are shown to have strong associations with discriminant analysis and regression, and unsupervized networks with cluster analysis. The paper concludes with some thoughts on the future of the interface between neural networks and statistics.},
  file = {/Users/wyq977/Zotero/storage/69JF9E3L/Cheng and Titterington - 1994 - Neural Networks A Review from a Statistical Persp.pdf}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  year = {1989},
  month = dec,
  journal = {Mathematics of Control, Signals and Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  urldate = {2022-06-23},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  langid = {english},
  keywords = {Approximation,Completeness,Neural networks},
  file = {/Users/wyq977/Zotero/storage/BBT8674Z/Cybenko - 1989 - Approximation by superpositions of a sigmoidal fun.pdf}
}

@misc{dehoopConvergenceRatesLearning2022,
  title = {Convergence {{Rates}} for {{Learning Linear Operators}} from {{Noisy Data}}},
  author = {{de Hoop}, Maarten V. and Kovachki, Nikola B. and Nelsen, Nicholas H. and Stuart, Andrew M.},
  year = {2022},
  month = nov,
  number = {arXiv:2108.12515},
  eprint = {arXiv:2108.12515},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.12515},
  urldate = {2023-02-17},
  abstract = {This paper studies the learning of linear operators between infinite-dimensional Hilbert spaces. The training data comprises pairs of random input vectors in a Hilbert space and their noisy images under an unknown self-adjoint linear operator. Assuming that the operator is diagonalizable in a known basis, this work solves the equivalent inverse problem of estimating the operator's eigenvalues given the data. Adopting a Bayesian approach, the theoretical analysis establishes posterior contraction rates in the infinite data limit with Gaussian priors that are not directly linked to the forward map of the inverse problem. The main results also include learning-theoretic generalization error guarantees for a wide range of distribution shifts. These convergence rates quantify the effects of data smoothness and true eigenvalue decay or growth, for compact or unbounded operators, respectively, on sample complexity. Numerical evidence supports the theory in diagonal and non-diagonal settings.},
  archiveprefix = {arxiv},
  keywords = {62G20; 62C10; 68T05; 47A62,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/wyq977/Zotero/storage/RFD3AJA5/de Hoop et al. - 2022 - Convergence Rates for Learning Linear Operators fr.pdf;/Users/wyq977/Zotero/storage/Q7PJJS7R/2108.html}
}

@article{devore_1998,
  title = {Nonlinear Approximation},
  author = {DeVore, Ronald A.},
  year = {1998},
  journal = {Acta Numerica},
  volume = {7},
  pages = {51--150},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/S0962492900002816},
  file = {/Users/wyq977/Zotero/storage/U2RMU2JW/DeVore - 1998 - Nonlinear approximation.pdf}
}

@article{devoreNeuralNetworkApproximation2021,
  title = {Neural Network Approximation},
  author = {DeVore, Ronald and Hanin, Boris and Petrova, Guergana},
  year = {2021},
  month = may,
  journal = {Acta Numerica},
  volume = {30},
  pages = {327--444},
  issn = {0962-4929, 1474-0508},
  doi = {10.1017/S0962492921000052},
  urldate = {2023-02-13},
  abstract = {Neural networks (NNs) are the method of choice for building learning algorithms. They are now being investigated for other numerical tasks such as solving high-dimensional partial differential equations. Their popularity stems from their empirical success on several challenging learning problems (computer chess/Go, autonomous navigation, face recognition). However, most scholars agree that a convincing theoretical explanation for this success is still lacking. Since these applications revolve around approximating an unknown function from data observations, part of the answer must involve the ability of NNs to produce accurate approximations.                            This article surveys the known approximation properties of the outputs of NNs with the aim of uncovering the properties that are not present in the more traditional methods of approximation used in numerical analysis, such as approximations using polynomials, wavelets, rational functions and splines. Comparisons are made with traditional approximation methods from the viewpoint of rate distortion,               i.e.               error versus the number of parameters used to create the approximant. Another major component in the analysis of numerical approximation is the computational time needed to construct the approximation, and this in turn is intimately connected with the stability of the approximation algorithm. So the stability of numerical approximation using NNs is a large part of the analysis put forward.                                         The survey, for the most part, is concerned with NNs using the popular ReLU activation function. In this case the outputs of the NNs are piecewise linear functions on rather complicated partitions of the domain of               f               into cells that are convex polytopes. When the architecture of the NN is fixed and the parameters are allowed to vary, the set of output functions of the NN is a parametrized nonlinear manifold. It is shown that this manifold has certain space-filling properties leading to an increased ability to approximate (better rate distortion) but at the expense of numerical stability. The space filling creates the challenge to the numerical method of finding best or good parameter choices when trying to approximate.},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/6JZAHZF3/DeVore et al. - 2021 - Neural network approximation.pdf}
}

@inproceedings{donohoHighDimensionalDataAnalysis2000,
  title = {High-{{Dimensional Data Analysis}}: {{The Curses}} and {{Blessings}} of {{Dimensionality}}},
  shorttitle = {High-Dimensional Data Analysis},
  booktitle = {{{AMS Conference}} on {{Math Challenges}} of the 21st {{Century}}},
  author = {Donoho, David L.},
  year = {2000},
  pages = {33},
  address = {{Los Angeles, CA, USA}},
  abstract = {The coming century is surely the century of data. A combination of blind faith and serious purpose makes our society invest massively in the collection and processing of data of all kinds, on scales unimaginable until recently. Hyperspectral Imagery, Internet Portals, Financial tick-by-tick data, and DNA Microarrays are just a few of the betterknown sources, feeding data in torrential streams into scientific and business databases worldwide. In traditional statistical data analysis, we think of observations of instances of particular phenomena (e.g. instance {$\leftrightarrow$} human being), these observations being a vector of values we measured on several variables (e.g. blood pressure, weight, height,...). In traditional statistical methodology, we assumed many observations and a few, wellchosen variables. The trend today is towards more observations but even more so, to radically larger numbers of variables \textendash{} voracious, automatic, systematic collection of hyper-informative detail about each observed instance. We are seeing examples where the observations gathered on individual instances are curves, or spectra, or images, or},
  file = {/Users/wyq977/Zotero/storage/2ZMTH9W7/Donoho - 2000 - High-dimensional data analysis The curses and ble.pdf;/Users/wyq977/Zotero/storage/WNWD6BNM/summary.html}
}

@article{eBanachSpacesAssociated2020,
  title = {On the {{Banach}} Spaces Associated with Multi-Layer {{ReLU}} Networks: {{Function}} Representation, Approximation Theory and Gradient Descent Dynamics},
  shorttitle = {On the {{Banach}} Spaces Associated with Multi-Layer {{ReLU}} Networks},
  author = {E, Weinan and Wojtowytsch, Stephan},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.15623 [cs, math, stat]},
  eprint = {2007.15623},
  primaryclass = {cs, math, stat},
  urldate = {2021-05-11},
  abstract = {We develop Banach spaces for ReLU neural networks of finite depth \$L\$ and infinite width. The spaces contain all finite fully connected \$L\$-layer networks and their \$L\^2\$-limiting objects under bounds on the natural path-norm. Under this norm, the unit ball in the space for \$L\$-layer networks has low Rademacher complexity and thus favorable generalization properties. Functions in these spaces can be approximated by multi-layer neural networks with dimension-independent convergence rates. The key to this work is a new way of representing functions in some form of expectations, motivated by multi-layer neural networks. This representation allows us to define a new class of continuous models for machine learning. We show that the gradient flow defined this way is the natural continuous analog of the gradient descent dynamics for the associated multi-layer neural networks. We show that the path-norm increases at most polynomially under this continuous gradient flow dynamics.},
  archiveprefix = {arxiv},
  keywords = {68T07; 46E15; 26B35; 26B40,Computer Science - Machine Learning,Mathematics - Functional Analysis,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/U5TPALKY/E and Wojtowytsch - 2020 - On the Banach spaces associated with multi-layer R.pdf;/Users/wyq977/Zotero/storage/TZFI8HDC/2007.html}
}

@article{eBarronSpaceFlowinduced2021,
  title = {The {{Barron Space}} and the {{Flow-induced Function Spaces}} for {{Neural Network Models}}},
  author = {E, Weinan and Ma, Chao and Wu, Lei},
  year = {2021},
  month = mar,
  journal = {arXiv:1906.08039 [cs, math, stat]},
  eprint = {1906.08039},
  primaryclass = {cs, math, stat},
  urldate = {2021-04-19},
  abstract = {One of the key issues in the analysis of machine learning models is to identify the appropriate function space and norm for the model. This is the set of functions endowed with a quantity which can control the approximation and estimation errors by a particular machine learning model. In this paper, we address this issue for two representative neural network models: the two-layer networks and the residual neural networks. We define the Barron space and show that it is the right space for two-layer neural network models in the sense that optimal direct and inverse approximation theorems hold for functions in the Barron space. For residual neural network models, we construct the so-called flow-induced function space, and prove direct and inverse approximation theorems for this space. In addition, we show that the Rademacher complexity for bounded sets under these norms has the optimal upper bounds.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Probability,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/QHIFWQDB/E et al. - 2021 - The Barron Space and the Flow-induced Function Spa.pdf}
}

@article{eComparativeAnalysisOptimization2020,
  title = {A {{Comparative Analysis}} of the {{Optimization}} and {{Generalization Property}} of {{Two-layer Neural Network}} and {{Random Feature Models Under Gradient Descent Dynamics}}},
  author = {E, Weinan and Ma, Chao and Wu, Lei},
  year = {2020},
  month = jul,
  journal = {Science China Mathematics},
  volume = {63},
  number = {7},
  eprint = {1904.04326},
  primaryclass = {cs, math, stat},
  pages = {1235--1258},
  issn = {1674-7283, 1869-1862},
  doi = {10.1007/s11425-019-1628-5},
  urldate = {2022-09-21},
  abstract = {A fairly comprehensive analysis is presented for the gradient descent dynamics for training two-layer neural network models in the situation when the parameters in both layers are updated. General initialization schemes as well as general regimes for the network width and training data size are considered. In the over-parametrized regime, it is shown that gradient descent dynamics can achieve zero training loss exponentially fast regardless of the quality of the labels. In addition, it is proved that throughout the training process the functions represented by the neural network model are uniformly close to that of a kernel method. For general values of the network width and training data size, sharp estimates of the generalization error is established for target functions in the appropriate reproducing kernel Hilbert space.},
  archiveprefix = {arxiv},
  keywords = {41A99; 49M99,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/YYTDVG9F/E et al. - 2020 - A Comparative Analysis of the Optimization and Gen.pdf;/Users/wyq977/Zotero/storage/ZEDVPGKA/1904.html}
}

@article{eDeepLearningbasedNumerical2017,
  title = {Deep Learning-Based Numerical Methods for High-Dimensional Parabolic Partial Differential Equations and Backward Stochastic Differential Equations},
  author = {E, Weinan and Han, Jiequn and Jentzen, Arnulf},
  year = {2017},
  month = dec,
  journal = {Communications in Mathematics and Statistics},
  volume = {5},
  number = {4},
  eprint = {1706.04702},
  primaryclass = {cs, math, stat},
  pages = {349--380},
  issn = {2194-6701, 2194-671X},
  doi = {10.1007/s40304-017-0117-6},
  urldate = {2023-02-17},
  abstract = {We propose a new algorithm for solving parabolic partial differential equations (PDEs) and backward stochastic differential equations (BSDEs) in high dimension, by making an analogy between the BSDE and reinforcement learning with the gradient of the solution playing the role of the policy function, and the loss function given by the error between the prescribed terminal condition and the solution of the BSDE. The policy function is then approximated by a neural network, as is done in deep reinforcement learning. Numerical results using TensorFlow illustrate the efficiency and accuracy of the proposed algorithms for several 100-dimensional nonlinear PDEs from physics and finance such as the Allen-Cahn equation, the Hamilton-Jacobi-Bellman equation, and a nonlinear pricing model for financial derivatives.},
  archiveprefix = {arxiv},
  keywords = {65M75; 60H35; 65C30,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Numerical Analysis,Mathematics - Probability,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/92JTS92C/E et al. - 2017 - Deep learning-based numerical methods for high-dim.pdf;/Users/wyq977/Zotero/storage/FS3ZTWVM/1706.html}
}

@misc{eDeepRitzMethod2017,
  title = {The {{Deep Ritz}} Method: {{A}} Deep Learning-Based Numerical Algorithm for Solving Variational Problems},
  shorttitle = {The {{Deep Ritz}} Method},
  author = {E, Weinan and Yu, Bing},
  year = {2017},
  month = sep,
  number = {arXiv:1710.00211},
  eprint = {arXiv:1710.00211},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1710.00211},
  urldate = {2023-02-17},
  abstract = {We propose a deep learning based method, the Deep Ritz Method, for numerically solving variational problems, particularly the ones that arise from partial differential equations. The Deep Ritz method is naturally nonlinear, naturally adaptive and has the potential to work in rather high dimensions. The framework is quite simple and fits well with the stochastic gradient descent method used in deep learning. We illustrate the method on several problems including some eigenvalue problems.},
  archiveprefix = {arxiv},
  keywords = {35Q68,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/F4886SFW/E and Yu - 2017 - The Deep Ritz method A deep learning-based numeri.pdf;/Users/wyq977/Zotero/storage/H9E2EFXR/1710.html}
}

@article{eMathematicalUnderstandingNeural2020,
  title = {Towards a Mathematical Understanding of Neural Network-Based Machine Learning: What We Know and What We Don't},
  shorttitle = {Towards a {{Mathematical Understanding}} of {{Neural Network-Based Machine Learning}}},
  author = {E, Weinan and Ma, Chao and Wojtowytsch, Stephan and Wu, Lei},
  year = {2020},
  month = dec,
  journal = {arXiv:2009.10713 [cs, math, stat]},
  eprint = {2009.10713},
  primaryclass = {cs, math, stat},
  urldate = {2021-04-21},
  abstract = {The purpose of this article is to review the achievements made in the last few years towards the understanding of the reasons behind the success and subtleties of neural network-based machine learning. In the tradition of good old applied mathematics, we will not only give attention to rigorous mathematical results, but also the insight we have gained from careful numerical experiments as well as the analysis of simplified models. Along the way, we also list the open problems which we believe to be the most important topics for further study. This is not a complete overview over this quickly moving field, but we hope to provide a perspective which may be helpful especially to new researchers in the area.},
  archiveprefix = {arxiv},
  keywords = {68T07 (primary); 26B40; 41A30; 35Q68,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/JVSZJFM4/E et al. - 2020 - Towards a Mathematical Understanding of Neural Net.pdf;/Users/wyq977/Zotero/storage/NQP5W5AR/2009.html}
}

@article{eObservationsPartialDifferential2020,
  title = {Some Observations on Partial Differential Equations in {{Barron}} and Multi-Layer Spaces},
  author = {E, Weinan and Wojtowytsch, Stephan},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.01484 [cs, math]},
  eprint = {2012.01484},
  primaryclass = {cs, math},
  urldate = {2021-04-21},
  abstract = {We use explicit representation formulas to show that solutions to certain partial differential equations lie in Barron spaces or multilayer spaces if the PDE data lie in such function spaces. Consequently, these solutions can be represented efficiently using artificial neural networks, even in high dimension. Conversely, we present examples in which the solution fails to lie in the function space associated to a neural network under consideration.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {68T07; 35C15; 65M80,Computer Science - Machine Learning,Mathematics - Analysis of PDEs},
  file = {/Users/wyq977/Zotero/storage/LUFYJVBI/E and Wojtowytsch - 2020 - Some observations on partial differential equation.pdf}
}

@article{ePrioriEstimatesPopulation2019,
  title = {A {{Priori Estimates}} of the {{Population Risk}} for {{Residual Networks}}},
  author = {E, Weinan and Ma, Chao and Wang, Qingcan},
  year = {2019},
  month = may,
  journal = {arXiv:1903.02154 [cs, stat]},
  eprint = {1903.02154},
  primaryclass = {cs, stat},
  urldate = {2021-04-29},
  abstract = {Optimal a priori estimates are derived for the population risk, also known as the generalization error, of a regularized residual network model. An important part of the regularized model is the usage of a new path norm, called the weighted path norm, as the regularization term. The weighted path norm treats the skip connections and the nonlinearities differently so that paths with more nonlinearities are regularized by larger weights. The error estimates are a priori in the sense that the estimates depend only on the target function, not on the parameters obtained in the training process. The estimates are optimal, in a high dimensional setting, in the sense that both the bound for the approximation and estimation errors are comparable to the Monte Carlo error rates. A crucial step in the proof is to establish an optimal bound for the Rademacher complexity of the residual networks. Comparisons are made with existing norm-based generalization error bounds.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/ZFYMGJVC/E et al. - 2019 - A Priori Estimates of the Population Risk for Resi.pdf;/Users/wyq977/Zotero/storage/WJ3ZNUL8/1903.html}
}

@article{ePrioriEstimatesPopulation2019a,
  title = {A {{Priori Estimates}} of the {{Population Risk}} for {{Two-layer Neural Networks}}},
  author = {E, Weinan and Ma, Chao and Wu, Lei},
  year = {2019},
  journal = {Communications in Mathematical Sciences},
  volume = {17},
  number = {5},
  eprint = {1810.06397},
  pages = {1407--1425},
  issn = {15396746, 19450796},
  doi = {10.4310/CMS.2019.v17.n5.a11},
  urldate = {2021-04-19},
  abstract = {New estimates for the population risk are established for two-layer neural networks. These estimates are nearly optimal in the sense that the error rates scale in the same way as the Monte Carlo error rates. They are equally effective in the over-parametrized regime when the network size is much larger than the size of the dataset. These new estimates are a priori in nature in the sense that the bounds depend only on some norms of the underlying functions to be fitted, not the parameters in the model, in contrast with most existing results which are a posteriori in nature. Using these a priori estimates, we provide a perspective for understanding why two-layer neural networks perform better than the related kernel methods.},
  archiveprefix = {arxiv},
  keywords = {41A46; 41A63; 62J02; 65D05,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/2SMQ4IAB/E et al. - 2019 - A Priori Estimates of the Population Risk for Two-.pdf;/Users/wyq977/Zotero/storage/89W733GF/1810.html}
}

@article{eRepresentationFormulasPointwise2020,
  title = {Representation Formulas and Pointwise Properties for {{Barron}} Functions},
  author = {E, Weinan and Wojtowytsch, Stephan},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.05982 [cs, math, stat]},
  eprint = {2006.05982},
  primaryclass = {cs, math, stat},
  urldate = {2021-04-19},
  abstract = {We study the natural function space for infinitely wide two-layer neural networks and establish different representation formulae. In two cases, we describe the space explicitly up to isomorphism. Using a convenient representation, we study the pointwise properties of two-layer networks and show that functions whose singular set is fractal or curved (for example distance functions from smooth submanifolds) cannot be represented by infinitely wide two-layer networks with finite path-norm.},
  archiveprefix = {arxiv},
  keywords = {68T07; 46E15; 26B35; 26B40,Computer Science - Machine Learning,Mathematics - Analysis of PDEs,Mathematics - Functional Analysis,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/3ACSK94A/E and Wojtowytsch - 2020 - Representation formulas and pointwise properties f.pdf;/Users/wyq977/Zotero/storage/FYRRN8LS/2006.html}
}

@article{funahashiApproximateRealizationContinuous1989,
  title = {On the Approximate Realization of Continuous Mappings by Neural Networks},
  author = {Funahashi, Ken-Ichi},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {3},
  pages = {183--192},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90003-8},
  urldate = {2023-03-23},
  abstract = {In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the Kolmogorov-Arnold-Sprecher theorem and this proof also gives non-trivial realizations.},
  langid = {english},
  keywords = {Back propagation,Continuous mapping,Hidden layer,Neural network,Output function,Realization,Sigmoid function,Unit},
  file = {/Users/wyq977/Zotero/storage/6YTD48IM/0893608089900038.html}
}

@article{gaoEntropyAbsoluteConvex2004,
  title = {Entropy of {{Absolute Convex Hulls}} in {{Hilbert Spaces}}},
  author = {Gao, Fuchang},
  year = {2004},
  month = jul,
  journal = {Bulletin of the London Mathematical Society},
  volume = {36},
  number = {4},
  pages = {460--468},
  publisher = {{Cambridge University Press}},
  issn = {1469-2120, 0024-6093},
  doi = {10.1112/S0024609304003121},
  urldate = {2022-12-13},
  abstract = {The metric entropy of absolute convex hulls of sets in Hilbert spaces is studied for the general case when the metric entropy of the sets is arbitrary. Under some regularity assumptions, the results are sharp.},
  langid = {english},
  keywords = {41A46 (primary),60G15 (secondary)},
  file = {/Users/wyq977/Zotero/storage/5X6KDQVF/Gao - 2004 - ENTROPY OF ABSOLUTE CONVEX HULLS IN HILBERT SPACES.pdf}
}

@article{halabiCombinatorialPenaltiesWhich2018,
  title = {Combinatorial {{Penalties}}: {{Which}} Structures Are Preserved by Convex Relaxations?},
  shorttitle = {Combinatorial {{Penalties}}},
  author = {Halabi, Marwa El and Bach, Francis and Cevher, Volkan},
  year = {2018},
  month = mar,
  journal = {arXiv:1710.06273 [cs, stat]},
  eprint = {1710.06273},
  primaryclass = {cs, stat},
  urldate = {2021-04-19},
  abstract = {We consider the homogeneous and the nonhomogeneous convex relaxations for combinatorial penalty functions defined on support sets. Our study identifies key differences in the tightness of the resulting relaxations through the notion of the lower combinatorial envelope of a setfunction along with new necessary conditions for support identification. We then propose a general adaptive estimator for convex monotone regularizers, and derive new sufficient conditions for support recovery in the asymptotic setting.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/AYM3T5MW/Halabi et al. - 2018 - Combinatorial Penalties Which structures are pres.pdf}
}

@article{hanClassDimensionalityfreeMetrics2021,
  title = {A {{Class}} of {{Dimensionality-free Metrics}} for the {{Convergence}} of {{Empirical Measures}}},
  author = {Han, Jiequn and Hu, Ruimeng and Long, Jihao},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.12036 [cs, math, stat]},
  eprint = {2104.12036},
  primaryclass = {cs, math, stat},
  urldate = {2021-05-13},
  abstract = {This paper concerns the convergence of empirical measures in high dimensions. We propose a new class of metrics and show that under such metrics, the convergence is free of the curse of dimensionality (CoD). Such a feature is critical for high-dimensional analysis and stands in contrast to classical metrics (\{\textbackslash it e.g.\}, the Wasserstein distance). The proposed metrics originate from the maximum mean discrepancy, which we generalize by proposing specific criteria for selecting test function spaces to guarantee the property of being free of CoD. Therefore, we call this class of metrics the generalized maximum mean discrepancy (GMMD). Examples of the selected test function spaces include the reproducing kernel Hilbert space, Barron space, and flow-induced function spaces. Three applications of the proposed metrics are presented: 1. The convergence of empirical measure in the case of random variables; 2. The convergence of \$n\$-particle system to the solution to McKean-Vlasov stochastic differential equation; 3. The construction of an \$\textbackslash varepsilon\$-Nash equilibrium for a homogeneous \$n\$-player game by its mean-field limit. As a byproduct, we prove that, given a distribution close to the target distribution measured by GMMD and a certain representation of the target distribution, we can generate a distribution close to the target one in terms of the Wasserstein distance and relative entropy. Overall, we show that the proposed class of metrics is a powerful tool to analyze the convergence of empirical measures in high dimensions without CoD.},
  archiveprefix = {arxiv},
  keywords = {60B10; 60E15; 60K35; 91A16; 60H10,Computer Science - Machine Learning,Mathematics - Probability,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/Z78ZRZAD/Han et al. - 2021 - A Class of Dimensionality-free Metrics for the Con.pdf;/Users/wyq977/Zotero/storage/BTHQW4WR/2104.html}
}

@misc{heissHowImplicitRegularization2021,
  title = {How {{Implicit Regularization}} of {{ReLU Neural Networks Characterizes}} the {{Learned Function}} -- {{Part I}}: The 1-{{D Case}} of {{Two Layers}} with {{Random First Layer}}},
  shorttitle = {How {{Implicit Regularization}} of {{ReLU Neural Networks Characterizes}} the {{Learned Function}} -- {{Part I}}},
  author = {Heiss, Jakob and Teichmann, Josef and Wutte, Hanna},
  year = {2021},
  month = jul,
  number = {arXiv:1911.02903},
  eprint = {arXiv:1911.02903},
  publisher = {{arXiv}},
  urldate = {2023-01-30},
  abstract = {Today, various forms of neural networks are trained to perform approximation tasks in many fields. However, the estimates obtained are not fully understood on function space. Empirical results suggest that typical training algorithms favor regularized solutions. These observations motivate us to analyze properties of the neural networks found by gradient descent initialized close to zero, that is frequently employed to perform the training task. As a starting point, we consider one dimensional (shallow) ReLU neural networks in which weights are chosen randomly and only the terminal layer is trained. First, we rigorously show that for such networks ridge regularized regression corresponds in function space to regularizing the estimate's second derivative for fairly general loss functionals. For least squares regression, we show that the trained network converges to the smooth spline interpolation of the training data as the number of hidden nodes tends to infinity. Moreover, we derive a correspondence between the early stopped gradient descent and the smoothing spline regression. Our analysis might give valuable insight on the properties of the solutions obtained using gradient descent methods in general settings.},
  archiveprefix = {arxiv},
  keywords = {41Axx; 93Exx; 68T05; 68Q32,Computer Science - Machine Learning,G.3,I.2.6,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/3Y3V3HZW/Heiss et al. - 2021 - How Implicit Regularization of ReLU Neural Network.pdf;/Users/wyq977/Zotero/storage/PCAW75PE/1911.html}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {08936080},
  doi = {10.1016/0893-6080(89)90020-8},
  urldate = {2022-10-11},
  langid = {english},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  file = {/Users/wyq977/Zotero/storage/PU5STPM5/Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf;/Users/wyq977/Zotero/storage/M93P3ZNU/0893608089900208.html}
}

@article{hornikMultilayerFeedforwardNetworks1989a,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  urldate = {2023-03-23},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  langid = {english},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  file = {/Users/wyq977/Zotero/storage/INBVJYWN/0893608089900208.html}
}

@inproceedings{irieCapabilitiesThreelayeredPerceptrons1988,
  title = {Capabilities of Three-Layered Perceptrons},
  booktitle = {{{IEEE}} 1988 {{International Conference}} on {{Neural Networks}}},
  author = {{Irie} and {Miyake}},
  year = {1988},
  month = jul,
  pages = {641-648 vol.1},
  doi = {10.1109/ICNN.1988.23901},
  abstract = {A theorem is proved to the effect that three-layered perceptrons with an infinite number of computing units can represent arbitrary mapping if the desired mapping and the input-output characteristics of the computing units satisfy some constraints. The proof is constructive, and each coefficient is explicitly presented. The theorem theoretically guarantees a kind of universality for three-layered perceptrons. Although two-layered perceptrons (simple perceptrons) cannot represent arbitrary functions, three-layers prove necessary and sufficient. The relationship between the model used in the proof and the distributed storage and processing of information is also discussed.{$<>$}},
  keywords = {Neural networks},
  file = {/Users/wyq977/Zotero/storage/XKSR6SUR/stamp.html}
}

@misc{javanmardAnalysisTwoLayerNeural2019,
  title = {Analysis of a {{Two-Layer Neural Network}} via {{Displacement Convexity}}},
  author = {Javanmard, Adel and Mondelli, Marco and Montanari, Andrea},
  year = {2019},
  month = aug,
  number = {arXiv:1901.01375},
  eprint = {arXiv:1901.01375},
  publisher = {{arXiv}},
  urldate = {2022-06-23},
  abstract = {Fitting a function by using linear combinations of a large number \$N\$ of `simple' components is one of the most fruitful ideas in statistical learning. This idea lies at the core of a variety of methods, from two-layer neural networks to kernel regression, to boosting. In general, the resulting risk minimization problem is non-convex and is solved by gradient descent or its variants. Unfortunately, little is known about global convergence properties of these approaches. Here we consider the problem of learning a concave function \$f\$ on a compact convex domain \$\textbackslash Omega\textbackslash subseteq \{\textbackslash mathbb R\}\^d\$, using linear combinations of `bump-like' components (neurons). The parameters to be fitted are the centers of \$N\$ bumps, and the resulting empirical risk minimization problem is highly non-convex. We prove that, in the limit in which the number of neurons diverges, the evolution of gradient descent converges to a Wasserstein gradient flow in the space of probability distributions over \$\textbackslash Omega\$. Further, when the bump width \$\textbackslash delta\$ tends to \$0\$, this gradient flow has a limit which is a viscous porous medium equation. Remarkably, the cost function optimized by this gradient flow exhibits a special property known as displacement convexity, which implies exponential convergence rates for \$N\textbackslash to\textbackslash infty\$, \$\textbackslash delta\textbackslash to 0\$. Surprisingly, this asymptotic theory appears to capture well the behavior for moderate values of \$\textbackslash delta, N\$. Explaining this phenomenon, and understanding the dependence on \$\textbackslash delta,N\$ in a quantitative manner remains an outstanding challenge.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory},
  file = {/Users/wyq977/Zotero/storage/C3G32MF3/Javanmard et al. - 2019 - Analysis of a Two-Layer Neural Network via Displac.pdf;/Users/wyq977/Zotero/storage/5LPAB7AJ/1901.html}
}

@misc{jinLogicalFallacyDetection2022,
  title = {Logical {{Fallacy Detection}}},
  author = {Jin, Zhijing and Lalwani, Abhinav and Vaidhya, Tejas and Shen, Xiaoyu and Ding, Yiwen and Lyu, Zhiheng and Sachan, Mrinmaya and Mihalcea, Rada and Sch{\"o}lkopf, Bernhard},
  year = {2022},
  month = may,
  number = {arXiv:2202.13758},
  eprint = {arXiv:2202.13758},
  publisher = {{arXiv}},
  urldate = {2022-09-28},
  abstract = {Reasoning is central to human intelligence. However, fallacious arguments are common, and some exacerbate problems such as spreading misinformation about climate change. In this paper, we propose the task of logical fallacy detection, and provide a new dataset (Logic) of logical fallacies generally found in text, together with an additional challenge set for detecting logical fallacies in climate change claims (LogicClimate). Detecting logical fallacies is a hard problem as the model must understand the underlying logical structure of the argument. We find that existing pretrained large language models perform poorly on this task. In contrast, we show that a simple structure-aware classifier outperforms the best language model by 5.46\% on Logic and 4.51\% on LogicClimate. We encourage future work to explore this task as (a) it can serve as a new reasoning challenge for language models, and (b) it can have potential applications in tackling the spread of misinformation. Our dataset and code are available at https://github.com/causalNLP/logical-fallacy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Logic in Computer Science,Computer Science - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/NW2KRTX2/Jin et al. - 2022 - Logical Fallacy Detection.pdf;/Users/wyq977/Zotero/storage/2G4E3GGE/2202.html}
}

@article{jonesSimpleLemmaGreedy1992,
  title = {A {{Simple Lemma}} on {{Greedy Approximation}} in {{Hilbert Space}} and {{Convergence Rates}} for {{Projection Pursuit Regression}} and {{Neural Network Training}}},
  author = {Jones, Lee K.},
  year = {1992},
  journal = {The Annals of Statistics},
  volume = {20},
  number = {1},
  eprint = {2242184},
  eprinttype = {jstor},
  pages = {608--613},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  urldate = {2023-03-02},
  abstract = {A general convergence criterion for certain iterative sequences in Hilbert space is presented. For an important subclass of these sequences, estimates of the rate of convergence are given. Under very mild assumptions these results establish an \$O(1/ \textbackslash sqrt n)\$ nonsampling convergence rate for projection pursuit regression and neural network training; where n represents the number of ridge functions, neurons or coefficients in a greedy basis expansion.},
  file = {/Users/wyq977/Zotero/storage/HTTEQGVW/Jones - 1992 - A Simple Lemma on Greedy Approximation in Hilbert .pdf}
}

@article{juppApproximationDataSplines1978,
  title = {Approximation to {{Data}} by {{Splines}} with {{Free Knots}}},
  author = {Jupp, David L. B.},
  year = {1978},
  month = apr,
  journal = {SIAM Journal on Numerical Analysis},
  volume = {15},
  number = {2},
  pages = {328--343},
  issn = {0036-1429, 1095-7170},
  doi = {10.1137/0715022},
  urldate = {2023-02-27},
  abstract = {Approximations to data by splines improve greatly if the knots are free variables. Using the B-spline representation for splines, and separating the linear and nonlinear aspects, the approximation problem reduces to nonlinear least squares in the variable knots.},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/6W6NJ57D/Jupp - 1978 - Approximation to Data by Splines with Free Knots.pdf}
}

@article{klusowskiApproximationCombinationsReLU2018,
  title = {Approximation by {{Combinations}} of {{ReLU}} and {{Squared ReLU Ridge Functions}} with \$ \textbackslash ell\^1 \$ and \$ \textbackslash ell\^0 \$ {{Controls}}},
  author = {Klusowski, Jason M. and Barron, Andrew R.},
  year = {2018},
  month = may,
  journal = {arXiv:1607.07819 [math, stat]},
  eprint = {1607.07819},
  primaryclass = {math, stat},
  urldate = {2021-04-22},
  abstract = {We establish \$ L\^\{\textbackslash infty\} \$ and \$ L\^2 \$ error bounds for functions of many variables that are approximated by linear combinations of ReLU (rectified linear unit) and squared ReLU ridge functions with \$ \textbackslash ell\^1 \$ and \$ \textbackslash ell\^0 \$ controls on their inner and outer parameters. With the squared ReLU ridge function, we show that the \$ L\^2 \$ approximation error is inversely proportional to the inner layer \$ \textbackslash ell\^0 \$ sparsity and it need only be sublinear in the outer layer \$ \textbackslash ell\^0 \$ sparsity. Our constructions are obtained using a variant of the Jones-Barron probabilistic method, which can be interpreted as either stratified sampling with proportionate allocation or two-stage cluster sampling. We also provide companion error lower bounds that reveal near optimality of our constructions. Despite the sparsity assumptions, we showcase the richness and flexibility of these ridge combinations by defining a large family of functions, in terms of certain spectral conditions, that are particularly well approximated by them.},
  archiveprefix = {arxiv},
  keywords = {62M45; 41A15,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/IJ275KKA/Klusowski and Barron - 2018 - Approximation by Combinations of ReLU and Squared .pdf;/Users/wyq977/Zotero/storage/SC7P9V4P/1607.html}
}

@misc{klusowskiApproximationCombinationsReLU2018a,
  title = {Approximation by {{Combinations}} of {{ReLU}} and {{Squared ReLU Ridge Functions}} with \$ \textbackslash ell\^1 \$ and \$ \textbackslash ell\^0 \$ {{Controls}}},
  author = {Barron, Andrew R.},
  year = {2018},
  month = may,
  number = {arXiv:1607.07819},
  eprint = {arXiv:1607.07819},
  publisher = {{arXiv}},
  urldate = {2023-02-28},
  abstract = {We establish sup-norm error bounds for functions that are approximated by linear combinations of first and second order ridge splines and show that these bounds are near-optimal.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {62M45; 41A15,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/64IYUULF/Klusowski and Barron - 2018 - Approximation by Combinations of ReLU and Squared .pdf}
}

@misc{klusowskiMinimaxLowerBounds2017,
  title = {Minimax {{Lower Bounds}} for {{Ridge Combinations Including Neural Nets}}},
  author = {Klusowski, Jason M. and Barron, Andrew R.},
  year = {2017},
  month = feb,
  number = {arXiv:1702.02828},
  eprint = {arXiv:1702.02828},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1702.02828},
  urldate = {2022-11-22},
  abstract = {Estimation of functions of \$ d \$ variables is considered using ridge combinations of the form \$ \textbackslash textstyle\textbackslash sum\_\{k=1\}\^m c\_\{1,k\} \textbackslash phi(\textbackslash textstyle\textbackslash sum\_\{j=1\}\^d c\_\{0,j,k\}x\_j-b\_k) \$ where the activation function \$ \textbackslash phi \$ is a function with bounded value and derivative. These include single-hidden layer neural networks, polynomials, and sinusoidal models. From a sample of size \$ n \$ of possibly noisy values at random sites \$ X \textbackslash in B = [-1,1]\^d \$, the minimax mean square error is examined for functions in the closure of the \$ \textbackslash ell\_1 \$ hull of ridge functions with activation \$ \textbackslash phi \$. It is shown to be of order \$ d/n \$ to a fractional power (when \$ d \$ is of smaller order than \$ n \$), and to be of order \$ (\textbackslash log d)/n \$ to a fractional power (when \$ d \$ is of larger order than \$ n \$). Dependence on constraints \$ v\_0 \$ and \$ v\_1 \$ on the \$ \textbackslash ell\_1 \$ norms of inner parameter \$ c\_0 \$ and outer parameter \$ c\_1 \$, respectively, is also examined. Also, lower and upper bounds on the fractional power are given. The heart of the analysis is development of information-theoretic packing numbers for these classes of functions.},
  archiveprefix = {arxiv},
  keywords = {62J02; 62G08; 68T05,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/94MED2BL/Klusowski and Barron - 2017 - Minimax Lower Bounds for Ridge Combinations Includ.pdf;/Users/wyq977/Zotero/storage/8HILZQCG/1702.html}
}

@article{klusowskiRiskBoundsHighdimensional2018a,
  title = {Risk {{Bounds}} for {{High-dimensional Ridge Function Combinations Including Neural Networks}}},
  author = {Klusowski, Jason M. and Barron, Andrew R.},
  year = {2018},
  month = oct,
  journal = {arXiv:1607.01434 [math, stat]},
  eprint = {1607.01434},
  primaryclass = {math, stat},
  urldate = {2021-04-19},
  abstract = {Let \$ f\^\{\textbackslash star\} \$ be a function on \$ \textbackslash mathbb\{R\}\^d \$ with an assumption of a spectral norm \$ v\_\{f\^\{\textbackslash star\}\} \$. For various noise settings, we show that \$ \textbackslash mathbb\{E\}\textbackslash |\textbackslash hat\{f\} - f\^\{\textbackslash star\} \textbackslash |\^2 \textbackslash leq \textbackslash left(v\^4\_\{f\^\{\textbackslash star\}\}\textbackslash frac\{\textbackslash log d\}\{n\}\textbackslash right)\^\{1/3\} \$, where \$ n \$ is the sample size and \$ \textbackslash hat\{f\} \$ is either a penalized least squares estimator or a greedily obtained version of such using linear combinations of sinusoidal, sigmoidal, ramp, ramp-squared or other smooth ridge functions. The candidate fits may be chosen from a continuum of functions, thus avoiding the rigidity of discretizations of the parameter space. On the other hand, if the candidate fits are chosen from a discretization, we show that \$ \textbackslash mathbb\{E\}\textbackslash |\textbackslash hat\{f\} - f\^\{\textbackslash star\} \textbackslash |\^2 \textbackslash leq \textbackslash left(v\^3\_\{f\^\{\textbackslash star\}\}\textbackslash frac\{\textbackslash log d\}\{n\}\textbackslash right)\^\{2/5\} \$. This work bridges non-linear and non-parametric function estimation and includes single-hidden layer nets. Unlike past theory for such settings, our bound shows that the risk is small even when the input dimension \$ d \$ of an infinite-dimensional parameterized dictionary is much larger than the available sample size. When the dimension is larger than the cube root of the sample size, this quantity is seen to improve the more familiar risk bound of \$ v\_\{f\^\{\textbackslash star\}\}\textbackslash left(\textbackslash frac\{d\textbackslash log (n/d)\}\{n\}\textbackslash right)\^\{1/2\} \$, also investigated here.},
  archiveprefix = {arxiv},
  keywords = {62J02; 62G08; 68T05,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/IBSSFDVL/Klusowski and Barron - 2018 - Risk Bounds for High-dimensional Ridge Function Co.pdf;/Users/wyq977/Zotero/storage/SGBT65GT/1607.html}
}

@misc{kuboImplicitRegularizationOverparameterized2019,
  title = {Implicit {{Regularization}} in {{Over-parameterized Neural Networks}}},
  author = {Kubo, Masayoshi and Banno, Ryotaro and Manabe, Hidetaka and Minoji, Masataka},
  year = {2019},
  month = mar,
  number = {arXiv:1903.01997},
  eprint = {arXiv:1903.01997},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1903.01997},
  urldate = {2023-02-20},
  abstract = {Over-parameterized neural networks generalize well in practice without any explicit regularization. Although it has not been proven yet, empirical evidence suggests that implicit regularization plays a crucial role in deep learning and prevents the network from overfitting. In this work, we introduce the gradient gap deviation and the gradient deflection as statistical measures corresponding to the network curvature and the Hessian matrix to analyze variations of network derivatives with respect to input parameters, and investigate how implicit regularization works in ReLU neural networks from both theoretical and empirical perspectives. Our result reveals that the network output between each pair of input samples is properly controlled by random initialization and stochastic gradient descent to keep interpolating between samples almost straight, which results in low complexity of over-parameterized neural networks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/339K9QCI/Kubo et al. - 2019 - Implicit Regularization in Over-parameterized Neur.pdf;/Users/wyq977/Zotero/storage/ALWNEZSK/1903.html}
}

@article{kundurMagnitudePhase,
  title = {Magnitude and {{Phase}}},
  author = {Kundur, Professor Deepa},
  pages = {5},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/IXES29C2/Kundur - Magnitude and Phase.pdf}
}

@article{kurkovaBoundsRatesVariablebasis2001,
  title = {Bounds on Rates of Variable-Basis and Neural-Network Approximation},
  author = {Kurkova, V. and Sanguineti, M.},
  year = {Sept./2001},
  journal = {IEEE Transactions on Information Theory},
  volume = {47},
  number = {6},
  pages = {2659--2665},
  issn = {00189448},
  doi = {10.1109/18.945285},
  urldate = {2023-03-02},
  abstract = {Tightness of bounds on rates of approximation by feedforward neural networks is investigated in a more general context of nonlinear approximation by variable-basis functions. Tight bounds on the worst case error in approximation by linear combinations of elements of an orthonormal variable basis are derived.},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/X3735Z52/Kurkova and Sanguineti - 2001 - Bounds on rates of variable-basis and neural-netwo.pdf}
}

@article{kurkovaComparisonWorstCase2002,
  title = {Comparison of Worst Case Errors in Linear and Neural Network Approximation},
  author = {Kurkova, V. and Sanguineti, M.},
  year = {2002},
  month = jan,
  journal = {IEEE Transactions on Information Theory},
  volume = {48},
  number = {1},
  pages = {264--275},
  issn = {1557-9654},
  doi = {10.1109/18.971754},
  abstract = {Sets of multivariable functions are described for which worst case errors in linear approximation are larger than those in approximation by neural networks. A theoretical framework for such a description is developed in the context of nonlinear approximation by fixed versus variable basis functions. Comparisons of approximation rates are formulated in terms of certain norms tailored to sets of basis functions. The results are applied to perceptron networks.},
  keywords = {Approximation methods},
  file = {/Users/wyq977/Zotero/storage/JLCZA5AM/Kurkova and Sanguineti - 2002 - Comparison of worst case errors in linear and neur.pdf;/Users/wyq977/Zotero/storage/TY5VFYU6/971754.html}
}

@misc{lanthalerErrorEstimatesDeepOnets2022,
  title = {Error Estimates for {{DeepOnets}}: {{A}} Deep Learning Framework in Infinite Dimensions},
  shorttitle = {Error Estimates for {{DeepOnets}}},
  author = {Lanthaler, Samuel and Mishra, Siddhartha and Karniadakis, George Em},
  year = {2022},
  month = jan,
  number = {arXiv:2102.09618},
  eprint = {arXiv:2102.09618},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.09618},
  urldate = {2023-02-17},
  abstract = {DeepONets have recently been proposed as a framework for learning nonlinear operators mapping between infinite dimensional Banach spaces. We analyze DeepONets and prove estimates on the resulting approximation and generalization errors. In particular, we extend the universal approximation property of DeepONets to include measurable mappings in non-compact spaces. By a decomposition of the error into encoding, approximation and reconstruction errors, we prove both lower and upper bounds on the total error, relating it to the spectral decay properties of the covariance operators, associated with the underlying measures. We derive almost optimal error bounds with very general affine reconstructors and with random sensor locations as well as bounds on the generalization error, using covering number arguments. We illustrate our general framework with four prototypical examples of nonlinear operators, namely those arising in a nonlinear forced ODE, an elliptic PDE with variable coefficients and nonlinear parabolic and hyperbolic PDEs. While the approximation of arbitrary Lipschitz operators by DeepONets to accuracy \$\textbackslash epsilon\$ is argued to suffer from a "curse of dimensionality" (requiring a neural networks of exponential size in \$1/\textbackslash epsilon\$), in contrast, for all the above concrete examples of interest, we rigorously prove that DeepONets can break this curse of dimensionality (achieving accuracy \$\textbackslash epsilon\$ with neural networks of size that can grow algebraically in \$1/\textbackslash epsilon\$). Thus, we demonstrate the efficient approximation of a potentially large class of operators with this machine learning framework.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Numerical Analysis},
  file = {/Users/wyq977/Zotero/storage/DZ8YVDNJ/Lanthaler et al. - 2022 - Error estimates for DeepOnets A deep learning fra.pdf;/Users/wyq977/Zotero/storage/TBPFFKEM/2102.html}
}

@misc{leeAbilityNeuralNets2021,
  title = {On the Ability of Neural Nets to Express Distributions},
  author = {Lee, Holden and Ge, Rong and Ma, Tengyu and Risteski, Andrej and Arora, Sanjeev},
  year = {2021},
  month = apr,
  number = {arXiv:1702.07028},
  eprint = {arXiv:1702.07028},
  publisher = {{arXiv}},
  urldate = {2022-10-11},
  abstract = {Deep neural nets have caused a revolution in many classification tasks. A related ongoing revolution -- also theoretically not understood -- concerns their ability to serve as generative models for complicated types of data such as images and texts. These models are trained using ideas like variational autoencoders and Generative Adversarial Networks. We take a first cut at explaining the expressivity of multilayer nets by giving a sufficient criterion for a function to be approximable by a neural network with \$n\$ hidden layers. A key ingredient is Barron's Theorem \textbackslash cite\{Barron1993\}, which gives a Fourier criterion for approximability of a function by a neural network with 1 hidden layer. We show that a composition of \$n\$ functions which satisfy certain Fourier conditions ("Barron functions") can be approximated by a \$n+1\$-layer neural network. For probability distributions, this translates into a criterion for a probability distribution to be approximable in Wasserstein distance -- a natural metric on probability distributions -- by a neural network applied to a fixed base distribution (e.g., multivariate gaussian). Building up recent lower bound work, we also give an example function that shows that composition of Barron functions is more expressive than Barron functions alone.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/KECHNJ2C/Lee et al. - 2021 - On the ability of neural nets to express distribut.pdf;/Users/wyq977/Zotero/storage/5Y5HSIEL/1702.html}
}

@article{leshnoMultilayerFeedforwardNetworks1993,
  title = {Multilayer Feedforward Networks with a Nonpolynomial Activation Function Can Approximate Any Function},
  author = {Leshno, Moshe and Lin, Vladimir Ya. and Pinkus, Allan and Schocken, Shimon},
  year = {1993},
  month = jan,
  journal = {Neural Networks},
  volume = {6},
  number = {6},
  pages = {861--867},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(05)80131-5},
  urldate = {2023-03-20},
  abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold.},
  langid = {english},
  keywords = {(μ) approximation,Activation functions,Multilayer feedforward networks,Role of threshold,Universal approximation capabilities},
  file = {/Users/wyq977/Zotero/storage/3XAMCYF8/Leshno et al. - 1993 - Multilayer feedforward networks with a nonpolynomi.pdf;/Users/wyq977/Zotero/storage/58YZVKFB/S0893608005801315.html}
}

@article{liangJustInterpolateKernel2019,
  title = {Just {{Interpolate}}: {{Kernel}} "{{Ridgeless}}" {{Regression Can Generalize}}},
  shorttitle = {Just {{Interpolate}}},
  author = {Liang, Tengyuan and Rakhlin, Alexander},
  year = {2019},
  month = feb,
  journal = {arXiv:1808.00387 [cs, math, stat]},
  eprint = {1808.00387},
  primaryclass = {cs, math, stat},
  doi = {10.1214/19-AOS1849},
  urldate = {2021-04-19},
  abstract = {In the absence of explicit regularization, Kernel ``Ridgeless'' Regression with nonlinear kernels has the potential to fit the training data perfectly. It has been observed empirically, however, that such interpolated solutions can still generalize well on test data. We isolate a phenomenon of implicit regularization for minimum-norm interpolated solutions which is due to a combination of high dimensionality of the input data, curvature of the kernel function, and favorable geometric properties of the data such as an eigenvalue decay of the empirical covariance and kernel matrices. In addition to deriving a data-dependent upper bound on the out-of-sample error, we present experimental evidence suggesting that the phenomenon occurs in the MNIST dataset.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/YMXK5Y2T/Liang and Rakhlin - 2019 - Just Interpolate Kernel Ridgeless Regression Ca.pdf}
}

@misc{liComplexityMeasuresNeural2020,
  title = {Complexity {{Measures}} for {{Neural Networks}} with {{General Activation Functions Using Path-based Norms}}},
  author = {Li, Zhong and Ma, Chao and Wu, Lei},
  year = {2020},
  month = sep,
  number = {arXiv:2009.06132},
  eprint = {arXiv:2009.06132},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.06132},
  urldate = {2022-12-01},
  abstract = {A simple approach is proposed to obtain complexity controls for neural networks with general activation functions. The approach is motivated by approximating the general activation functions with one-dimensional ReLU networks, which reduces the problem to the complexity controls of ReLU networks. Specifically, we consider two-layer networks and deep residual networks, for which path-based norms are derived to control complexities. We also provide preliminary analyses of the function spaces induced by these norms and a priori estimates of the corresponding regularized estimators.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/WHHAPTWP/Li et al. - 2020 - Complexity Measures for Neural Networks with Gener.pdf;/Users/wyq977/Zotero/storage/LLCGZPVP/2009.html}
}

@misc{liLearningOverparameterizedNeural2019,
  title = {Learning {{Overparameterized Neural Networks}} via {{Stochastic Gradient Descent}} on {{Structured Data}}},
  author = {Li, Yuanzhi and Liang, Yingyu},
  year = {2019},
  month = aug,
  number = {arXiv:1808.01204},
  eprint = {arXiv:1808.01204},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1808.01204},
  urldate = {2023-02-20},
  abstract = {Neural networks have many successful applications, while much less theoretical understanding has been gained. Towards bridging this gap, we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting, when the data comes from mixtures of well-separated distributions, we prove that SGD learns a network with a small generalization error, albeit the network has enough capacity to fit arbitrary labels. Furthermore, the analysis provides interesting insights into several aspects of learning neural networks and can be verified based on empirical studies on synthetic data and on the MNIST dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/NNNMD9JI/Li and Liang - 2019 - Learning Overparameterized Neural Networks via Sto.pdf;/Users/wyq977/Zotero/storage/2ZFAF3MS/1808.html}
}

@misc{liuDeepNonparametricEstimation2022,
  title = {Deep {{Nonparametric Estimation}} of {{Operators}} between {{Infinite Dimensional Spaces}}},
  author = {Liu, Hao and Yang, Haizhao and Chen, Minshuo and Zhao, Tuo and Liao, Wenjing},
  year = {2022},
  month = jan,
  number = {arXiv:2201.00217},
  eprint = {arXiv:2201.00217},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.00217},
  urldate = {2023-02-17},
  abstract = {Learning operators between infinitely dimensional spaces is an important learning task arising in wide applications in machine learning, imaging science, mathematical modeling and simulations, etc. This paper studies the nonparametric estimation of Lipschitz operators using deep neural networks. Non-asymptotic upper bounds are derived for the generalization error of the empirical risk minimizer over a properly chosen network class. Under the assumption that the target operator exhibits a low dimensional structure, our error bounds decay as the training sample size increases, with an attractive fast rate depending on the intrinsic dimension in our estimation. Our assumptions cover most scenarios in real applications and our results give rise to fast rates by exploiting low dimensional structures of data in operator estimation. We also investigate the influence of network structures (e.g., network width, depth, and sparsity) on the generalization error of the neural network estimator and propose a general suggestion on the choice of network structures to maximize the learning efficiency quantitatively.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/4UXME46X/Liu et al. - 2022 - Deep Nonparametric Estimation of Operators between.pdf;/Users/wyq977/Zotero/storage/IACSJ2W4/2201.html}
}

@article{liUnprecedentedGenomicDiversity2015,
  title = {Unprecedented Genomic Diversity of {{RNA}} Viruses in Arthropods Reveals the Ancestry of Negative-Sense {{RNA}} Viruses},
  author = {Li, Ci-Xiu and Shi, Mang and Tian, Jun-Hua and Lin, Xian-Dan and Kang, Yan-Jun and Chen, Liang-Jun and Qin, Xin-Cheng and Xu, Jianguo and Holmes, Edward C. and Zhang, Yong-Zhen},
  year = {2015},
  month = jan,
  journal = {eLife},
  volume = {4},
  issn = {2050-084X},
  doi = {10.7554/eLife.05378},
  abstract = {Although arthropods are important viral vectors, the biodiversity of arthropod viruses, as well as the role that arthropods have played in viral origins and evolution, is unclear. Through RNA sequencing of 70 arthropod species we discovered 112 novel viruses that appear to be ancestral to much of the documented genetic diversity of negative-sense RNA viruses, a number of which are also present as endogenous genomic copies. With this greatly enriched diversity we revealed that arthropods contain viruses that fall basal to major virus groups, including the vertebrate-specific arenaviruses, filoviruses, hantaviruses, influenza viruses, lyssaviruses, and paramyxoviruses. We similarly documented a remarkable diversity of genome structures in arthropod viruses, including a putative circular form, that sheds new light on the evolution of genome organization. Hence, arthropods are a major reservoir of viral genetic diversity and have likely been central to viral evolution.},
  langid = {english},
  pmcid = {PMC4384744},
  pmid = {25633976},
  keywords = {Animals,arthropods,Arthropods,Biodiversity,evolution,Evolution; Molecular,Genome,infectious disease,microbiology,negative-sense,phylogeny,Phylogeny,RNA virus,RNA Viruses,segmentation,viruses},
  file = {/Users/wyq977/Zotero/storage/B34W32VX/Li et al. - 2015 - Unprecedented genomic diversity of RNA viruses in .pdf}
}

@misc{maennelGradientDescentQuantizes2018,
  title = {Gradient {{Descent Quantizes ReLU Network Features}}},
  author = {Maennel, Hartmut and Bousquet, Olivier and Gelly, Sylvain},
  year = {2018},
  month = mar,
  number = {arXiv:1803.08367},
  eprint = {arXiv:1803.08367},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.08367},
  urldate = {2023-02-20},
  abstract = {Deep neural networks are often trained in the over-parametrized regime (i.e. with far more parameters than training examples), and understanding why the training converges to solutions that generalize remains an open problem. Several studies have highlighted the fact that the training procedure, i.e. mini-batch Stochastic Gradient Descent (SGD) leads to solutions that have specific properties in the loss landscape. However, even with plain Gradient Descent (GD) the solutions found in the over-parametrized regime are pretty good and this phenomenon is poorly understood. We propose an analysis of this behavior for feedforward networks with a ReLU activation function under the assumption of small initialization and learning rate and uncover a quantization effect: The weight vectors tend to concentrate at a small number of directions determined by the input data. As a consequence, we show that for given input data there are only finitely many, "simple" functions that can be obtained, independent of the network size. This puts these functions in analogy to linear interpolations (for given input data there are finitely many triangulations, which each determine a function by linear interpolation). We ask whether this analogy extends to the generalization properties - while the usual distribution-independent generalization property does not hold, it could be that for e.g. smooth functions with bounded second derivative an approximation property holds which could "explain" generalization of networks (of unbounded size) to unseen inputs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/MQ2G82MD/Maennel et al. - 2018 - Gradient Descent Quantizes ReLU Network Features.pdf;/Users/wyq977/Zotero/storage/9KFXW32A/1803.html}
}

@misc{maFunctionSpaceTheory2020,
  title = {A {{Function Space Theory}} and {{Generalization Error Estimates}} for {{Neural Network Models}}},
  author = {Ma, Chao},
  year = {2020},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/TB6UJM4G/Ma - A Function Space Theory and Generalization Error E.pdf}
}

@article{makovozRandomApproximantsNeural1996,
  title = {Random {{Approximants}} and {{Neural Networks}}},
  author = {Makovoz, Y.},
  year = {1996},
  month = apr,
  journal = {Journal of Approximation Theory},
  volume = {85},
  number = {1},
  pages = {98--109},
  issn = {00219045},
  doi = {10.1006/jath.1996.0031},
  urldate = {2023-02-28},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/K6V8CQN9/Makovoz - 1996 - Random Approximants and Neural Networks.pdf}
}

@article{makovozUniformApproximationNeural1998,
  title = {Uniform {{Approximation}} by {{Neural Networks}}},
  author = {Makovoz, Y.},
  year = {1998},
  month = nov,
  journal = {Journal of Approximation Theory},
  volume = {95},
  number = {2},
  pages = {215--228},
  issn = {0021-9045},
  doi = {10.1006/jath.1997.3217},
  urldate = {2023-03-06},
  abstract = {LetD{$\subset$}Rdbe a compact set and let{$\Phi$}be a uniformly bounded set ofD\textrightarrow Rfunctions. For a given real-valued functionfdefined onDand a given natural numbern, we are looking for a good uniform approximation tofof the form {$\sum$}ni=1a1{$\varphi$}i, with{$\varphi$}i{$\in\Phi$},ai{$\in$}R. Two main cases are considered: (1) whenDis a finite set and (2) when the set{$\Phi$}is formed by the functions{$\varphi$}v,b(x):=s(v{$\cdot$}x+b), wherev{$\in$}Rd,b{$\in$}R, andsis a fixedR\textrightarrow Rfunction.},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/QZM7V5ME/Makovoz - 1998 - Uniform Approximation by Neural Networks.pdf;/Users/wyq977/Zotero/storage/YM26JCN4/S0021904597932172.html}
}

@article{matousekDiscrepancyApproximationsBounded1993,
  title = {Discrepancy and Approximations for Bounded {{VC-dimension}}},
  author = {Matou{\v s}ek, Ji{\v r}{\'i} and Welzl, Emo and Wernisch, Lorenz},
  year = {1993},
  month = dec,
  journal = {Combinatorica},
  volume = {13},
  number = {4},
  pages = {455--466},
  issn = {1439-6912},
  doi = {10.1007/BF01303517},
  urldate = {2023-03-17},
  abstract = {Let (X, {$\mathscr{R}$}) be a set system on ann-point setX. For a two-coloring onX, itsdiscrepancy is defined as the maximum number by which the occurrences of the two colors differ in any set in {$\mathscr{R}$}. We show that if for anym-point subset\$\$Y \textbackslash subseteq X\$\$the number of distinct subsets induced by {$\mathscr{R}$} onY is bounded byO(md) for a fixed integerd, then there is a coloring with discrepancy bounded byO(n1/2-1/2d(logn)1+1/2d). Also if any subcollection ofm sets of {$\mathscr{R}$} partitions the points into at mostO(md) classes, then there is a coloring with discrepancy at mostO(n1/2-1/2dlogn). These bounds imply improved upper bounds on the size of {$\epsilon$}-approximations for (X, {$\mathscr{R}$}). All the bounds are tight up to polylogarithmic factors in the worst case. Our results allow to generalize several results of Beck bounding the discrepancy in certain geometric settings to the case when the discrepancy is taken relative to an arbitrary measure.},
  langid = {english},
  keywords = {05 A 05,05 C 65,52 C 10,52 C 99}
}

@article{matousekTightUpperBounds1995,
  title = {Tight Upper Bounds for the Discrepancy of Half-Spaces},
  author = {Matou{\v s}ek, J.},
  year = {1995},
  month = jun,
  journal = {Discrete \& Computational Geometry},
  volume = {13},
  number = {3},
  pages = {593--601},
  issn = {1432-0444},
  doi = {10.1007/BF02574066},
  urldate = {2023-03-17},
  abstract = {We show that the discrepancy of anyn-point setP in the Euclideand-space with respect to half-spaces is bounded byCdn1/2-1/2d, that is, a mapping {$\chi$}:P\textrightarrow\{-1,1\} exists such that, for any half-space {$\gamma$}, {$\gamma$}, |{$\Sigma$}p{$\in$}P{$\bigcap\gamma\chi$}(p)|{$\leq$}Cdn1/2-1/2d. In fact, the result holds for arbitrary set systems as long as theprimal shatter function isO(md). This matches known lower bounds, improving previous upper bounds by a\% MathType!MTEF!2!1!+-\% feaafiart1ev1aaatCvAUfKttLearuqr1ngBPrgarmWu51MyVXgatC\% vAUfeBSjuyZL2yd9gzLbvyNv2CaeHbd9wDYLwzYbItLDharyavP1wz\% ZbItLDhis9wBH5garqqtubsr4rNCHbGeaGqiVu0Je9sqqrpepC0xbb\% L8F4rqqrFfpeea0xe9Lq-Jc9vqaqpepm0xbba9pwe9Q8fs0-yqaqpe\% pae9pg0FirpepeKkFr0xfr-xfr-xb9adbaqaaeGaciGaaiaabeqaam\% aaeaqbaaGcbaWaaOaaaeaacyGGSbaBcqGGVbWBcqGGNbWziqGacaWF\% Ubaaleqaaaaa!4001!\$\$\textbackslash sqrt \{\textbackslash log n\} \$\$factor.},
  langid = {english},
  keywords = {34th IEEE Symposium,Computational Geometry,Discrete Comput Geom,Partial Coloring,Random Coloring},
  file = {/Users/wyq977/Zotero/storage/RNCMJWQM/Matoušek - 1995 - Tight upper bounds for the discrepancy of half-spa.pdf}
}

@article{maUniformApproximationRates2022,
  title = {Uniform Approximation Rates and Metric Entropy of Shallow Neural Networks},
  author = {Ma, Limin and Siegel, Jonathan W. and Xu, Jinchao},
  year = {2022},
  month = sep,
  journal = {Research in the Mathematical Sciences},
  volume = {9},
  number = {3},
  pages = {46},
  issn = {2522-0144, 2197-9847},
  doi = {10.1007/s40687-022-00346-y},
  urldate = {2023-03-06},
  abstract = {We study the approximation properties of the variation spaces corresponding to shallow neural networks with respect to the uniform norm. Specifically, we consider the spectral Barron space, which consists of the convex hull of decaying Fourier modes, and the convex hull of indicator functions of half-spaces, which corresponds to shallow neural networks with sigmoidal activation function. Up to logarithmic factors, we determine the metric entropy and nonlinear dictionary approximation rates for these spaces with respect to the uniform norm. Combined with previous results with respect to the L2-norm, this also gives the metric entropy up to logarithmic factors with respect to any Lp-norm with 1 {$\leq$} p {$\leq$} {$\infty$}. In addition, we study the approximation rates for high-order spectral Barron spaces using shallow neural networks with ReLUk activation function. Specifically, we show that for a sufficiently high-order spectral Barron space, ReLUk networks are able to achieve an approximation rate of n-(k+1) with respect to the uniform norm.},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/JI8GL5BN/Ma et al. - 2022 - Uniform approximation rates and metric entropy of .pdf}
}

@phdthesis{meiComputationalStatisticalTheories2020,
  title = {Computational and Statistical Theories for Large-Scale Neural Networks},
  author = {Mei, Song and Montanari, Andrea and Johnstone, Iain and Ying, Lexing},
  year = {2020},
  address = {{Stanford, California}},
  abstract = {Deep learning methods operate in regimes that defy the traditional computational and statistical mindsets. Despite the non-convexity of empirical risks and the huge complexity of neural network architectures, stochastic gradient algorithms can often find an approximate global minimizer of the training loss and achieve small generalization error on test data. In recent years, an important research direction is to theoretically explain these observed optimization efficiency and generalization efficacy of neural network systems. This thesis tries to tackle these challenges in the model of two-layers neural networks, by analyzing its computational and statistical properties in various scaling limits. On the computational aspects, we introduce two competing theories for neural network dynamics: the mean field theory and the tangent kernel theory. These two theories characterize training dynamics of neural networks in different regimes that exhibit different behaviors. In the mean field framework, the training dynamics, in the large neuron limit, is captured by a particular non-linear partial differential equation. This characterization allows us to prove global convergence of the dynamics in certain scenarios. Comparatively, the tangent kernel theory characterizes the same dynamics in a different scaling limit and provides global convergence guarantees in more general scenarios. On the statistical aspects, we study the generalization properties of neural networks trained in the two regimes as described above. We first show that, in the high dimensional limit, neural tangent kernels are no better than polynomial regression, while neural networks trained in the mean field regime can potentially perform better. Next, we study more carefully the random features model, which is equivalent to a two-layers neural network in the kernel regime. We compute the precise asymptotics of its test error in the high dimensional limit and confirm that it exhibits an interesting double-descent curve that was observed in experiments},
  collaborator = {Stanford University},
  school = {Stanford University},
  file = {/Users/wyq977/Zotero/storage/E2MABAX4/Mei et al. - 2020 - Computational and statistical theories for large-s.pdf}
}

@article{montanariOneLectureTwolayer2018,
  title = {One Lecture on Two-Layer Neural Networks},
  author = {Montanari, Andrea},
  year = {2018},
  month = aug,
  pages = {10},
  abstract = {Notes for a lecture at the Carg`ese Summer School `Statistical Physics and Machine Learning Back Together,' August 21, 2018.},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/B84VZ8A5/Montanari - One lecture on two-layer neural networks.pdf}
}

@misc{mousavi-hosseiniNeuralNetworksEfficiently2022,
  title = {Neural {{Networks Efficiently Learn Low-Dimensional Representations}} with {{SGD}}},
  author = {{Mousavi-Hosseini}, Alireza and Park, Sejun and Girotti, Manuela and Mitliagkas, Ioannis and Erdogdu, Murat A.},
  year = {2022},
  month = sep,
  number = {arXiv:2209.14863},
  eprint = {arXiv:2209.14863},
  publisher = {{arXiv}},
  urldate = {2023-02-08},
  abstract = {We study the problem of training a two-layer neural network (NN) of arbitrary width using stochastic gradient descent (SGD) where the input \$\textbackslash boldsymbol\{x\}\textbackslash in \textbackslash mathbb\{R\}\^d\$ is Gaussian and the target \$y \textbackslash in \textbackslash mathbb\{R\}\$ follows a multiple-index model, i.e., \$y=g(\textbackslash langle\textbackslash boldsymbol\{u\_1\},\textbackslash boldsymbol\{x\}\textbackslash rangle,...,\textbackslash langle\textbackslash boldsymbol\{u\_k\},\textbackslash boldsymbol\{x\}\textbackslash rangle)\$ with a noisy link function \$g\$. We prove that the first-layer weights of the NN converge to the \$k\$-dimensional principal subspace spanned by the vectors \$\textbackslash boldsymbol\{u\_1\},...,\textbackslash boldsymbol\{u\_k\}\$ of the true model, when online SGD with weight decay is used for training. This phenomenon has several important consequences when \$k \textbackslash ll d\$. First, by employing uniform convergence on this smaller subspace, we establish a generalization error bound of \$\textbackslash mathcal\{O\}(\textbackslash sqrt\{\{kd\}/\{T\}\})\$ after \$T\$ iterations of SGD, which is independent of the width of the NN. We further demonstrate that, SGD-trained ReLU NNs can learn a single-index target of the form \$y=f(\textbackslash langle\textbackslash boldsymbol\{u\},\textbackslash boldsymbol\{x\}\textbackslash rangle) + \textbackslash epsilon\$ by recovering the principal direction, with a sample complexity linear in \$d\$ (up to log factors), where \$f\$ is a monotonic function with at most polynomial growth, and \$\textbackslash epsilon\$ is the noise. This is in contrast to the known \$d\^\{\textbackslash Omega(p)\}\$ sample requirement to learn any degree \$p\$ polynomial in the kernel regime, and it shows that NNs trained with SGD can outperform the neural tangent kernel at initialization. Finally, we also provide compressibility guarantees for NNs using the approximate low-rank structure produced by SGD.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/RIR53HDJ/Mousavi-Hosseini et al. - 2022 - Neural Networks Efficiently Learn Low-Dimensional .pdf;/Users/wyq977/Zotero/storage/VPHLRTY4/2209.html}
}

@misc{neyshaburImplicitRegularizationDeep2017,
  title = {Implicit {{Regularization}} in {{Deep Learning}}},
  author = {Neyshabur, Behnam},
  year = {2017},
  month = sep,
  number = {arXiv:1709.01953},
  eprint = {arXiv:1709.01953},
  publisher = {{arXiv}},
  urldate = {2022-11-04},
  abstract = {In an attempt to better understand generalization in deep learning, we study several possible explanations. We show that implicit regularization induced by the optimization method is playing a key role in generalization and success of deep learning models. Motivated by this view, we study how different complexity measures can ensure generalization and explain how optimization algorithms can implicitly regularize complexity measures. We empirically investigate the ability of these measures to explain different observed phenomena in deep learning. We further study the invariances in neural networks, suggest complexity measures and optimization algorithms that have similar invariances to those in neural networks and evaluate them on a number of learning tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/9WB6TY4D/Neyshabur - 2017 - Implicit Regularization in Deep Learning.pdf;/Users/wyq977/Zotero/storage/7286XC2I/1709.html}
}

@article{neyshaburNormBasedCapacityControl2015,
  title = {Norm-{{Based Capacity Control}} in {{Neural Networks}}},
  author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  year = {2015},
  month = apr,
  journal = {arXiv:1503.00036 [cs, stat]},
  eprint = {1503.00036},
  primaryclass = {cs, stat},
  urldate = {2021-04-22},
  abstract = {We investigate the capacity, convexity and characterization of a general family of norm-constrained feed-forward networks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/H47XLQHN/Neyshabur et al. - 2015 - Norm-Based Capacity Control in Neural Networks.pdf;/Users/wyq977/Zotero/storage/ATQ32VYT/1503.html}
}

@misc{neyshaburSearchRealInductive2015,
  title = {In {{Search}} of the {{Real Inductive Bias}}: {{On}} the {{Role}} of {{Implicit Regularization}} in {{Deep Learning}}},
  shorttitle = {In {{Search}} of the {{Real Inductive Bias}}},
  author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  year = {2015},
  month = apr,
  number = {arXiv:1412.6614},
  eprint = {arXiv:1412.6614},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.6614},
  urldate = {2023-02-20},
  abstract = {We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multilayer feed-forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/VE2T4SUW/Neyshabur et al. - 2015 - In Search of the Real Inductive Bias On the Role .pdf;/Users/wyq977/Zotero/storage/KX8VQBBK/1412.html}
}

@article{oymakNearOptimalBoundsBinary2015,
  title = {Near-{{Optimal Bounds}} for {{Binary Embeddings}} of {{Arbitrary Sets}}},
  author = {Oymak, Samet and Recht, Ben},
  year = {2015},
  month = dec,
  journal = {arXiv:1512.04433 [cs, math]},
  eprint = {1512.04433},
  primaryclass = {cs, math},
  urldate = {2021-04-19},
  abstract = {We study embedding a subset K of the unit sphere to the Hamming cube \{-1, +1\}m. We characterize the tradeoff between distortion and sample complexity m in terms of the Gaussian width {$\omega$}(K) of the set. For subspaces and several structured-sparse sets we show that Gaussian maps provide the optimal tradeoff m {$\sim$} {$\delta-$}2{$\omega$}2(K), in particular for {$\delta$} distortion one needs m {$\approx$} {$\delta-$}2d where d is the subspace dimension. For general sets, we provide sharp characterizations which reduces to m {$\approx$} {$\delta-$}4{$\omega$}2(K) after simplification. We provide improved results for local embedding of points that are in close proximity of each other which is related to locality sensitive hashing. We also discuss faster binary embedding where one takes advantage of an initial sketching procedure based on Fast Johnson-Lindenstauss Transform. Finally, we list several numerical observations and discuss open problems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Mathematics - Functional Analysis},
  file = {/Users/wyq977/Zotero/storage/GTN638CU/Oymak and Recht - 2015 - Near-Optimal Bounds for Binary Embeddings of Arbit.pdf}
}

@article{paluzo-hidalgoTwohiddenlayerFeedforwardNetworks2020,
  title = {Two-Hidden-Layer Feed-Forward Networks Are Universal Approximators: {{A}} Constructive Approach},
  shorttitle = {Two-Hidden-Layer Feed-Forward Networks Are Universal Approximators},
  author = {{Paluzo-Hidalgo}, Eduardo and {Gonzalez-Diaz}, Rocio and {Guti{\'e}rrez-Naranjo}, Miguel A.},
  year = {2020},
  month = nov,
  journal = {Neural Networks},
  volume = {131},
  pages = {29--36},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2020.07.021},
  urldate = {2023-03-03},
  abstract = {It is well-known that artificial neural networks are universal approximators. The classical existence result proves that, given a continuous function on a compact set embedded in an n-dimensional space, there exists a one-hidden-layer feed-forward network that approximates the function. In this paper, a constructive approach to this problem is given for the case of a continuous function on triangulated spaces. Once a triangulation of the space is given, a two-hidden-layer feed-forward network with a concrete set of weights is computed. The level of the approximation depends on the refinement of the triangulation.},
  langid = {english},
  keywords = {Multi-layer feed-forward network,Simplicial Approximation Theorem,Triangulations,Universal Approximation Theorem},
  file = {/Users/wyq977/Zotero/storage/EH7RGRCD/Paluzo-Hidalgo et al. - 2020 - Two-hidden-layer feed-forward networks are univers.pdf;/Users/wyq977/Zotero/storage/742MT6L6/S0893608020302628.html}
}

@misc{parhiBanachSpaceRepresenter2021,
  title = {Banach {{Space Representer Theorems}} for {{Neural Networks}} and {{Ridge Splines}}},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2021},
  month = feb,
  number = {arXiv:2006.05626},
  eprint = {arXiv:2006.05626},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.05626},
  urldate = {2022-09-20},
  abstract = {We develop a variational framework to understand the properties of the functions learned by neural networks fit to data. We propose and study a family of continuous-domain linear inverse problems with total variation-like regularization in the Radon domain subject to data fitting constraints. We derive a representer theorem showing that finite-width, single-hidden layer neural networks are solutions to these inverse problems. We draw on many techniques from variational spline theory and so we propose the notion of polynomial ridge splines, which correspond to single-hidden layer neural networks with truncated power functions as the activation function. The representer theorem is reminiscent of the classical reproducing kernel Hilbert space representer theorem, but we show that the neural network problem is posed over a non-Hilbertian Banach space. While the learning problems are posed in the continuous-domain, similar to kernel methods, the problems can be recast as finite-dimensional neural network training problems. These neural network training problems have regularizers which are related to the well-known weight decay and path-norm regularizers. Thus, our result gives insight into functional characteristics of trained neural networks and also into the design neural network regularizers. We also show that these regularizers promote neural network solutions with desirable generalization properties.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/TRTMDXUH/Parhi and Nowak - 2021 - Banach Space Representer Theorems for Neural Netwo.pdf;/Users/wyq977/Zotero/storage/7C9YN4MA/2006.html}
}

@article{parhiRoleNeuralNetwork2020,
  title = {The {{Role}} of {{Neural Network Activation Functions}}},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2020},
  journal = {IEEE Signal Processing Letters},
  volume = {27},
  eprint = {1910.02333},
  primaryclass = {cs, stat},
  pages = {1779--1783},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2020.3027517},
  urldate = {2022-10-27},
  abstract = {A wide variety of activation functions have been proposed for neural networks. The Rectified Linear Unit (ReLU) is especially popular today. There are many practical reasons that motivate the use of the ReLU. This paper provides new theoretical characterizations that support the use of the ReLU, its variants such as the leaky ReLU, as well as other activation functions in the case of univariate, single-hidden layer feedforward neural networks. Our results also explain the importance of commonly used strategies in the design and training of neural networks such as "weight decay" and "path-norm" regularization, and provide a new justification for the use of "skip connections" in network architectures. These new insights are obtained through the lens of spline theory. In particular, we show how neural network training problems are related to infinite-dimensional optimizations posed over Banach spaces of functions whose solutions are well-known to be fractional and polynomial splines, where the particular Banach space (which controls the order of the spline) depends on the choice of activation function.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/2QLXE6X7/Parhi and Nowak - 2020 - The Role of Neural Network Activation Functions.pdf;/Users/wyq977/Zotero/storage/8JCZB47V/1910.html}
}

@article{parhiWhatKindsFunctions2022,
  title = {What {{Kinds}} of {{Functions}} Do {{Deep Neural Networks Learn}}? {{Insights}} from {{Variational Spline Theory}}},
  shorttitle = {What {{Kinds}} of {{Functions}} Do {{Deep Neural Networks Learn}}?},
  author = {Parhi, Rahul and Nowak, Robert D.},
  year = {2022},
  month = jun,
  journal = {SIAM Journal on Mathematics of Data Science},
  volume = {4},
  number = {2},
  eprint = {2105.03361},
  primaryclass = {cs, stat},
  pages = {464--489},
  issn = {2577-0187},
  doi = {10.1137/21M1418642},
  urldate = {2022-09-20},
  abstract = {We develop a variational framework to understand the properties of functions learned by fitting deep neural networks with rectified linear unit activations to data. We propose a new function space, which is reminiscent of classical bounded variation-type spaces, that captures the compositional structure associated with deep neural networks. We derive a representer theorem showing that deep ReLU networks are solutions to regularized data fitting problems over functions from this space. The function space consists of compositions of functions from the Banach spaces of second-order bounded variation in the Radon domain. These are Banach spaces with sparsity-promoting norms, giving insight into the role of sparsity in deep neural networks. The neural network solutions have skip connections and rank bounded weight matrices, providing new theoretical support for these common architectural choices. The variational problem we study can be recast as a finite-dimensional neural network training problem with regularization schemes related to the notions of weight decay and path-norm regularization. Finally, our analysis builds on techniques from variational spline theory, providing new connections between deep neural networks and splines.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/TIK3K2PG/Parhi and Nowak - 2022 - What Kinds of Functions do Deep Neural Networks Le.pdf;/Users/wyq977/Zotero/storage/ZSDU6IU2/2105.html}
}

@article{pinkusApproximationTheoryMLP1999,
  title = {Approximation Theory of the {{MLP}} Model in Neural Networks},
  author = {Pinkus, Allan},
  year = {1999},
  month = jan,
  journal = {Acta Numerica},
  volume = {8},
  pages = {143--195},
  publisher = {{Cambridge University Press}},
  issn = {1474-0508, 0962-4929},
  doi = {10.1017/S0962492900002919},
  urldate = {2023-02-24},
  abstract = {In this survey we discuss various approximation-theoretic problems that arise in the multilayer feedforward perceptron (MLP) model in neural networks. The MLP model is one of the more popular and practical of the many neural network models. Mathematically it is also one of the simpler models. Nonetheless the mathematics of this model is not well understood, and many of these problems are approximation-theoretic in character. Most of the research we will discuss is of very recent vintage. We will report on what has been done and on various unanswered questions. We will not be presenting practical (algorithmic) methods. We will, however, be exploring the capabilities and limitations of this model.},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/A2KS77D7/Pinkus - 1999 - Approximation theory of the MLP model in neural ne.pdf}
}

@article{pisierRemarquesResultatNon1980,
  title = {Remarques Sur Un R\'esultat Non Publi\'e de {{B}}. {{Maurey}}},
  author = {Pisier, G.},
  year = {1980},
  journal = {S\'eminaire d'Analyse fonctionnelle (dit "Maurey-Schwartz")},
  pages = {1--12},
  issn = {2497-4005},
  urldate = {2023-03-01},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/K8DA2JWK/Pisier - 1980 - Remarques sur un résultat non publié de B. Maurey.pdf}
}

@misc{poggioWhyWhenCan2017,
  title = {Why and {{When Can Deep}} -- but {{Not Shallow}} -- {{Networks Avoid}} the {{Curse}} of {{Dimensionality}}: A {{Review}}},
  shorttitle = {Why and {{When Can Deep}} -- but {{Not Shallow}} -- {{Networks Avoid}} the {{Curse}} of {{Dimensionality}}},
  author = {Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
  year = {2017},
  month = feb,
  number = {arXiv:1611.00740},
  eprint = {arXiv:1611.00740},
  publisher = {{arXiv}},
  urldate = {2023-02-16},
  abstract = {The paper characterizes classes of functions for which deep learning can be exponentially better than shallow learning. Deep convolutional networks are a special case of these conditions, though weight sharing is not the main reason for their exponential advantage.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/HKDCVAMJ/Poggio et al. - 2017 - Why and When Can Deep -- but Not Shallow -- Networ.pdf;/Users/wyq977/Zotero/storage/25WCT3XT/1611.html}
}

@inproceedings{rahimiUniformApproximationFunctions2008,
  title = {Uniform Approximation of Functions with Random Bases},
  booktitle = {2008 46th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}}},
  author = {Rahimi, Ali and Recht, Benjamin},
  year = {2008},
  month = sep,
  pages = {555--561},
  publisher = {{IEEE}},
  address = {{Monticello, IL, USA}},
  doi = {10.1109/ALLERTON.2008.4797607},
  urldate = {2021-04-29},
  abstract = {Random networks of nonlinear functions have a long history of empirical success in function fitting but few theoretical guarantees. In this paper, using techniques from probability on Banach Spaces, we analyze a specific architecture of random nonlinearities, provide L{$\infty$} and L2 error bounds for approximating functions in Reproducing Kernel Hilbert Spaces, and discuss scenarios when these expansions are dense in the continuous functions. We discuss connections between these random nonlinear networks and popular machine learning algorithms and show experimentally that these networks provide competitive performance at far lower computational cost on large-scale pattern recognition tasks.},
  isbn = {978-1-4244-2925-7},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/WQALQ84F/Rahimi and Recht - 2008 - Uniform approximation of functions with random bas.pdf}
}

@book{rudinFunctionalAnalysis1991,
  title = {Functional Analysis},
  author = {Rudin, Walter},
  year = {1991},
  series = {International Series in Pure and Applied Mathematics},
  edition = {2nd ed},
  publisher = {{McGraw-Hill}},
  address = {{New York}},
  isbn = {978-0-07-054236-5},
  langid = {english},
  lccn = {QA320 .R83 1991},
  keywords = {Functional analysis},
  file = {/Users/wyq977/Zotero/storage/3DPXYV58/Rudin - 1991 - Functional analysis.pdf}
}

@book{rudinRealComplexAnalysis1987,
  title = {Real and Complex Analysis},
  author = {Rudin, Walter},
  year = {1987},
  edition = {3rd ed},
  publisher = {{McGraw-Hill}},
  address = {{New York}},
  isbn = {978-0-07-054234-1},
  langid = {english},
  lccn = {QA300 .R82 1987},
  keywords = {Mathematical analysis},
  file = {/Users/wyq977/Zotero/storage/NFEGFBCZ/Rudin - 1987 - Real and complex analysis.pdf}
}

@article{schmidt-hieberNonparametricRegressionUsing2020,
  title = {Nonparametric Regression Using Deep Neural Networks with {{ReLU}} Activation Function},
  author = {{Schmidt-Hieber}, Johannes},
  year = {2020},
  month = sep,
  journal = {arXiv:1708.06633 [cs, math, stat]},
  eprint = {1708.06633},
  primaryclass = {cs, math, stat},
  doi = {10.1214/19-AOS1875},
  urldate = {2021-04-21},
  abstract = {Consider the multivariate nonparametric regression model. It is shown that estimators based on sparsely connected deep neural networks with ReLU activation function and properly chosen network architecture achieve the minimax rates of convergence (up to \$\textbackslash log n\$-factors) under a general composition assumption on the regression function. The framework includes many well-studied structural constraints such as (generalized) additive models. While there is a lot of flexibility in the network architecture, the tuning parameter is the sparsity of the network. Specifically, we consider large networks with number of potential network parameters exceeding the sample size. The analysis gives some insights into why multilayer feedforward neural networks perform well in practice. Interestingly, for ReLU activation function the depth (number of layers) of the neural network architectures plays an important role and our theory suggests that for nonparametric regression, scaling the network depth with the sample size is natural. It is also shown that under the composition assumption wavelet estimators can only achieve suboptimal rates.},
  archiveprefix = {arxiv},
  keywords = {62G08,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/ETB8XX9L/Schmidt-Hieber - 2020 - Nonparametric regression using deep neural network.pdf;/Users/wyq977/Zotero/storage/GX95FRWR/1708.html}
}

@book{shalev-shwartzUnderstandingMachineLearning2014,
  title = {Understanding Machine Learning: From Theory to Algorithms},
  shorttitle = {Understanding Machine Learning},
  author = {{Shalev-Shwartz}, Shai and {Ben-David}, Shai},
  year = {2014},
  publisher = {{Cambridge University Press}},
  address = {{New York, NY, USA}},
  abstract = {"Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering"--},
  isbn = {978-1-107-05713-5},
  langid = {english},
  lccn = {Q325.5 .S475 2014},
  keywords = {Algorithms,COMPUTERS / Computer Vision \& Pattern Recognition,Machine learning},
  file = {/Users/wyq977/Zotero/storage/TFGV6SZI/Shalev-Shwartz and Ben-David - 2014 - Understanding machine learning from theory to alg.pdf}
}

@misc{siegelApproximationRatesNeural2021,
  title = {Approximation {{Rates}} for {{Neural Networks}} with {{General Activation Functions}}},
  author = {Siegel, Jonathan W. and Xu, Jinchao},
  year = {2021},
  month = jan,
  number = {arXiv:1904.02311},
  eprint = {arXiv:1904.02311},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.02311},
  urldate = {2023-02-28},
  abstract = {We prove some new results concerning the approximation rate of neural networks with general activation functions. Our first result concerns the rate of approximation of a two layer neural network with a polynomially-decaying non-sigmoidal activation function. We extend the dimension independent approximation rates previously obtained to this new class of activation functions. Our second result gives a weaker, but still dimension independent, approximation rate for a larger class of activation functions, removing the polynomial decay assumption. This result applies to any bounded, integrable activation function. Finally, we show that a stratified sampling approach can be used to improve the approximation rate for polynomially decaying activation functions under mild additional assumptions.},
  archiveprefix = {arxiv},
  keywords = {41A25; 41A30,Computer Science - Machine Learning,Mathematics - Classical Analysis and ODEs},
  file = {/Users/wyq977/Zotero/storage/LFX9W3CE/Siegel and Xu - 2021 - Approximation Rates for Neural Networks with Gener.pdf;/Users/wyq977/Zotero/storage/5HUH4IZT/1904.html}
}

@misc{siegelCharacterizationVariationSpaces2022,
  title = {Characterization of the {{Variation Spaces Corresponding}} to {{Shallow Neural Networks}}},
  author = {Siegel, Jonathan W. and Xu, Jinchao},
  year = {2022},
  month = apr,
  number = {arXiv:2106.15002},
  eprint = {arXiv:2106.15002},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.15002},
  urldate = {2023-03-08},
  abstract = {We study the variation space corresponding to a dictionary of functions in \$L\^2(\textbackslash Omega)\$ for a bounded domain \$\textbackslash Omega\textbackslash subset \textbackslash mathbb\{R\}\^d\$. Specifically, we compare the variation space, which is defined in terms of a convex hull with related notions based on integral representations. This allows us to show that three important notions relating to the approximation theory of shallow neural networks, the Barron space, the spectral Barron space, and the Radon BV space, are actually variation spaces with respect to certain natural dictionaries.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/WDJINW7V/Siegel and Xu - 2022 - Characterization of the Variation Spaces Correspon.pdf;/Users/wyq977/Zotero/storage/3446DE8R/2106.html}
}

@misc{siegelHighOrderApproximationRates2021,
  title = {High-{{Order Approximation Rates}} for {{Shallow Neural Networks}} with {{Cosine}} and {{ReLU}}\$\^k\$ {{Activation Functions}}},
  author = {Siegel, Jonathan W. and Xu, Jinchao},
  year = {2021},
  month = dec,
  number = {arXiv:2012.07205},
  eprint = {arXiv:2012.07205},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.07205},
  urldate = {2023-02-24},
  abstract = {We study the approximation properties of shallow neural networks with an activation function which is a power of the rectified linear unit. Specifically, we consider the dependence of the approximation rate on the dimension and the smoothness in the spectral Barron space of the underlying function \$f\$ to be approximated. We show that as the smoothness index \$s\$ of \$f\$ increases, shallow neural networks with ReLU\$\^k\$ activation function obtain an improved approximation rate up to a best possible rate of \$O(n\^\{-(k+1)\}\textbackslash log(n))\$ in \$L\^2\$, independent of the dimension \$d\$. The significance of this result is that the activation function ReLU\$\^k\$ is fixed independent of the dimension, while for classical methods the degree of polynomial approximation or the smoothness of the wavelets used would have to increase in order to take advantage of the dimension dependent smoothness of \$f\$. In addition, we derive improved approximation rates for shallow neural networks with cosine activation function on the spectral Barron space. Finally, we prove lower bounds showing that the approximation rates attained are optimal under the given assumptions.},
  archiveprefix = {arxiv},
  keywords = {41A25,Mathematics - Numerical Analysis},
  file = {/Users/wyq977/Zotero/storage/EJTQANHI/Siegel and Xu - 2021 - High-Order Approximation Rates for Shallow Neural .pdf;/Users/wyq977/Zotero/storage/EHUSDAP4/2012.html}
}

@misc{siegelSharpBoundsApproximation2022,
  title = {Sharp {{Bounds}} on the {{Approximation Rates}}, {{Metric Entropy}}, and \$n\$-Widths of {{Shallow Neural Networks}}},
  author = {Siegel, Jonathan W. and Xu, Jinchao},
  year = {2022},
  month = jul,
  number = {arXiv:2101.12365},
  eprint = {arXiv:2101.12365},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2101.12365},
  urldate = {2023-02-28},
  abstract = {In this article, we study approximation properties of the variation spaces corresponding to shallow neural networks with a variety of activation functions. We introduce two main tools for estimating the metric entropy, approximation rates, and \$n\$-widths of these spaces. First, we introduce the notion of a smoothly parameterized dictionary and give upper bounds on the non-linear approximation rates, metric entropy and \$n\$-widths of their absolute convex hull. The upper bounds depend upon the order of smoothness of the parameterization. This result is applied to dictionaries of ridge functions corresponding to shallow neural networks, and they improve upon existing results in many cases. Next, we provide a method for lower bounding the metric entropy and \$n\$-widths of variation spaces which contain certain classes of ridge functions. This result gives sharp lower bounds on the \$L\^2\$-approximation rates, metric entropy, and \$n\$-widths for variation spaces corresponding to neural networks with a range of important activation functions, including ReLU\$\^k\$ activation functions and sigmoidal activation functions with bounded variation.},
  archiveprefix = {arxiv},
  keywords = {62M45; 41A46,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/7INC9LWR/Siegel and Xu - 2022 - Sharp Bounds on the Approximation Rates, Metric En.pdf;/Users/wyq977/Zotero/storage/YWG2MHDM/2101.html}
}

@misc{soudryImplicitBiasGradient2022,
  title = {The {{Implicit Bias}} of {{Gradient Descent}} on {{Separable Data}}},
  author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  year = {2022},
  month = jul,
  number = {arXiv:1710.10345},
  eprint = {arXiv:1710.10345},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1710.10345},
  urldate = {2023-02-20},
  abstract = {We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization n more complex models and with other optimization methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wyq977/Zotero/storage/88LTUMRH/Soudry et al. - 2022 - The Implicit Bias of Gradient Descent on Separable.pdf;/Users/wyq977/Zotero/storage/77N82LDM/1710.html}
}

@article{starlingTargetedSmoothBayesian2020,
  title = {Targeted {{Smooth Bayesian Causal Forests}}: {{An}} Analysis of Heterogeneous Treatment Effects for Simultaneous versus Interval Medical Abortion Regimens over Gestation},
  shorttitle = {Targeted {{Smooth Bayesian Causal Forests}}},
  author = {Starling, Jennifer E. and Murray, Jared S. and Lohr, Patricia A. and Aiken, Abigail R. A. and Carvalho, Carlos M. and Scott, James G.},
  year = {2020},
  month = feb,
  journal = {arXiv:1905.09405 [stat]},
  eprint = {1905.09405},
  primaryclass = {stat},
  urldate = {2021-04-19},
  abstract = {We introduce Targeted Smooth Bayesian Causal Forests (tsBCF), a nonparametric Bayesian approach for estimating heterogeneous treatment effects which vary smoothly over a single covariate in the observational data setting. The tsBCF method induces smoothness by parameterizing terminal tree nodes with smooth functions, and allows for separate regularization of treatment effects versus prognostic effect of control covariates. Smoothing parameters for prognostic and treatment effects can be chosen to reflect prior knowledge or tuned in a datadependent way.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Applications},
  file = {/Users/wyq977/Zotero/storage/7D9WEMZL/Starling et al. - 2020 - Targeted Smooth Bayesian Causal Forests An analys.pdf}
}

@misc{todorovaPaperReviewMultilayer2016,
  title = {Paper {{Review}} "{{Multilayer Feedforward Networks}} Are {{Universal Approximators}}"},
  author = {Todorova, Sonia},
  year = {2016},
  file = {/Users/wyq977/Zotero/storage/QDN96WDA/Todorova - 2016 - Paper Review Multilayer Feedforward Networks are .pdf}
}

@article{vandegeerEMPIRICALPROCESSTHEORY,
  title = {{{EMPIRICAL PROCESS THEORY AND APPLICATIONS}}},
  author = {{van de Geer}, Sara},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/DVX4B8VB/van de Geer - EMPIRICAL PROCESS THEORY AND APPLICATIONS.pdf}
}

@book{vandervaartWeakConvergenceEmpirical1996,
  title = {Weak {{Convergence}} and {{Empirical Processes}}},
  author = {{van der Vaart}, Aad W. and Wellner, Jon A.},
  year = {1996},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4757-2545-2},
  urldate = {2023-03-01},
  isbn = {978-1-4757-2547-6 978-1-4757-2545-2},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/R67LNRFD/van der Vaart and Wellner - 1996 - Weak Convergence and Empirical Processes.pdf}
}

@incollection{wahbaRepresenterTheorem2019,
  title = {Representer {{Theorem}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Wahba, Grace and Wang, Yuedong},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2019},
  month = nov,
  edition = {First},
  pages = {1--11},
  publisher = {{Wiley}},
  doi = {10.1002/9781118445112.stat08200},
  urldate = {2023-02-16},
  abstract = {The representer theorem plays an outsized role in a large class of learning problems. It provides a means to reduce infinite dimensional optimization problems to tractable finite dimensional ones. This article reviews the representer theorem for various learning problems under the reproducing kernel Hilbert spaces framework. We present solutions to the penalized least squares and penalized likelihood for nonparametric regression, and support vector machines for classification as a solution to the penalized hinge loss. We discuss extensions of the representer theorem for regression with functional data.},
  isbn = {978-1-118-44511-2},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/2UNJLD59/Wahba and Wang - 2019 - Representer Theorem.pdf}
}

@article{wangDebiasedInferenceTreatment,
  title = {Debiased {{Inference}} on {{Treatment Effect}} in a {{High-Dimensional Model}}},
  author = {Wang, Jingshen and He, Xuming and Xu, Gongjun},
  pages = {14},
  abstract = {This article concerns the potential bias in statistical inference on treatment effects when a large number of covariates are present in a linear or partially linear model. While the estimation bias in an under-fitted model is well understood, we address a lesser-known bias that arises from an over-fitted model. The overfitting bias can be eliminated through data splitting at the cost of statistical efficiency, and we show that smoothing over random data splits can be pursued to mitigate the efficiency loss. We also discuss some of the existing methods for debiased inference and provide insights into their intrinsic bias-variance trade-off, which leads to an improvement in bias controls. Under appropriate conditions, we show that the proposed estimators for the treatment effects are asymptotically normal and their variances can be well estimated. We discuss the pros and cons of various methods both theoretically and empirically, and show that the proposed methods are valuable options in post-selection inference. Supplementary materials for this article are available online.},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/QZGGM2TR/Wang et al. - Debiased Inference on Treatment Effect in a High-D.pdf}
}

@article{williamsonEntropyNumbersLinear,
  title = {Entropy {{Numbers}} of {{Linear Function Classes}}},
  author = {Williamson, Robert C and Smola, Alex J and Scholkopf, Bernhard},
  abstract = {This paper collects together a miscellany of results originally motivated by the analysis of the generalization performance of the ``maximum-margin'' algorithm due to Vapnik and others. The key feature of the paper is its operator-theoretic viewpoint. New bounds on covering numbers for classes related to Maximum Margin classes are derived directly without making use of a combinatorial dimension such as the VC-dimension. Specific contents of the paper include: \textasciimacron{} a new and self-contained proof of Maurey's theorem and some generalizations with small explicit values of constants; \textasciimacron{} bounds on the covering numbers of maximum margin classes suitable for the analysis of their generalization performance; \textasciimacron{} the extension of such classes to those induced {$\frac{1}{2}$} by balls in quasi-Banach spaces (such as \^Onorms with {$\frac{1}{4}$} \^O ).},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/4WP6VQXN/Williamson et al. - Entropy Numbers of Linear Function Classes.pdf}
}

@phdthesis{wuUnderstandingWhenKernel,
  title = {Understanding {{When Kernel Ridgeless Regression Works}}},
  author = {Wu, Mingqi},
  langid = {english},
  file = {/Users/wyq977/Zotero/storage/9ISXLHQA/Wu - Understanding When Kernel Ridgeless Regression Wor.pdf}
}

@article{xuFiniteNeuronMethod2020,
  title = {The {{Finite Neuron Method}} and {{Convergence Analysis}}},
  author = {Xu, Jinchao},
  year = {2020},
  month = jun,
  journal = {Communications in Computational Physics},
  volume = {28},
  number = {5},
  eprint = {2010.01458},
  primaryclass = {cs, math},
  pages = {1707--1745},
  issn = {1815-2406, 1991-7120},
  doi = {10.4208/cicp.OA-2020-0191},
  urldate = {2023-02-28},
  abstract = {We study a family of \$H\^m\$-conforming piecewise polynomials based on artificial neural network, named as the finite neuron method (FNM), for numerical solution of \$2m\$-th order partial differential equations in \$\textbackslash mathbb\{R\}\^d\$ for any \$m,d \textbackslash geq 1\$ and then provide convergence analysis for this method. Given a general domain \$\textbackslash Omega\textbackslash subset\textbackslash mathbb R\^d\$ and a partition \$\textbackslash mathcal T\_h\$ of \$\textbackslash Omega\$, it is still an open problem in general how to construct conforming finite element subspace of \$H\^m(\textbackslash Omega)\$ that have adequate approximation properties. By using techniques from artificial neural networks, we construct a family of \$H\^m\$-conforming set of functions consisting of piecewise polynomials of degree \$k\$ for any \$k\textbackslash ge m\$ and we further obtain the error estimate when they are applied to solve elliptic boundary value problem of any order in any dimension. For example, the following error estimates between the exact solution \$u\$ and finite neuron approximation \$u\_N\$ are obtained. \$\$ \textbackslash |u-u\_N\textbackslash |\_\{H\^m(\textbackslash Omega)\}=\textbackslash mathcal O(N\^\{-\{1\textbackslash over 2\}-\{1\textbackslash over d\}\}). \$\$ Discussions will also be given on the difference and relationship between the finite neuron method and finite element methods (FEM). For example, for finite neuron method, the underlying finite element grids are not given a priori and the discrete solution can only be obtained by solving a non-linear and non-convex optimization problem. Despite of many desirable theoretical properties of the finite neuron method analyzed in the paper, its practical value is a subject of further investigation since the aforementioned underlying non-linear and non-convex optimization problem can be expensive and challenging to solve. For completeness and also convenience to readers, some basic known results and their proofs are also included in this manuscript.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Numerical Analysis},
  file = {/Users/wyq977/Zotero/storage/2DZGGBX4/Xu - 2020 - The Finite Neuron Method and Convergence Analysis.pdf;/Users/wyq977/Zotero/storage/HGI836RF/2010.html}
}

@article{yukichSupnormApproximationBounds1995,
  title = {Sup-Norm Approximation Bounds for Networks through Probabilistic Methods},
  author = {Yukich, J.E. and Stinchcombe, M.B. and White, H.},
  year = {1995},
  month = jul,
  journal = {IEEE Transactions on Information Theory},
  volume = {41},
  number = {4},
  pages = {1021--1027},
  issn = {1557-9654},
  doi = {10.1109/18.391247},
  abstract = {We consider the problem of approximating a smooth target function and its derivatives by networks involving superpositions and translations of a fixed activation function. The approximation is with respect to the sup-norm and the rate is shown to be of order O(n/sup -1/2/); that is, the rate is independent of the dimension d. The results apply to neural and wavelet networks and extend the work of Barren(see Proc. 7th Yale Workshop on Adaptive and Learning Systems, May, 1992, and ibid., vol.39, p.930, 1993). The approach involves probabilistic methods based on central limit theorems for empirical processes indexed by classes of functions.{$<>$}},
  keywords = {Artificial neural networks,Chaos,Fourier transforms,Mathematics,Neural networks,Robots},
  file = {/Users/wyq977/Zotero/storage/PYPK9LI5/Yukich et al. - 1995 - Sup-norm approximation bounds for networks through.pdf;/Users/wyq977/Zotero/storage/TIJAZIN5/391247.html}
}
