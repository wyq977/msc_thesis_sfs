@inproceedings{barron_neural_1992,
	title = {Neural {N}et {A}pproximation},
	volume = {1},
	booktitle = {Proc. 7th {Yale} workshop on adaptive and learning systems},
	author = {Barron, Andrew R.},
	year = {1992},
	pages = {69--72}
}

@article{cybenko_approximation_1989,
	title = {Approximation by {S}uperpositions of a {S}igmoidal {F}unction},
	volume = {2},
	issn = {1435-568X},
	url = {https://doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	language = {en},
	number = {4},
	urldate = {2022-06-23},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	keywords = {Approximation, Completeness, Neural networks},
	pages = {303--314}
}

@article{barron_universal_1993,
	title = {Universal {A}pproximation {B}ounds for {S}uperpositions of a {S}igmoidal {F}unction},
	volume = {39},
	issn = {1557-9654},
	doi = {10.1109/18.256500},
	abstract = {Approximation properties of a class of artificial neural networks are established. It is shown that feedforward networks with one layer of sigmoidal nonlinearities achieve integrated squared error of order O(1/n), where n is the number of nodes. The approximated function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. The nonlinear parameters associated with the sigmoidal nodes, as well as the parameters of linear combination, are adjusted in the approximation. In contrast, it is shown that for series expansions with n terms, in which only the parameters of linear combination are adjusted, the integrated squared approximation error cannot be made smaller than order 1/n/sup 2/d/ uniformly for functions satisfying the same smoothness assumption, where d is the dimension of the input to the function. For the class of functions examined, the approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings.{\textless}{\textgreater}},
	number = {3},
	journal = {IEEE Transactions on Information Theory},
	author = {Barron, A. R.},
	month = may,
	year = {1993},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {Neural networks, Approximation error, Artificial neural networks, Feedforward neural networks, Feeds, Fourier transforms, Information theory, Linear approximation, Statistical distributions, Statistics},
	pages = {930--945}
}
